{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❗ 5. SKHynix PBL 시계열 시퀀스 모델링 ❗\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 개요\n",
    "\n",
    "시간대(timekey_hr) 내에서 공정 순서(oper_id)를 고려한 시퀀스 기반 TAT 예측 모델입니다. 동일한 timekey_hr 내의 oper_id들을 순서대로 정렬하여 시퀀스 데이터로 구성하고, 각 oper별 개별 예측(sequence-to-sequence)을 수행합니다.\n",
    "\n",
    "**데이터 구조**: `[batch_size, sequence_length, feature_dim]`\n",
    "- **sequence_length**: timekey_hr 내 최대 oper_id 개수 (하이퍼파라미터)\n",
    "- **feature_dim**: 연속형 변수 개수 + 범주형 변수 개수 × 임베딩 차원\n",
    "\n",
    "**지원 모델**:\n",
    "1. **RNN/LSTM/GRU**: 기본 순환 신경망\n",
    "2. **RNN + Self-Attention**: 순환 신경망에 어텐션 메커니즘 추가\n",
    "3. **CNN 1D**: 다중 커널 1차원 합성곱 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 환경 설정 및 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 유틸리티 함수들\n",
    "\n",
    "### 설정 로딩 및 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_dir: str = \"configs\") -> Dict:\n",
    "    \"\"\"YAML 설정 파일들을 통합하여 로드\"\"\"\n",
    "    configs = {}\n",
    "    config_files = [\"dataset\", \"model\", \"training\"]\n",
    "\n",
    "    for file in config_files:\n",
    "        config_path = os.path.join(config_dir, f\"{file}.yaml\")\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            configs.update(config)\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def set_random_seeds(seed: int = 42):\n",
    "    \"\"\"재현성을 위한 랜덤 시드 설정\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def setup_logging(log_file: str = \"training.log\"):\n",
    "    \"\"\"로깅 설정\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범주형 데이터 처리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalProcessor:\n",
    "    \"\"\"범주형 변수 임베딩을 위한 처리기\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 8):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_encoders = {}\n",
    "        self.vocab_sizes = {}\n",
    "        self.categorical_columns = []\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, categorical_columns: List[str]):\n",
    "        \"\"\"전체 데이터에 대해 범주형 인코더 학습\"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            unique_values = df[col].astype(str).unique()\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(unique_values)\n",
    "            \n",
    "            self.label_encoders[col] = encoder\n",
    "            self.vocab_sizes[col] = len(encoder.classes_)\n",
    "        \n",
    "        logger.info(f\"범주형 변수별 고유값 개수:\")\n",
    "        for col in categorical_columns:\n",
    "            logger.info(f\"  {col}: {self.vocab_sizes[col]}개\")\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"DataFrame의 범주형 컬럼들을 숫자로 변환\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            df_encoded[col] = self.label_encoders[col].transform(\n",
    "                df_encoded[col].astype(str)\n",
    "            )\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def get_vocab_sizes(self) -> List[int]:\n",
    "        \"\"\"각 범주형 변수의 vocab_size 리스트 반환\"\"\"\n",
    "        return [self.vocab_sizes[col] for col in self.categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ 시퀀스 데이터셋 클래스\n",
    "\n",
    "### 메인 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceOperDataset(Dataset):\n",
    "    \"\"\"시퀀스 기반 oper 데이터셋\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        categorical_columns: List[str],\n",
    "        continuous_columns: List[str],\n",
    "        target_column: str = \"y\",\n",
    "        categorical_processor: Optional[CategoricalProcessor] = None,\n",
    "        max_sequence_length: int = 50,\n",
    "        embedding_dim: int = 8,\n",
    "        padding_value: float = -999999.0\n",
    "    ):\n",
    "        self.df = df.copy()\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.continuous_columns = continuous_columns\n",
    "        self.target_column = target_column\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # 범주형 데이터 처리기 설정\n",
    "        if categorical_processor is None:\n",
    "            self.categorical_processor = CategoricalProcessor(embedding_dim)\n",
    "            self.categorical_processor.fit(df, categorical_columns)\n",
    "        else:\n",
    "            self.categorical_processor = categorical_processor\n",
    "        \n",
    "        # 데이터 전처리 및 시퀀스 생성\n",
    "        self._preprocess_data()\n",
    "        self._create_sequences()\n",
    "        \n",
    "        logger.info(f\"시퀀스 데이터셋 구성 완료:\")\n",
    "        logger.info(f\"  - 총 시퀀스 수: {len(self.sequences)}\")\n",
    "        logger.info(f\"  - 최대 시퀀스 길이: {max_sequence_length}\")\n",
    "        logger.info(f\"  - 특성 차원: {self.feature_dim}\")\n",
    "        logger.info(f\"  - 패딩값: {padding_value}\")\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        # 범주형 데이터 인코딩\n",
    "        if self.categorical_columns:\n",
    "            categorical_encoded = self.categorical_processor.transform(\n",
    "                self.df[self.categorical_columns]\n",
    "            )\n",
    "            self.df[self.categorical_columns] = categorical_encoded\n",
    "        \n",
    "        # 특성 차원 계산\n",
    "        continuous_dim = len(self.continuous_columns)\n",
    "        categorical_dim = len(self.categorical_columns) * self.embedding_dim\n",
    "        self.feature_dim = continuous_dim + categorical_dim\n",
    "    \n",
    "    def _create_sequences(self):\n",
    "        \"\"\"timekey_hr별로 oper_id 순서 기준 시퀀스 생성\"\"\"\n",
    "        self.sequences = []\n",
    "        \n",
    "        # timekey_hr별로 그룹화\n",
    "        grouped = self.df.groupby('timekey_hr')\n",
    "        \n",
    "        for timekey_hr, group in grouped:\n",
    "            # oper_id 순서로 정렬\n",
    "            group_sorted = group.sort_values('oper_id').reset_index(drop=True)\n",
    "            \n",
    "            if len(group_sorted) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 연속형 데이터 추출\n",
    "            if self.continuous_columns:\n",
    "                continuous_data = group_sorted[self.continuous_columns].values\n",
    "            else:\n",
    "                continuous_data = np.empty((len(group_sorted), 0))\n",
    "            \n",
    "            # 범주형 데이터 추출 (나중에 임베딩으로 변환)\n",
    "            if self.categorical_columns:\n",
    "                categorical_data = group_sorted[self.categorical_columns].values\n",
    "            else:\n",
    "                categorical_data = np.empty((len(group_sorted), 0))\n",
    "            \n",
    "            # 타겟 데이터 추출\n",
    "            target_data = group_sorted[self.target_column].values\n",
    "            \n",
    "            # oper_id 정보 (디버깅용)\n",
    "            oper_ids = group_sorted['oper_id'].values\n",
    "            \n",
    "            sequence_info = {\n",
    "                'timekey_hr': timekey_hr,\n",
    "                'continuous_data': continuous_data,\n",
    "                'categorical_data': categorical_data,\n",
    "                'target_data': target_data,\n",
    "                'oper_ids': oper_ids,\n",
    "                'sequence_length': len(group_sorted)\n",
    "            }\n",
    "            \n",
    "            self.sequences.append(sequence_info)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        \n",
    "        return {\n",
    "            'continuous_data': sequence['continuous_data'],\n",
    "            'categorical_data': sequence['categorical_data'], \n",
    "            'target_data': sequence['target_data'],\n",
    "            'sequence_length': sequence['sequence_length'],\n",
    "            'timekey_hr': sequence['timekey_hr'],\n",
    "            'oper_ids': sequence['oper_ids']\n",
    "        }\n",
    "\n",
    "\n",
    "def sequence_collate_fn(batch, max_sequence_length: int, padding_value: float = -999999.0):\n",
    "    \"\"\"시퀀스 배치를 위한 패딩 함수 (구조 정보 포함)\"\"\"\n",
    "    batch_size = len(batch)\n",
    "    \n",
    "    # 첫 번째 샘플에서 차원 정보 추출\n",
    "    first_sample = batch[0]\n",
    "    continuous_dim = first_sample['continuous_data'].shape[1]\n",
    "    categorical_dim = first_sample['categorical_data'].shape[1]\n",
    "    \n",
    "    # 기존 데이터 패딩\n",
    "    batch_continuous = np.full(\n",
    "        (batch_size, max_sequence_length, continuous_dim), \n",
    "        padding_value, dtype=np.float32\n",
    "    )\n",
    "    batch_categorical = np.full(\n",
    "        (batch_size, max_sequence_length, categorical_dim), \n",
    "        0, dtype=np.int64\n",
    "    )\n",
    "    batch_targets = np.full(\n",
    "        (batch_size, max_sequence_length), \n",
    "        padding_value, dtype=np.float32\n",
    "    )\n",
    "    batch_masks = np.ones(\n",
    "        (batch_size, max_sequence_length), \n",
    "        dtype=bool\n",
    "    )\n",
    "    \n",
    "    # 구조 정보를 위한 리스트들 추가\n",
    "    batch_timekey_hrs = []\n",
    "    batch_oper_ids_list = []\n",
    "    batch_lengths = []\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        seq_len = min(sample['sequence_length'], max_sequence_length)\n",
    "        batch_lengths.append(seq_len)\n",
    "        \n",
    "        # 기존 데이터 채우기\n",
    "        batch_continuous[i, :seq_len] = sample['continuous_data'][:seq_len]\n",
    "        batch_categorical[i, :seq_len] = sample['categorical_data'][:seq_len]  \n",
    "        batch_targets[i, :seq_len] = sample['target_data'][:seq_len]\n",
    "        batch_masks[i, :seq_len] = False\n",
    "        \n",
    "        # 구조 정보 추가\n",
    "        batch_timekey_hrs.append(sample['timekey_hr'])\n",
    "        # max_sequence_length만큼 oper_id 리스트 생성 (패딩된 부분은 None)\n",
    "        oper_ids_padded = list(sample['oper_ids'][:seq_len])\n",
    "        while len(oper_ids_padded) < max_sequence_length:\n",
    "            oper_ids_padded.append(None)\n",
    "        batch_oper_ids_list.append(oper_ids_padded)\n",
    "    \n",
    "    return {\n",
    "        'continuous_data': torch.tensor(batch_continuous),\n",
    "        'categorical_data': torch.tensor(batch_categorical),\n",
    "        'targets': torch.tensor(batch_targets),\n",
    "        'masks': torch.tensor(batch_masks),\n",
    "        'sequence_lengths': batch_lengths,\n",
    "        'timekey_hrs': batch_timekey_hrs,  \n",
    "        'oper_ids_list': batch_oper_ids_list\n",
    "    }\n",
    "\n",
    "\n",
    "def create_dataloaders(dataset_config: Dict) -> Tuple[DataLoader, DataLoader, DataLoader]:\n",
    "    \"\"\"데이터로더 생성\"\"\"\n",
    "    \n",
    "    # 데이터 로드 및 전처리\n",
    "    data_path = dataset_config[\"file_path\"]\n",
    "    excel = pd.read_excel(data_path, sheet_name=None, header=1)\n",
    "    sheet_names = dataset_config[\"sheet_names\"]\n",
    "    \n",
    "    total_df = pd.concat([excel[sheet_name] for sheet_name in sheet_names])\n",
    "    \n",
    "    # 기본 전처리\n",
    "    if \"Unnamed: 0\" in total_df.columns:\n",
    "        total_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    \n",
    "    # y값 결측치 제거\n",
    "    df = total_df[~total_df[dataset_config[\"target_column\"]].isna()].copy()\n",
    "    \n",
    "    # 불필요한 컬럼 제거\n",
    "    drop_columns = dataset_config.get(\"additional_drop_columns\", [])\n",
    "    if drop_columns:\n",
    "        existing_drops = [col for col in drop_columns if col in df.columns]\n",
    "        if existing_drops:\n",
    "            df = df.drop(columns=existing_drops)\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # 전체 데이터에 대해 범주형 처리기 학습\n",
    "    categorical_processor = CategoricalProcessor(\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8)\n",
    "    )\n",
    "    categorical_processor.fit(df, dataset_config[\"categorical_columns\"])\n",
    "    \n",
    "    # 데이터 분할 (8:1:1)\n",
    "    total_size = len(df)\n",
    "    train_end = int(total_size * dataset_config.get(\"train_ratio\", 0.8))\n",
    "    val_end = int(total_size * (dataset_config.get(\"train_ratio\", 0.8) + dataset_config.get(\"val_ratio\", 0.1)))\n",
    "    \n",
    "    train_df = df[:train_end].copy()\n",
    "    val_df = df[train_end:val_end].copy()\n",
    "    test_df = df[val_end:].copy()\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = SequenceOperDataset(\n",
    "        df=train_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        max_sequence_length=dataset_config.get(\"max_sequence_length\", 50),\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=dataset_config.get(\"padding_value\", -999999.0)\n",
    "    )\n",
    "    \n",
    "    val_dataset = SequenceOperDataset(\n",
    "        df=val_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"], \n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        max_sequence_length=dataset_config.get(\"max_sequence_length\", 50),\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=dataset_config.get(\"padding_value\", -999999.0)\n",
    "    )\n",
    "    \n",
    "    test_dataset = SequenceOperDataset(\n",
    "        df=test_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        target_column=dataset_config[\"target_column\"], \n",
    "        categorical_processor=categorical_processor,\n",
    "        max_sequence_length=dataset_config.get(\"max_sequence_length\", 50),\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=dataset_config.get(\"padding_value\", -999999.0)\n",
    "    )\n",
    "    \n",
    "    # Collate 함수 설정\n",
    "    def collate_fn(batch):\n",
    "        return sequence_collate_fn(\n",
    "            batch, \n",
    "            max_sequence_length=dataset_config.get(\"max_sequence_length\", 50),\n",
    "            padding_value=dataset_config.get(\"padding_value\", -999999.0)\n",
    "        )\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    batch_size = dataset_config.get(\"batch_size\", 32)\n",
    "    num_workers = dataset_config.get(\"num_workers\", 4)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"데이터로더 생성 완료:\")\n",
    "    logger.info(f\"  - 훈련 샘플: {len(train_dataset)}\")\n",
    "    logger.info(f\"  - 검증 샘플: {len(val_dataset)}\")\n",
    "    logger.info(f\"  - 테스트 샘플: {len(test_dataset)}\")\n",
    "    logger.info(f\"  - 배치 크기: {batch_size}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, categorical_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel = pd.read_excel(\"/home/doyooni303/teaching/SK_Hynix/PBL_TAT_Set_Data Rev2(사외).xlsx\", sheet_name=None, header=1)\n",
    "\n",
    "total = pd.concat([excel[sheet_name] for sheet_name in [\"Data_Set1(사외)\",\"Data_Set2(사외)\"]] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtotal\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total' is not defined"
     ]
    }
   ],
   "source": [
    "total.drop(columns=\"Unnamed: 0\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"시계열 시퀀스 모델링\")\n",
    "parser.add_argument(\"--config-dir\", default=\"configs\", help=\"설정 파일 디렉토리\")\n",
    "parser.add_argument(\"--mode\", choices=[\"train\", \"eval\"], default=\"train\", help=\"실행 모드\")\n",
    "parser.add_argument(\"--model-path\", default=None, help=\"평가용 모델 경로\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU 번호\")\n",
    "parser.add_argument(\"--exp-name\", default=\"code-test\", help=\"실험명\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# 설정 로드\n",
    "config = load_config(args.config_dir)\n",
    "set_random_seeds(42)\n",
    "\n",
    "# 실험명 설정\n",
    "if args.exp_name:\n",
    "    exp_name = args.exp_name\n",
    "else:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_type = config.get(\"model_type\", \"lstm\")\n",
    "    exp_name = f\"{model_type}_{timestamp}\"\n",
    "\n",
    "# 저장 디렉토리\n",
    "save_dir = config.get(\"save_dir\", \"models\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(save_dir, f\"{exp_name}.pth\")\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# 데이터로더 생성\n",
    "logger.info(\"데이터 로딩 중...\")\n",
    "train_loader, val_loader, test_loader, categorical_processor = create_dataloaders(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ 모델 아키텍처들\n",
    "\n",
    "### RNN 기본 모델 (models/rnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"기본 RNN/LSTM/GRU 모델\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_sizes: List[int],  # 각 범주형 변수의 고유값 개수 리스트 [277, 7, 3, 20]\n",
    "        continuous_dim: int,\n",
    "        embedding_dim: int = 8,\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: bool = True,\n",
    "        padding_value: float = -999999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.padding_value = padding_value\n",
    "        self.num_categorical_vars = len(vocab_sizes)  # 범주형 변수의 개수\n",
    "        \n",
    "        # 각 범주형 변수별로 별도의 임베딩 레이어 생성\n",
    "        # 예: oper_group(277) -> 8차원, days(7) -> 8차원, shift(3) -> 8차원, x1(20) -> 8차원\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # 입력 차원 = 연속형 차원 + (범주형 변수 개수 × 임베딩 차원)\n",
    "        # 예: continuous_dim=20, num_categorical_vars=4, embedding_dim=8\n",
    "        # → input_dim = 20 + 4*8 = 52\n",
    "        input_dim = continuous_dim + self.num_categorical_vars * embedding_dim\n",
    "        \n",
    "        # RNN 레이어\n",
    "        if rnn_type.upper() == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type.upper() == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                input_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # 출력 차원 계산\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, continuous_data, categorical_data, masks, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            continuous_data: [batch_size, seq_len, continuous_dim]\n",
    "            categorical_data: [batch_size, seq_len, num_categorical_vars] (각 위치는 범주형 변수의 인덱스)\n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "            sequence_lengths: [batch_size] \n",
    "        \"\"\"\n",
    "        batch_size, seq_len = continuous_data.shape[:2]\n",
    "        \n",
    "        # 각 범주형 변수별로 임베딩 적용\n",
    "        # categorical_data[:, :, 0] = oper_group 인덱스들 → 8차원 임베딩\n",
    "        # categorical_data[:, :, 1] = days 인덱스들 → 8차원 임베딩  \n",
    "        # categorical_data[:, :, 2] = shift 인덱스들 → 8차원 임베딩\n",
    "        # categorical_data[:, :, 3] = x1 인덱스들 → 8차원 임베딩\n",
    "        embedded_categorical = []\n",
    "        for i, embedding_layer in enumerate(self.embeddings):\n",
    "            # categorical_data[:, :, i]: [batch_size, seq_len] → [batch_size, seq_len, embedding_dim]\n",
    "            embedded = embedding_layer(categorical_data[:, :, i])\n",
    "            embedded_categorical.append(embedded)\n",
    "        \n",
    "        if embedded_categorical:\n",
    "            # 모든 범주형 변수의 임베딩을 concatenate\n",
    "            # [batch_size, seq_len, num_categorical_vars * embedding_dim]\n",
    "            categorical_embedded = torch.cat(embedded_categorical, dim=-1)\n",
    "        else:\n",
    "            categorical_embedded = torch.empty(batch_size, seq_len, 0, device=continuous_data.device)\n",
    "        \n",
    "        # 연속형과 범주형 결합\n",
    "        # [batch_size, seq_len, continuous_dim + num_categorical_vars * embedding_dim]\n",
    "        combined_input = torch.cat([continuous_data, categorical_embedded], dim=-1)\n",
    "        \n",
    "        # 패딩된 위치를 마스킹\n",
    "        combined_input = combined_input.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # RNN forward\n",
    "        rnn_output, _ = self.rnn(combined_input)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        predictions = self.output_layer(rnn_output).squeeze(-1)\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN + Self-Attention 모델 (models/attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention 메커니즘\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, hidden_dim]\n",
    "            mask: [batch_size, seq_len] (True = 패딩)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Multi-head attention\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_mask = mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended = torch.matmul(attention_weights, V)\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Residual connection + Layer norm\n",
    "        output = self.layer_norm(x + attended)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class RNNAttentionModel(nn.Module):\n",
    "    \"\"\"RNN + Self-Attention 모델\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_sizes: List[int],  # 각 범주형 변수의 고유값 개수 리스트 [277, 7, 3, 20]\n",
    "        continuous_dim: int,\n",
    "        embedding_dim: int = 8,\n",
    "        rnn_type: str = \"LSTM\", \n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: bool = True,\n",
    "        padding_value: float = -999999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding_value = padding_value\n",
    "        self.num_categorical_vars = len(vocab_sizes)  # 범주형 변수의 개수\n",
    "        \n",
    "        # 각 범주형 변수별로 별도의 임베딩 레이어 생성\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # 입력 차원 = 연속형 차원 + (범주형 변수 개수 × 임베딩 차원)\n",
    "        input_dim = continuous_dim + self.num_categorical_vars * embedding_dim\n",
    "        \n",
    "        # RNN 레이어\n",
    "        if rnn_type.upper() == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type.upper() == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                input_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # RNN 출력 차원\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        # RNN 출력을 어텐션 입력 차원으로 변환\n",
    "        self.rnn_projection = nn.Linear(rnn_output_dim, hidden_dim)\n",
    "        \n",
    "        # Self-Attention\n",
    "        self.self_attention = SelfAttention(\n",
    "            hidden_dim, num_attention_heads, dropout\n",
    "        )\n",
    "        \n",
    "        # 출력 레이어\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, continuous_data, categorical_data, masks, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            continuous_data: [batch_size, seq_len, continuous_dim]\n",
    "            categorical_data: [batch_size, seq_len, num_categorical]\n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = continuous_data.shape[:2]\n",
    "        \n",
    "        # 범주형 데이터 임베딩\n",
    "        embedded_categorical = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            embedded = embedding(categorical_data[:, :, i])\n",
    "            embedded_categorical.append(embedded)\n",
    "        \n",
    "        if embedded_categorical:\n",
    "            categorical_embedded = torch.cat(embedded_categorical, dim=-1)\n",
    "        else:\n",
    "            categorical_embedded = torch.empty(batch_size, seq_len, 0, device=continuous_data.device)\n",
    "        \n",
    "        # 연속형과 범주형 결합\n",
    "        combined_input = torch.cat([continuous_data, categorical_embedded], dim=-1)\n",
    "        \n",
    "        # 패딩된 위치를 마스킹\n",
    "        combined_input = combined_input.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # RNN forward\n",
    "        rnn_output, _ = self.rnn(combined_input)\n",
    "        \n",
    "        # RNN 출력 차원 변환\n",
    "        projected_output = self.rnn_projection(rnn_output)\n",
    "        \n",
    "        # Self-Attention 적용\n",
    "        attended_output = self.self_attention(projected_output, masks)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        predictions = self.output_layer(attended_output).squeeze(-1)\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1D 모델 (models/cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    \"\"\"1D CNN 모델 (다중 커널)\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_sizes: List[int],  # 각 범주형 변수의 고유값 개수 리스트 [277, 7, 3, 20]\n",
    "        continuous_dim: int,\n",
    "        embedding_dim: int = 8,\n",
    "        kernel_sizes: List[int] = [3, 5, 7],\n",
    "        num_filters: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        padding_value: float = -999999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.continuous_dim = continuous_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.padding_value = padding_value\n",
    "        self.num_categorical_vars = len(vocab_sizes)  # 범주형 변수의 개수\n",
    "        \n",
    "        # 각 범주형 변수별로 별도의 임베딩 레이어 생성\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "            for vocab_size in vocab_sizes\n",
    "        ])\n",
    "        \n",
    "        # 입력 차원 = 연속형 차원 + (범주형 변수 개수 × 임베딩 차원)\n",
    "        input_dim = continuous_dim + self.num_categorical_vars * embedding_dim\n",
    "        \n",
    "        # 다중 커널 1D Conv 레이어들\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(input_dim, num_filters, kernel_size, padding=kernel_size//2)\n",
    "            for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(num_filters) for _ in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # 출력 레이어\n",
    "        total_filters = len(kernel_sizes) * num_filters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters, total_filters // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, continuous_data, categorical_data, masks, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            continuous_data: [batch_size, seq_len, continuous_dim]\n",
    "            categorical_data: [batch_size, seq_len, num_categorical]\n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = continuous_data.shape[:2]\n",
    "        \n",
    "        # 범주형 데이터 임베딩\n",
    "        embedded_categorical = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            embedded = embedding(categorical_data[:, :, i])\n",
    "            embedded_categorical.append(embedded)\n",
    "        \n",
    "        if embedded_categorical:\n",
    "            categorical_embedded = torch.cat(embedded_categorical, dim=-1)\n",
    "        else:\n",
    "            categorical_embedded = torch.empty(batch_size, seq_len, 0, device=continuous_data.device)\n",
    "        \n",
    "        # 연속형과 범주형 결합\n",
    "        combined_input = torch.cat([continuous_data, categorical_embedded], dim=-1)\n",
    "        \n",
    "        # 패딩된 위치를 마스킹\n",
    "        combined_input = combined_input.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # Conv1d를 위해 차원 변환: [batch, seq_len, features] -> [batch, features, seq_len]\n",
    "        conv_input = combined_input.transpose(1, 2)\n",
    "        \n",
    "        # 다중 커널 Conv1D 적용\n",
    "        conv_outputs = []\n",
    "        for conv, bn in zip(self.conv_layers, self.batch_norms):\n",
    "            conv_out = F.relu(bn(conv(conv_input)))  # [batch, filters, seq_len]\n",
    "            conv_outputs.append(conv_out)\n",
    "        \n",
    "        # 모든 커널 출력 결합\n",
    "        combined_conv = torch.cat(conv_outputs, dim=1)  # [batch, total_filters, seq_len]\n",
    "        \n",
    "        # 다시 원래 차원으로: [batch, total_filters, seq_len] -> [batch, seq_len, total_filters]\n",
    "        combined_conv = combined_conv.transpose(1, 2)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        predictions = self.output_layer(combined_conv).squeeze(-1)\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 팩토리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_config: Dict, vocab_sizes: List[int], continuous_dim: int):\n",
    "    \"\"\"설정에 따른 모델 생성\"\"\"\n",
    "    \n",
    "    model_type = model_config.get(\"model_type\", \"lstm\").lower()\n",
    "    embedding_dim = model_config.get(\"embedding_dim\", 8)\n",
    "    hidden_dim = model_config.get(\"hidden_dim\", 128)\n",
    "    num_layers = model_config.get(\"num_layers\", 2)\n",
    "    dropout = model_config.get(\"dropout\", 0.1)\n",
    "    bidirectional = model_config.get(\"bidirectional\", True)\n",
    "    padding_value = model_config.get(\"padding_value\", -999999.0)\n",
    "    \n",
    "    if model_type in [\"rnn\", \"lstm\", \"gru\"]:\n",
    "        model = RNNModel(\n",
    "            vocab_sizes=vocab_sizes,\n",
    "            continuous_dim=continuous_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            rnn_type=model_type.upper(),\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "    elif model_type in [\"rnn_attention\", \"lstm_attention\", \"gru_attention\"]:\n",
    "        rnn_type = model_type.replace(\"_attention\", \"\").upper()\n",
    "        num_attention_heads = model_config.get(\"num_attention_heads\", 8)\n",
    "        \n",
    "        model = RNNAttentionModel(\n",
    "            vocab_sizes=vocab_sizes,\n",
    "            continuous_dim=continuous_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            rnn_type=rnn_type,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "    elif model_type == \"cnn1d\":\n",
    "        kernel_sizes = model_config.get(\"kernel_sizes\", [3, 5, 7])\n",
    "        num_filters = model_config.get(\"num_filters\", 64)\n",
    "        \n",
    "        model = CNN1DModel(\n",
    "            vocab_sizes=vocab_sizes,\n",
    "            continuous_dim=continuous_dim,\n",
    "            embedding_dim=embedding_dim,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            num_filters=num_filters,\n",
    "            dropout=dropout,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    logger.info(f\"모델 생성 완료:\")\n",
    "    logger.info(f\"  - 모델 타입: {model_type}\")\n",
    "    logger.info(f\"  - 총 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    logger.info(f\"  - 학습 가능한 파라미터 수: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚂 훈련 및 평가 함수들\n",
    "\n",
    "### 마스크 기반 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"패딩을 고려한 MSE Loss\"\"\"\n",
    "    \n",
    "    def __init__(self, padding_value: float = -999999.0):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "    \n",
    "    def forward(self, predictions, targets, masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: [batch_size, seq_len]\n",
    "            targets: [batch_size, seq_len]  \n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "        \"\"\"\n",
    "        # 패딩되지 않은 위치만 선택\n",
    "        valid_mask = ~masks\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        valid_predictions = predictions[valid_mask]\n",
    "        valid_targets = targets[valid_mask]\n",
    "        \n",
    "        return F.mse_loss(valid_predictions, valid_targets)\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets, masks, padding_value: float = -999999.0):\n",
    "    \"\"\"패딩을 고려한 메트릭 계산\"\"\"\n",
    "    valid_mask = ~masks\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        return {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_targets = targets[valid_mask]\n",
    "    \n",
    "    # CPU로 변환\n",
    "    valid_predictions = valid_predictions.detach().cpu().numpy()\n",
    "    valid_targets = valid_targets.detach().cpu().numpy()\n",
    "    \n",
    "    mse = np.mean((valid_predictions - valid_targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(valid_predictions - valid_targets))\n",
    "    \n",
    "    # MAPE 계산 (0으로 나누기 방지)\n",
    "    epsilon = 1e-8\n",
    "    abs_targets = np.abs(valid_targets)\n",
    "    abs_errors = np.abs(valid_predictions - valid_targets)\n",
    "    safe_targets = np.maximum(abs_targets, epsilon)\n",
    "    mape = np.mean(abs_errors / safe_targets * 100)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse, \n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"valid_count\": len(valid_predictions)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 에폭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"한 에폭 훈련\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_metrics = {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        desc=f\"Epoch {epoch} [Train]\",\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    for batch_idx, batch in pbar:\n",
    "        continuous_data = batch[\"continuous_data\"].to(device)\n",
    "        categorical_data = batch[\"categorical_data\"].to(device)\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "        masks = batch[\"masks\"].to(device)\n",
    "        sequence_lengths = batch[\"sequence_lengths\"]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(continuous_data, categorical_data, masks, sequence_lengths)\n",
    "        loss = criterion(predictions, targets, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        with torch.no_grad():\n",
    "            batch_metrics = compute_metrics(predictions, targets, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "            total_metrics[key] += batch_metrics[key]\n",
    "        total_metrics[\"valid_count\"] += batch_metrics[\"valid_count\"]\n",
    "        \n",
    "        # 진행바 업데이트\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"MAPE\": f\"{batch_metrics['mape']:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # 평균 계산\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "        total_metrics[key] = total_metrics[key] / len(dataloader)\n",
    "    \n",
    "    return avg_loss, total_metrics\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device, epoch=None):\n",
    "    \"\"\"검증 에폭\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_metrics = {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    desc = f\"Epoch {epoch} [Val]\" if epoch is not None else \"Validation\"\n",
    "    pbar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            continuous_data = batch[\"continuous_data\"].to(device)\n",
    "            categorical_data = batch[\"categorical_data\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            masks = batch[\"masks\"].to(device)\n",
    "            sequence_lengths = batch[\"sequence_lengths\"]\n",
    "            \n",
    "            predictions = model(continuous_data, categorical_data, masks, sequence_lengths)\n",
    "            loss = criterion(predictions, targets, masks)\n",
    "            \n",
    "            batch_metrics = compute_metrics(predictions, targets, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "                total_metrics[key] += batch_metrics[key]\n",
    "            total_metrics[\"valid_count\"] += batch_metrics[\"valid_count\"]\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"MAPE\": f\"{batch_metrics['mape']:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "        total_metrics[key] = total_metrics[key] / len(dataloader)\n",
    "    \n",
    "    return avg_loss, total_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메인 훈련 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, training_config, device, save_path):\n",
    "    \"\"\"메인 훈련 루프\"\"\"\n",
    "    \n",
    "    num_epochs = training_config.get(\"num_epochs\", 100)\n",
    "    learning_rate = training_config.get(\"learning_rate\", 1e-3)\n",
    "    patience = training_config.get(\"patience\", 20)\n",
    "    padding_value = training_config.get(\"padding_value\", -999999.0)\n",
    "    \n",
    "    # 손실 함수 및 옵티마이저\n",
    "    criterion = MaskedMSELoss(padding_value)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience//2, verbose=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    logger.info(f\"훈련 시작: {num_epochs} 에폭, 학습률 {learning_rate}\")\n",
    "    \n",
    "    # 에폭 진행바\n",
    "    epoch_pbar = tqdm(range(1, num_epochs + 1), desc=\"Training Progress\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        train_loss, train_metrics = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        val_loss, val_metrics = validate_epoch(\n",
    "            model, val_loader, criterion, device, epoch\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 로그 출력\n",
    "        logger.info(\n",
    "            f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "            f'Train MAPE={train_metrics[\"mape\"]:.2f}%, Val MAPE={val_metrics[\"mape\"]:.2f}%'\n",
    "        )\n",
    "        \n",
    "        # 진행바 업데이트\n",
    "        epoch_pbar.set_postfix({\n",
    "            \"T_Loss\": f\"{train_loss:.4f}\",\n",
    "            \"V_Loss\": f\"{val_loss:.4f}\",\n",
    "            \"V_MAPE\": f'{val_metrics[\"mape\"]:.2f}%',\n",
    "            \"Best\": f\"{best_val_loss:.4f}\",\n",
    "            \"Patience\": f\"{patience_counter}/{patience}\"\n",
    "        })\n",
    "        \n",
    "        # 최고 모델 저장\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'train_metrics': train_metrics\n",
    "            }, save_path)\n",
    "            \n",
    "            logger.info(f\"  → Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # 조기 종료\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, device, model_path):\n",
    "    \"\"\"모델 평가 (구조 정보 포함)\"\"\"\n",
    "    logger.info(f\"모델 로드: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    padding_value = -999999.0\n",
    "    criterion = MaskedMSELoss(padding_value)\n",
    "    \n",
    "    # 구조화된 결과를 위한 리스트들\n",
    "    structured_predictions = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    logger.info(\"테스트 시작\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            continuous_data = batch[\"continuous_data\"].to(device)\n",
    "            categorical_data = batch[\"categorical_data\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            masks = batch[\"masks\"].to(device)\n",
    "            sequence_lengths = batch[\"sequence_lengths\"]\n",
    "            \n",
    "            # 구조 정보 추출\n",
    "            timekey_hrs = batch[\"timekey_hrs\"]\n",
    "            oper_ids_list = batch[\"oper_ids_list\"]\n",
    "            \n",
    "            predictions = model(continuous_data, categorical_data, masks, sequence_lengths)\n",
    "            loss = criterion(predictions, targets, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # CPU로 변환\n",
    "            predictions_cpu = predictions.cpu()\n",
    "            targets_cpu = targets.cpu()\n",
    "            masks_cpu = masks.cpu()\n",
    "            \n",
    "            # 배치 내 각 샘플에 대해 구조화된 결과 생성\n",
    "            batch_size = predictions_cpu.shape[0]\n",
    "            for sample_idx in range(batch_size):\n",
    "                timekey_hr = timekey_hrs[sample_idx]\n",
    "                oper_ids = oper_ids_list[sample_idx]\n",
    "                sample_predictions = predictions_cpu[sample_idx]\n",
    "                sample_targets = targets_cpu[sample_idx]\n",
    "                sample_masks = masks_cpu[sample_idx]\n",
    "                \n",
    "                # 각 시퀀스 위치에 대해\n",
    "                for seq_idx in range(sample_predictions.shape[0]):\n",
    "                    # 패딩되지 않은 위치만 처리\n",
    "                    if not sample_masks[seq_idx] and oper_ids[seq_idx] is not None:\n",
    "                        pred_val = sample_predictions[seq_idx].item()\n",
    "                        target_val = sample_targets[seq_idx].item()\n",
    "                        oper_id = oper_ids[seq_idx]\n",
    "                        \n",
    "                        structured_predictions.append({\n",
    "                            'timekey_hr': timekey_hr,\n",
    "                            'oper_id': oper_id,\n",
    "                            'predicted': pred_val,\n",
    "                            'actual': target_val\n",
    "                        })\n",
    "                        \n",
    "                        all_predictions.append(pred_val)\n",
    "                        all_targets.append(target_val)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "    \n",
    "    epsilon = 1e-8\n",
    "    abs_targets = np.abs(all_targets)\n",
    "    abs_errors = np.abs(all_predictions - all_targets)\n",
    "    safe_targets = np.maximum(abs_targets, epsilon)\n",
    "    mape = np.mean(abs_errors / safe_targets * 100)\n",
    "    \n",
    "    metrics = {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"valid_count\": len(all_predictions)\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"테스트 결과: RMSE={rmse:.4f}, MAE={mae:.4f}, MAPE={mape:.2f}%\")\n",
    "    logger.info(f\"구조화된 예측 결과: {len(structured_predictions):,}개\")\n",
    "    \n",
    "    return {\n",
    "        \"test_loss\": avg_loss,\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": all_predictions,\n",
    "        \"targets\": all_targets,\n",
    "        \"structured_predictions\": structured_predictions  # 추가\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 메인 실행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    import argparse\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description=\"시계열 시퀀스 모델링\")\n",
    "    parser.add_argument(\"--config-dir\", default=\"configs\", help=\"설정 파일 디렉토리\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"train\", \"eval\"], default=\"train\", help=\"실행 모드\")\n",
    "    parser.add_argument(\"--model-path\", default=None, help=\"평가용 모델 경로\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU 번호\")\n",
    "    parser.add_argument(\"--exp-name\", default=None, help=\"실험명\")\n",
    "    \n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    # 설정 로드\n",
    "    config = load_config(args.config_dir)\n",
    "    set_random_seeds(42)\n",
    "    \n",
    "    # 실험명 설정\n",
    "    if args.exp_name:\n",
    "        exp_name = args.exp_name\n",
    "    else:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_type = config.get(\"model_type\", \"lstm\")\n",
    "        exp_name = f\"{model_type}_{timestamp}\"\n",
    "    \n",
    "    # 저장 디렉토리\n",
    "    save_dir = config.get(\"save_dir\", \"models\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_save_path = os.path.join(save_dir, f\"{exp_name}.pth\")\n",
    "    \n",
    "    # 디바이스 설정\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    logger.info(\"데이터 로딩 중...\")\n",
    "    train_loader, val_loader, test_loader, categorical_processor = create_dataloaders(config)\n",
    "    \n",
    "    # 모델 생성\n",
    "    vocab_sizes = categorical_processor.get_vocab_sizes()\n",
    "    continuous_dim = len(config[\"continuous_columns\"])\n",
    "    \n",
    "    model = create_model(config, vocab_sizes, continuous_dim)\n",
    "    \n",
    "    if args.mode == \"train\":\n",
    "        # 훈련\n",
    "        logger.info(\"훈련 시작...\")\n",
    "        train_results = train_model(\n",
    "            model, train_loader, val_loader, config, device, model_save_path\n",
    "        )\n",
    "        \n",
    "        logger.info(\"훈련 완료, 테스트 시작...\")\n",
    "        test_results = evaluate_model(model, test_loader, device, model_save_path)\n",
    "        \n",
    "    else:\n",
    "        # 평가\n",
    "        if not args.model_path:\n",
    "            raise ValueError(\"--model-path must be provided in eval mode\")\n",
    "        test_results = evaluate_model(model, test_loader, device, args.model_path)\n",
    "    \n",
    "    # 결과 저장\n",
    "    results = {\n",
    "        \"exp_name\": exp_name,\n",
    "        \"config\": config,\n",
    "        \"test_metrics\": test_results[\"metrics\"],\n",
    "        \"model_info\": {\n",
    "            \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "            \"model_type\": config.get(\"model_type\", \"lstm\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    results_path = os.path.join(save_dir, f\"{exp_name}_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "    \n",
    "    # 예측 결과 저장\n",
    "    if \"structured_predictions\" in test_results and test_results[\"structured_predictions\"]:\n",
    "        # 구조화된 예측 결과 저장 (timekey_hr, oper_id 포함)\n",
    "        structured_df = pd.DataFrame(test_results[\"structured_predictions\"])\n",
    "        structured_df[\"error\"] = structured_df[\"predicted\"] - structured_df[\"actual\"]\n",
    "        structured_df[\"abs_error\"] = structured_df[\"error\"].abs()\n",
    "        structured_df[\"abs_percent_error\"] = (\n",
    "            structured_df[\"abs_error\"] / structured_df[\"actual\"].abs().clip(lower=1e-8) * 100\n",
    "        )\n",
    "        \n",
    "        # 구조화된 결과를 메인 예측 파일로 저장\n",
    "        predictions_path = os.path.join(save_dir, f\"{exp_name}_predictions.csv\")\n",
    "        structured_df.to_csv(predictions_path, index=False)\n",
    "        \n",
    "        logger.info(f\"  - 구조화된 예측 결과: {predictions_path}\")\n",
    "        logger.info(f\"  - 저장된 예측 개수: {len(structured_df):,}개\")\n",
    "        logger.info(f\"  - 고유한 timekey_hr: {structured_df['timekey_hr'].nunique()}개\")\n",
    "        logger.info(f\"  - 고유한 oper_id: {structured_df['oper_id'].nunique()}개\")\n",
    "        \n",
    "    else:\n",
    "        # 구조화된 정보가 없는 경우 기본 방식으로 저장 (호환성 유지)\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"actual\": test_results[\"targets\"],\n",
    "            \"predicted\": test_results[\"predictions\"],\n",
    "            \"residual\": test_results[\"targets\"] - test_results[\"predictions\"],\n",
    "            \"abs_error\": np.abs(test_results[\"targets\"] - test_results[\"predictions\"]),\n",
    "            \"abs_percent_error\": (\n",
    "                np.abs(test_results[\"targets\"] - test_results[\"predictions\"]) / \n",
    "                np.maximum(np.abs(test_results[\"targets\"]), 1e-8) * 100\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        predictions_path = os.path.join(save_dir, f\"{exp_name}_predictions.csv\")\n",
    "        predictions_df.to_csv(predictions_path, index=False)\n",
    "        \n",
    "        logger.info(f\"  - 기본 예측 결과: {predictions_path}\")\n",
    "        logger.info(f\"  - 저장된 예측 개수: {len(predictions_df):,}개\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 14:25:18,724 - INFO - Using device: cuda:0\n",
      "2025-08-31 14:25:18,728 - INFO - 데이터 로딩 중...\n",
      "2025-08-31 14:38:12,837 - INFO - 범주형 변수별 고유값 개수:\n",
      "2025-08-31 14:38:12,838 - INFO -   oper_group: 277개\n",
      "2025-08-31 14:38:12,839 - INFO -   days: 7개\n",
      "2025-08-31 14:38:12,839 - INFO -   shift: 3개\n",
      "2025-08-31 14:38:12,840 - INFO -   x1: 20개\n",
      "2025-08-31 14:38:17,881 - INFO - 시퀀스 데이터셋 구성 완료:\n",
      "2025-08-31 14:38:17,882 - INFO -   - 총 시퀀스 수: 2136\n",
      "2025-08-31 14:38:17,883 - INFO -   - 최대 시퀀스 길이: 50\n",
      "2025-08-31 14:38:17,883 - INFO -   - 특성 차원: 52\n",
      "2025-08-31 14:38:17,884 - INFO -   - 패딩값: -999999.0\n",
      "2025-08-31 14:38:20,385 - INFO - 시퀀스 데이터셋 구성 완료:\n",
      "2025-08-31 14:38:20,386 - INFO -   - 총 시퀀스 수: 2136\n",
      "2025-08-31 14:38:20,386 - INFO -   - 최대 시퀀스 길이: 50\n",
      "2025-08-31 14:38:20,387 - INFO -   - 특성 차원: 52\n",
      "2025-08-31 14:38:20,387 - INFO -   - 패딩값: -999999.0\n",
      "2025-08-31 14:38:23,052 - INFO - 시퀀스 데이터셋 구성 완료:\n",
      "2025-08-31 14:38:23,053 - INFO -   - 총 시퀀스 수: 2136\n",
      "2025-08-31 14:38:23,054 - INFO -   - 최대 시퀀스 길이: 50\n",
      "2025-08-31 14:38:23,054 - INFO -   - 특성 차원: 52\n",
      "2025-08-31 14:38:23,055 - INFO -   - 패딩값: -999999.0\n",
      "2025-08-31 14:38:23,056 - INFO - 데이터로더 생성 완료:\n",
      "2025-08-31 14:38:23,056 - INFO -   - 훈련 샘플: 2136\n",
      "2025-08-31 14:38:23,056 - INFO -   - 검증 샘플: 2136\n",
      "2025-08-31 14:38:23,057 - INFO -   - 테스트 샘플: 2136\n",
      "2025-08-31 14:38:23,057 - INFO -   - 배치 크기: 364\n",
      "2025-08-31 14:38:23,238 - INFO - 모델 생성 완료:\n",
      "2025-08-31 14:38:23,239 - INFO -   - 모델 타입: lstm\n",
      "2025-08-31 14:38:23,239 - INFO -   - 총 파라미터 수: 617,113\n",
      "2025-08-31 14:38:23,240 - INFO -   - 학습 가능한 파라미터 수: 617,113\n",
      "2025-08-31 14:38:23,240 - INFO - 훈련 시작...\n",
      "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2025-08-31 14:38:24,448 - INFO - 훈련 시작: 100 에폭, 학습률 0.001\n",
      "Training Progress:   0%|          | 0/100 [00:00<?, ?it/s]2025-08-31 14:38:26,300 - INFO - Epoch   1: Train Loss=0.2122, Val Loss=0.0124, Train MAPE=85057.35%, Val MAPE=43842.21%\n",
      "Training Progress:   0%|          | 0/100 [00:01<?, ?it/s, T_Loss=0.2122, V_Loss=0.0124, V_MAPE=43842.21%, Best=inf, Patience=0/20]2025-08-31 14:38:26,326 - INFO -   → Best model saved! (Val Loss: 0.0124)\n",
      "Training Progress:   1%|          | 1/100 [00:01<03:05,  1.88s/it, T_Loss=0.2122, V_Loss=0.0124, V_MAPE=43842.21%, Best=inf, Patience=0/20]2025-08-31 14:38:27,695 - INFO - Epoch   2: Train Loss=0.1530, Val Loss=0.0283, Train MAPE=266213.09%, Val MAPE=38449.30%\n",
      "Training Progress:   2%|▏         | 2/100 [00:03<02:34,  1.58s/it, T_Loss=0.1530, V_Loss=0.0283, V_MAPE=38449.30%, Best=0.0124, Patience=0/20]2025-08-31 14:38:29,003 - INFO - Epoch   3: Train Loss=0.1375, Val Loss=0.0125, Train MAPE=248363.36%, Val MAPE=55303.24%\n",
      "Training Progress:   3%|▎         | 3/100 [00:04<02:21,  1.45s/it, T_Loss=0.1375, V_Loss=0.0125, V_MAPE=55303.24%, Best=0.0124, Patience=1/20]2025-08-31 14:38:31,007 - INFO - Epoch   4: Train Loss=0.1291, Val Loss=0.0113, Train MAPE=239184.33%, Val MAPE=47051.70%\n",
      "Training Progress:   3%|▎         | 3/100 [00:06<02:21,  1.45s/it, T_Loss=0.1291, V_Loss=0.0113, V_MAPE=47051.70%, Best=0.0124, Patience=2/20]2025-08-31 14:38:31,078 - INFO -   → Best model saved! (Val Loss: 0.0113)\n",
      "Training Progress:   4%|▍         | 4/100 [00:06<02:43,  1.70s/it, T_Loss=0.1291, V_Loss=0.0113, V_MAPE=47051.70%, Best=0.0124, Patience=2/20]2025-08-31 14:38:32,435 - INFO - Epoch   5: Train Loss=0.1258, Val Loss=0.0109, Train MAPE=276576.88%, Val MAPE=49548.10%\n",
      "Training Progress:   4%|▍         | 4/100 [00:07<02:43,  1.70s/it, T_Loss=0.1258, V_Loss=0.0109, V_MAPE=49548.10%, Best=0.0113, Patience=0/20]2025-08-31 14:38:32,459 - INFO -   → Best model saved! (Val Loss: 0.0109)\n",
      "Training Progress:   5%|▌         | 5/100 [00:08<02:30,  1.58s/it, T_Loss=0.1258, V_Loss=0.0109, V_MAPE=49548.10%, Best=0.0113, Patience=0/20]2025-08-31 14:38:33,797 - INFO - Epoch   6: Train Loss=0.1254, Val Loss=0.0122, Train MAPE=291849.16%, Val MAPE=56635.50%\n",
      "Training Progress:   6%|▌         | 6/100 [00:09<02:21,  1.50s/it, T_Loss=0.1254, V_Loss=0.0122, V_MAPE=56635.50%, Best=0.0109, Patience=0/20]2025-08-31 14:38:35,124 - INFO - Epoch   7: Train Loss=0.1249, Val Loss=0.0113, Train MAPE=268570.69%, Val MAPE=49640.14%\n",
      "Training Progress:   7%|▋         | 7/100 [00:10<02:14,  1.44s/it, T_Loss=0.1249, V_Loss=0.0113, V_MAPE=49640.14%, Best=0.0109, Patience=1/20]2025-08-31 14:38:36,500 - INFO - Epoch   8: Train Loss=0.1246, Val Loss=0.0113, Train MAPE=256918.30%, Val MAPE=53991.79%\n",
      "Training Progress:   8%|▊         | 8/100 [00:12<02:10,  1.42s/it, T_Loss=0.1246, V_Loss=0.0113, V_MAPE=53991.79%, Best=0.0109, Patience=2/20]2025-08-31 14:38:37,809 - INFO - Epoch   9: Train Loss=0.1241, Val Loss=0.0110, Train MAPE=249341.88%, Val MAPE=48591.14%\n",
      "Training Progress:   9%|▉         | 9/100 [00:13<02:06,  1.39s/it, T_Loss=0.1241, V_Loss=0.0110, V_MAPE=48591.14%, Best=0.0109, Patience=3/20]2025-08-31 14:38:39,157 - INFO - Epoch  10: Train Loss=0.1235, Val Loss=0.0116, Train MAPE=286960.72%, Val MAPE=53061.81%\n",
      "Training Progress:  10%|█         | 10/100 [00:14<02:03,  1.37s/it, T_Loss=0.1235, V_Loss=0.0116, V_MAPE=53061.81%, Best=0.0109, Patience=4/20]2025-08-31 14:38:40,497 - INFO - Epoch  11: Train Loss=0.1234, Val Loss=0.0113, Train MAPE=268388.28%, Val MAPE=49638.86%\n",
      "Training Progress:  11%|█         | 11/100 [00:16<02:01,  1.36s/it, T_Loss=0.1234, V_Loss=0.0113, V_MAPE=49638.86%, Best=0.0109, Patience=5/20]2025-08-31 14:38:41,828 - INFO - Epoch  12: Train Loss=0.1227, Val Loss=0.0111, Train MAPE=281134.53%, Val MAPE=49044.41%\n",
      "Training Progress:  12%|█▏        | 12/100 [00:17<01:59,  1.35s/it, T_Loss=0.1227, V_Loss=0.0111, V_MAPE=49044.41%, Best=0.0109, Patience=6/20]2025-08-31 14:38:43,142 - INFO - Epoch  13: Train Loss=0.1221, Val Loss=0.0118, Train MAPE=312178.00%, Val MAPE=50572.58%\n",
      "Training Progress:  13%|█▎        | 13/100 [00:18<01:56,  1.34s/it, T_Loss=0.1221, V_Loss=0.0118, V_MAPE=50572.58%, Best=0.0109, Patience=7/20]2025-08-31 14:38:44,466 - INFO - Epoch  14: Train Loss=0.1222, Val Loss=0.0116, Train MAPE=254217.98%, Val MAPE=55119.02%\n",
      "Training Progress:  14%|█▍        | 14/100 [00:20<01:54,  1.34s/it, T_Loss=0.1222, V_Loss=0.0116, V_MAPE=55119.02%, Best=0.0109, Patience=8/20]2025-08-31 14:38:45,792 - INFO - Epoch  15: Train Loss=0.1231, Val Loss=0.0116, Train MAPE=273660.66%, Val MAPE=52123.32%\n",
      "Training Progress:  15%|█▌        | 15/100 [00:21<01:53,  1.33s/it, T_Loss=0.1231, V_Loss=0.0116, V_MAPE=52123.32%, Best=0.0109, Patience=9/20]2025-08-31 14:38:47,163 - INFO - Epoch  16: Train Loss=0.1222, Val Loss=0.0116, Train MAPE=267461.41%, Val MAPE=52613.26%\n",
      "Training Progress:  16%|█▌        | 16/100 [00:22<01:52,  1.34s/it, T_Loss=0.1222, V_Loss=0.0116, V_MAPE=52613.26%, Best=0.0109, Patience=10/20]2025-08-31 14:38:48,627 - INFO - Epoch  17: Train Loss=0.1211, Val Loss=0.0115, Train MAPE=283100.66%, Val MAPE=52253.86%\n",
      "Training Progress:  17%|█▋        | 17/100 [00:24<01:54,  1.38s/it, T_Loss=0.1211, V_Loss=0.0115, V_MAPE=52253.86%, Best=0.0109, Patience=11/20]2025-08-31 14:38:49,973 - INFO - Epoch  18: Train Loss=0.1203, Val Loss=0.0111, Train MAPE=275078.34%, Val MAPE=48055.20%\n",
      "Training Progress:  18%|█▊        | 18/100 [00:25<01:52,  1.37s/it, T_Loss=0.1203, V_Loss=0.0111, V_MAPE=48055.20%, Best=0.0109, Patience=12/20]2025-08-31 14:38:51,355 - INFO - Epoch  19: Train Loss=0.1202, Val Loss=0.0114, Train MAPE=275046.00%, Val MAPE=53828.34%\n",
      "Training Progress:  19%|█▉        | 19/100 [00:26<01:51,  1.37s/it, T_Loss=0.1202, V_Loss=0.0114, V_MAPE=53828.34%, Best=0.0109, Patience=13/20]2025-08-31 14:38:52,717 - INFO - Epoch  20: Train Loss=0.1183, Val Loss=0.0113, Train MAPE=291573.38%, Val MAPE=49848.51%\n",
      "Training Progress:  20%|██        | 20/100 [00:28<01:49,  1.37s/it, T_Loss=0.1183, V_Loss=0.0113, V_MAPE=49848.51%, Best=0.0109, Patience=14/20]2025-08-31 14:38:54,100 - INFO - Epoch  21: Train Loss=0.1182, Val Loss=0.0113, Train MAPE=324644.19%, Val MAPE=49536.65%\n",
      "Training Progress:  21%|██        | 21/100 [00:29<01:48,  1.37s/it, T_Loss=0.1182, V_Loss=0.0113, V_MAPE=49536.65%, Best=0.0109, Patience=15/20]2025-08-31 14:38:55,437 - INFO - Epoch  22: Train Loss=0.1178, Val Loss=0.0114, Train MAPE=298139.69%, Val MAPE=49774.39%\n",
      "Training Progress:  22%|██▏       | 22/100 [00:30<01:46,  1.36s/it, T_Loss=0.1178, V_Loss=0.0114, V_MAPE=49774.39%, Best=0.0109, Patience=16/20]2025-08-31 14:38:56,763 - INFO - Epoch  23: Train Loss=0.1168, Val Loss=0.0115, Train MAPE=303203.00%, Val MAPE=48601.57%\n",
      "Training Progress:  23%|██▎       | 23/100 [00:32<01:44,  1.35s/it, T_Loss=0.1168, V_Loss=0.0115, V_MAPE=48601.57%, Best=0.0109, Patience=17/20]2025-08-31 14:38:58,101 - INFO - Epoch  24: Train Loss=0.1180, Val Loss=0.0114, Train MAPE=286847.53%, Val MAPE=48936.80%\n",
      "Training Progress:  24%|██▍       | 24/100 [00:33<01:42,  1.35s/it, T_Loss=0.1180, V_Loss=0.0114, V_MAPE=48936.80%, Best=0.0109, Patience=18/20]2025-08-31 14:38:59,412 - INFO - Epoch  25: Train Loss=0.1168, Val Loss=0.0113, Train MAPE=245007.88%, Val MAPE=52410.64%\n",
      "Training Progress:  24%|██▍       | 24/100 [00:34<01:42,  1.35s/it, T_Loss=0.1168, V_Loss=0.0113, V_MAPE=52410.64%, Best=0.0109, Patience=19/20]2025-08-31 14:38:59,414 - INFO - Early stopping at epoch 25\n",
      "Training Progress:  24%|██▍       | 24/100 [00:34<01:50,  1.46s/it, T_Loss=0.1168, V_Loss=0.0113, V_MAPE=52410.64%, Best=0.0109, Patience=19/20]\n",
      "2025-08-31 14:38:59,417 - INFO - 훈련 완료, 테스트 시작...\n",
      "2025-08-31 14:38:59,418 - INFO - 모델 로드: 250831_test/lstm_20250831_142503.pth\n",
      "/tmp/ipykernel_614454/118785919.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n",
      "2025-08-31 14:38:59,443 - INFO - 테스트 시작\n",
      "Testing: 100%|██████████| 6/6 [00:01<00:00,  5.07it/s]\n",
      "2025-08-31 14:39:00,643 - INFO - 테스트 결과: RMSE=0.1112, MAE=0.0660, MAPE=27890.64%\n",
      "2025-08-31 14:39:00,644 - INFO - 구조화된 예측 결과: 106,784개\n",
      "2025-08-31 14:39:01,351 - INFO -   - 구조화된 예측 결과: 250831_test/lstm_20250831_142503_predictions.csv\n",
      "2025-08-31 14:39:01,352 - INFO -   - 저장된 예측 개수: 106,784개\n",
      "2025-08-31 14:39:01,354 - INFO -   - 고유한 timekey_hr: 2136개\n",
      "2025-08-31 14:39:01,361 - INFO -   - 고유한 oper_id: 84개\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 하이퍼파라미터 튜닝 가이드\n",
    "\n",
    "### 1. 모델별 권장 설정\n",
    "\n",
    "#### **LSTM/GRU (기본)**\n",
    "```yaml\n",
    "model_type: \"lstm\"\n",
    "hidden_dim: 128\n",
    "num_layers: 2\n",
    "bidirectional: true\n",
    "dropout: 0.1\n",
    "```\n",
    "\n",
    "#### **LSTM + Self-Attention**\n",
    "```yaml\n",
    "model_type: \"lstm_attention\"\n",
    "hidden_dim: 128\n",
    "num_layers: 2\n",
    "num_attention_heads: 8\n",
    "bidirectional: true\n",
    "dropout: 0.2\n",
    "```\n",
    "\n",
    "#### **CNN 1D**\n",
    "```yaml\n",
    "model_type: \"cnn1d\"\n",
    "kernel_sizes: [3, 5, 7]\n",
    "num_filters: 64\n",
    "dropout: 0.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 성능 최적화 팁\n",
    "\n",
    "#### **과적합 방지**\n",
    "```yaml\n",
    "dropout: 0.3\n",
    "patience: 10\n",
    "learning_rate: 0.0001\n",
    "```\n",
    "\n",
    "#### **빠른 수렴**\n",
    "```yaml\n",
    "learning_rate: 0.01\n",
    "scheduler_patience: 5\n",
    "num_attention_heads: 4  # Attention 모델의 경우\n",
    "```\n",
    "\n",
    "#### **메모리 최적화**\n",
    "```yaml\n",
    "max_sequence_length: 30\n",
    "batch_size: 16\n",
    "num_layers: 1\n",
    "bidirectional: false\n",
    "```\n",
    "\n",
    "## 📝 실행 예시\n",
    "\n",
    "### 기본 훈련\n",
    "```bash\n",
    "python main.py --mode train --gpu 0\n",
    "```\n",
    "\n",
    "### 특정 모델 훈련\n",
    "```bash\n",
    "# LSTM with Attention\n",
    "python main.py --mode train --exp-name lstm_attention_exp1\n",
    "\n",
    "# CNN 1D  \n",
    "python main.py --mode train --exp-name cnn1d_exp1\n",
    "\n",
    "# 설정 변경 후\n",
    "python main.py --mode train --config-dir ./my_configs\n",
    "```\n",
    "\n",
    "### 모델 평가\n",
    "```bash\n",
    "python main.py --mode eval --model-path ./models/lstm_20241201_120000.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**참고**: 이 프로젝트는 시퀀스 모델링 관점에서 공정 데이터를 처리하며, timekey_hr 내 oper_id 순서를 고려한 sequence-to-sequence 예측을 수행합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
