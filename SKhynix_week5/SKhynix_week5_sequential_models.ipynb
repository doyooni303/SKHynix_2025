{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❗ 5. SKHynix PBL 시계열 시퀀스 모델링 ❗\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 개요\n",
    "\n",
    "시간대(timekey_hr) 내에서 공정 순서(oper_id)를 고려한 시퀀스 기반 TAT 예측 모델입니다. 동일한 timekey_hr 내의 oper_id들을 순서대로 정렬하여 시퀀스 데이터로 구성하고, 각 oper별 개별 예측(sequence-to-sequence)을 수행합니다.\n",
    "\n",
    "**데이터 구조**: `[batch_size, sequence_length, feature_dim]`\n",
    "- **sequence_length**: timekey_hr 내 최대 oper_id 개수 (하이퍼파라미터)\n",
    "- **feature_dim**: 연속형 변수 개수 + 범주형 변수 개수 × 임베딩 차원\n",
    "\n",
    "**지원 모델**:\n",
    "1. **RNN/LSTM/GRU**: 기본 순환 신경망\n",
    "2. **RNN + Self-Attention**: 순환 신경망에 어텐션 메커니즘 추가\n",
    "3. **CNN 1D**: 다중 커널 1차원 합성곱 신경망\n",
    "4. **Transformer Encoder**: Transformer Encoder 기반의 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 환경 설정 및 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 유틸리티 함수들\n",
    "\n",
    "### 설정 로딩 및 시드 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_dir: str = \"configs\") -> Dict:\n",
    "    \"\"\"YAML 설정 파일들을 통합하여 로드\"\"\"\n",
    "    configs = {}\n",
    "    config_files = [\"dataset\", \"model\", \"training\"]\n",
    "\n",
    "    for file in config_files:\n",
    "        config_path = os.path.join(config_dir, f\"{file}.yaml\")\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            configs.update(config)\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def set_random_seeds(seed: int = 42):\n",
    "    \"\"\"재현성을 위한 랜덤 시드 설정\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Global logger 변수\n",
    "logger = None\n",
    "\n",
    "def setup_logging(log_file: str = \"training.log\"):\n",
    "    \"\"\"로깅 설정 및 global logger 설정\"\"\"\n",
    "    global logger\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True  # 기존 핸들러 제거 후 새로 설정\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 범주형 데이터 처리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalProcessor:\n",
    "    \"\"\"범주형 변수 임베딩을 위한 처리기\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 8):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_encoders = {}\n",
    "        self.vocab_sizes = {}\n",
    "        self.categorical_columns = []\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, categorical_columns: List[str]):\n",
    "        \"\"\"전체 데이터에 대해 범주형 인코더 학습\"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            unique_values = df[col].astype(str).unique()\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(unique_values)\n",
    "            \n",
    "            self.label_encoders[col] = encoder\n",
    "            self.vocab_sizes[col] = len(encoder.classes_)\n",
    "        \n",
    "        logger.info(f\"범주형 변수별 고유값 개수:\")\n",
    "        for col in categorical_columns:\n",
    "            logger.info(f\"  {col}: {self.vocab_sizes[col]}개\")\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"DataFrame의 범주형 컬럼들을 숫자로 변환\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            df_encoded[col] = self.label_encoders[col].transform(\n",
    "                df_encoded[col].astype(str)\n",
    "            )\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def get_vocab_sizes(self) -> List[int]:\n",
    "        \"\"\"각 범주형 변수의 vocab_size 리스트 반환\"\"\"\n",
    "        return [self.vocab_sizes[col] for col in self.categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ 시퀀스 데이터셋 클래스\n",
    "\n",
    "### 메인 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "def extract_oper_number(oper_id):\n",
    "    \"\"\"oper_id에서 숫자 부분 추출 (예: 'oper123' -> 123)\"\"\"\n",
    "    match = re.search(r'\\d+', str(oper_id))\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "\n",
    "class CategoricalProcessor:\n",
    "    \"\"\"통합된 범주형 변수 처리기 - 모든 범주형 변수를 하나의 vocabulary로 통합\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_columns: List[str], categories: List[int], embedding_dim: int = 8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            categorical_columns: 범주형 컬럼명 리스트 (예: [\"oper_group\", \"days\", \"shift\", \"x1\"])\n",
    "            categories: 각 범주형 변수의 카테고리 수 (예: [277, 7, 3, 20])\n",
    "            embedding_dim: 임베딩 차원\n",
    "        \"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.categories = categories\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # 각 변수별 인코더와 오프셋 계산\n",
    "        self.label_encoders = {}\n",
    "        self.category_offsets = {}  # 각 변수의 시작 인덱스\n",
    "        self.category_ranges = {}   # 각 변수의 (start, end) 범위\n",
    "        \n",
    "        # 통합 vocabulary 크기 계산\n",
    "        self.total_vocab_size = sum(categories)\n",
    "        \n",
    "        logger.info(f\"통합된 범주형 변수 설정:\")\n",
    "        logger.info(f\"  - 변수별 카테고리 수: {dict(zip(categorical_columns, categories))}\")\n",
    "        logger.info(f\"  - 총 vocabulary 크기: {self.total_vocab_size}\")\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        \"\"\"전체 데이터에 대해 통합 범주형 인코더 학습\"\"\"\n",
    "        \n",
    "        current_offset = 0\n",
    "        \n",
    "        for i, col in enumerate(self.categorical_columns):\n",
    "            # 해당 변수의 고유값들 추출\n",
    "            unique_values = df[col].astype(str).unique()\n",
    "            \n",
    "            # LabelEncoder 학습 (0부터 시작)\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(unique_values)\n",
    "            \n",
    "            # 실제 고유값 개수 확인\n",
    "            actual_vocab_size = len(encoder.classes_)\n",
    "            expected_vocab_size = self.categories[i]\n",
    "            \n",
    "            if actual_vocab_size != expected_vocab_size:\n",
    "                logger.info(f\"  경고: {col}의 실제 카테고리 수({actual_vocab_size})가 설정값({expected_vocab_size})과 다릅니다\")\n",
    "                # 실제값으로 업데이트\n",
    "                self.categories[i] = actual_vocab_size\n",
    "            \n",
    "            self.label_encoders[col] = encoder\n",
    "            self.category_offsets[col] = current_offset\n",
    "            self.category_ranges[col] = (current_offset, current_offset + actual_vocab_size)\n",
    "            \n",
    "            logger.info(f\"  - {col}: 인덱스 {current_offset}~{current_offset + actual_vocab_size - 1} ({actual_vocab_size}개)\")\n",
    "            \n",
    "            current_offset += actual_vocab_size\n",
    "        \n",
    "        # 총 vocabulary 크기 재계산\n",
    "        self.total_vocab_size = current_offset\n",
    "        logger.info(f\"  - 최종 통합 vocabulary 크기: {self.total_vocab_size}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"DataFrame의 범주형 컬럼들을 통합 인덱스로 변환\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            # 먼저 LabelEncoder로 0부터 시작하는 인덱스로 변환\n",
    "            encoded_values = self.label_encoders[col].transform(df_encoded[col].astype(str))\n",
    "            \n",
    "            # 오프셋 추가하여 통합 vocabulary 인덱스로 변환\n",
    "            df_encoded[col] = encoded_values + self.category_offsets[col]\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def get_total_vocab_size(self) -> int:\n",
    "        \"\"\"통합된 총 vocabulary 크기 반환\"\"\"\n",
    "        return self.total_vocab_size\n",
    "    \n",
    "    def get_category_info(self) -> Dict:\n",
    "        \"\"\"범주형 변수 정보 딕셔너리 반환\"\"\"\n",
    "        return {\n",
    "            'categorical_columns': self.categorical_columns,\n",
    "            'categories': self.categories,\n",
    "            'category_offsets': self.category_offsets,\n",
    "            'category_ranges': self.category_ranges,\n",
    "            'total_vocab_size': self.total_vocab_size\n",
    "        }\n",
    "    \n",
    "    def decode_categorical_value(self, encoded_value: int) -> Tuple[str, str]:\n",
    "        \"\"\"통합 인덱스를 원래 (변수명, 값) 으로 디코딩\"\"\"\n",
    "        for col in self.categorical_columns:\n",
    "            start, end = self.category_ranges[col]\n",
    "            if start <= encoded_value < end:\n",
    "                # 오프셋 제거하여 원래 LabelEncoder 인덱스로 변환\n",
    "                original_idx = encoded_value - self.category_offsets[col]\n",
    "                # 원래 값 복원\n",
    "                original_value = self.label_encoders[col].inverse_transform([original_idx])[0]\n",
    "                return col, original_value\n",
    "        \n",
    "        return \"unknown\", \"unknown\"\n",
    "\n",
    "\n",
    "class SequenceOperDataset(Dataset):\n",
    "    \"\"\"시퀀스 기반 oper 데이터셋 - 통합 임베딩 + Window sliding\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        categorical_columns: List[str],\n",
    "        continuous_columns: List[str],\n",
    "        categories: List[int],  # 새로운 파라미터\n",
    "        target_column: str = \"y\",\n",
    "        categorical_processor: Optional[CategoricalProcessor] = None,\n",
    "        window_size: int = 30,\n",
    "        stride: int = None,\n",
    "        embedding_dim: int = 8,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        self.df = df.copy()\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.continuous_columns = continuous_columns\n",
    "        self.categories = categories\n",
    "        self.target_column = target_column\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride if stride is not None else window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # 통합 범주형 데이터 처리기 설정\n",
    "        if categorical_processor is None:\n",
    "            self.categorical_processor = CategoricalProcessor(\n",
    "                categorical_columns, categories, embedding_dim\n",
    "            )\n",
    "            self.categorical_processor.fit(df)\n",
    "        else:\n",
    "            self.categorical_processor = categorical_processor\n",
    "        \n",
    "        # 통합 임베딩 레이어 생성 (학습 가능)\n",
    "        total_vocab_size = self.categorical_processor.get_total_vocab_size()\n",
    "        self.categorical_embedding = nn.Embedding(\n",
    "            total_vocab_size, embedding_dim, padding_idx=0\n",
    "        )\n",
    "        \n",
    "        # 데이터 전처리 및 시퀀스 생성\n",
    "        self._preprocess_data()\n",
    "        self._create_windowed_sequences()\n",
    "        \n",
    "        logger.info(f\"시퀀스 데이터셋 구성 완료:\")\n",
    "        logger.info(f\"  - 총 샘플 수: {len(self.samples)}\")\n",
    "        logger.info(f\"  - Window 크기: {window_size}\")\n",
    "        logger.info(f\"  - Stride: {self.stride}\")\n",
    "        logger.info(f\"  - 연속형 차원: {len(continuous_columns)}\")\n",
    "        logger.info(f\"  - 범주형 변수 수: {len(categorical_columns)}\")\n",
    "        logger.info(f\"  - 임베딩 차원: {embedding_dim}\")\n",
    "        logger.info(f\"  - 최종 특성 차원: {len(continuous_columns) + len(categorical_columns) * embedding_dim}\")\n",
    "        logger.info(f\"  - 패딩값: {padding_value}\")\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        # timekey_hr에서 날짜(day) 추출\n",
    "        self.df['date'] = (self.df['timekey_hr'] // 100).astype(int)\n",
    "        \n",
    "        # 통합 범주형 데이터 인코딩\n",
    "        if self.categorical_columns:\n",
    "            self.df = self.categorical_processor.transform(self.df)\n",
    "        \n",
    "        # 최종 특성 차원 계산\n",
    "        self.continuous_dim = len(self.continuous_columns)\n",
    "        self.categorical_dim = len(self.categorical_columns) * self.embedding_dim\n",
    "        self.feature_dim = self.continuous_dim + self.categorical_dim\n",
    "    \n",
    "    def _create_windowed_sequences(self):\n",
    "        \"\"\"timekey_hr별로 window sliding하여 시퀀스 생성\"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        # timekey_hr별로 그룹화\n",
    "        grouped = self.df.groupby('timekey_hr')\n",
    "        \n",
    "        for timekey_hr, group in grouped:\n",
    "            # oper_id 순서로 정렬\n",
    "            group_sorted = group.iloc[group['oper_id'].map(extract_oper_number).argsort()].reset_index(drop=True)\n",
    "            \n",
    "            if len(group_sorted) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Window sliding\n",
    "            for start_idx in range(0, len(group_sorted), self.stride):\n",
    "                end_idx = start_idx + self.window_size\n",
    "                \n",
    "                # 윈도우 데이터 추출\n",
    "                window_data = group_sorted.iloc[start_idx:min(end_idx, len(group_sorted))]\n",
    "                actual_length = len(window_data)\n",
    "                \n",
    "                if actual_length == 0:\n",
    "                    continue\n",
    "                \n",
    "                # 연속형 데이터 추출 및 패딩\n",
    "                if self.continuous_columns:\n",
    "                    continuous_data = window_data[self.continuous_columns].values\n",
    "                    if actual_length < self.window_size:\n",
    "                        padding_rows = self.window_size - actual_length\n",
    "                        padding_matrix = np.full(\n",
    "                            (padding_rows, len(self.continuous_columns)), \n",
    "                            self.padding_value, \n",
    "                            dtype=np.float32\n",
    "                        )\n",
    "                        continuous_data = np.vstack([continuous_data, padding_matrix])\n",
    "                    continuous_data = continuous_data.astype(np.float32)\n",
    "                else:\n",
    "                    continuous_data = np.empty((self.window_size, 0), dtype=np.float32)\n",
    "                \n",
    "                # 범주형 데이터 추출 및 패딩 (통합 인덱스, 1부터 시작)\n",
    "                if self.categorical_columns:\n",
    "                    categorical_data = window_data[self.categorical_columns].values\n",
    "                    if actual_length < self.window_size:\n",
    "                        padding_rows = self.window_size - actual_length\n",
    "                        # 패딩은 0으로 (패딩용 인덱스)\n",
    "                        padding_matrix = np.zeros(\n",
    "                            (padding_rows, len(self.categorical_columns)), \n",
    "                            dtype=np.int64\n",
    "                        )\n",
    "                        categorical_data = np.vstack([categorical_data, padding_matrix])\n",
    "                    categorical_data = categorical_data.astype(np.int64)\n",
    "                else:\n",
    "                    categorical_data = np.empty((self.window_size, 0), dtype=np.int64)\n",
    "                \n",
    "                # 타겟 데이터 추출 및 패딩\n",
    "                target_data = window_data[self.target_column].values\n",
    "                if actual_length < self.window_size:\n",
    "                    padding_rows = self.window_size - actual_length\n",
    "                    padding_targets = np.full(padding_rows, self.padding_value, dtype=np.float32)\n",
    "                    target_data = np.hstack([target_data, padding_targets])\n",
    "                target_data = target_data.astype(np.float32)\n",
    "                \n",
    "                # 마스크 생성 (True = 패딩된 위치)\n",
    "                mask = np.zeros(self.window_size, dtype=bool)\n",
    "                if actual_length < self.window_size:\n",
    "                    mask[actual_length:] = True\n",
    "                \n",
    "                # oper_id 정보 (전체 리스트)\n",
    "                oper_ids = window_data['oper_id'].values.tolist()\n",
    "                if actual_length < self.window_size:\n",
    "                    oper_ids.extend([None] * (self.window_size - actual_length))\n",
    "                \n",
    "                sample_info = {\n",
    "                    'timekey_hr': timekey_hr,\n",
    "                    'oper_ids_list': oper_ids,\n",
    "                    'continuous_data': continuous_data,\n",
    "                    'categorical_data': categorical_data,\n",
    "                    'target_data': target_data,\n",
    "                    'mask': mask,\n",
    "                    'actual_length': actual_length,\n",
    "                    'first_oper_id': window_data['oper_id'].iloc[0],\n",
    "                    'last_oper_id': window_data['oper_id'].iloc[-1]\n",
    "                }\n",
    "                \n",
    "                self.samples.append(sample_info)\n",
    "        \n",
    "        logger.info(f\"Window sliding 결과:\")\n",
    "        logger.info(f\"  - 총 샘플 수: {len(self.samples)}\")\n",
    "        if self.samples:\n",
    "            actual_lengths = [sample['actual_length'] for sample in self.samples]\n",
    "            logger.info(f\"  - 평균 실제 길이: {np.mean(actual_lengths):.1f}\")\n",
    "            logger.info(f\"  - 최대 실제 길이: {np.max(actual_lengths)}\")\n",
    "            logger.info(f\"  - 최소 실제 길이: {np.min(actual_lengths)}\")\n",
    "            \n",
    "            timekey_hrs = [sample['timekey_hr'] for sample in self.samples]\n",
    "            unique_timekey_hrs = len(set(timekey_hrs))\n",
    "            logger.info(f\"  - 고유한 timekey_hr: {unique_timekey_hrs}개\")\n",
    "            logger.info(f\"  - timekey_hr당 평균 샘플 수: {len(self.samples)/unique_timekey_hrs:.1f}개\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     sample = self.samples[idx]\n",
    "        \n",
    "    #     # 연속형 데이터\n",
    "    #     continuous_data = torch.tensor(sample['continuous_data'])  # [window_size, continuous_dim]\n",
    "        \n",
    "    #     # 범주형 데이터 → 임베딩 적용 → flatten\n",
    "    #     categorical_data = torch.tensor(sample['categorical_data'])  # [window_size, num_categorical]\n",
    "        \n",
    "    #     # 임베딩 적용: [window_size, num_categorical, embed_dim]\n",
    "    #     with torch.no_grad():  # 여기서는 gradient 계산 안함 (forward에서 계산)\n",
    "    #         categorical_embedded = self.categorical_embedding(categorical_data)\n",
    "        \n",
    "    #     # Flatten: [window_size, num_categorical * embed_dim]\n",
    "    #     window_size, num_categorical, embed_dim = categorical_embedded.shape\n",
    "    #     categorical_flattened = categorical_embedded.view(window_size, num_categorical * embed_dim)\n",
    "        \n",
    "    #     # 연속형 + 범주형 결합: [window_size, total_feature_dim]\n",
    "    #     if continuous_data.numel() > 0:  # 연속형 변수가 있는 경우\n",
    "    #         combined_features = torch.cat([continuous_data, categorical_flattened], dim=-1)\n",
    "    #     else:  # 연속형 변수가 없는 경우\n",
    "    #         combined_features = categorical_flattened\n",
    "        \n",
    "    #     return {\n",
    "    #         'features': combined_features,  # 최종 결합된 특성\n",
    "    #         'targets': torch.tensor(sample['target_data']),\n",
    "    #         'masks': torch.tensor(sample['mask']),\n",
    "    #         'actual_length': sample['actual_length'],\n",
    "    #         'timekey_hr': sample['timekey_hr'],\n",
    "    #         'oper_ids_list': sample['oper_ids_list'],\n",
    "    #         'first_oper_id': sample['first_oper_id'],\n",
    "    #         'last_oper_id': sample['last_oper_id'],\n",
    "    #         # 디버깅용 원본 데이터도 포함\n",
    "    #         'continuous_data': continuous_data,\n",
    "    #         'categorical_data': categorical_data\n",
    "    #     }\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"디버깅 버전의 __getitem__ 메소드\"\"\"\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "\n",
    "        continuous_data = torch.tensor(sample['continuous_data'])\n",
    "        categorical_data = torch.tensor(sample['categorical_data'])\n",
    "        \n",
    "        # 임베딩 적용 전 범위 체크\n",
    "        max_index = categorical_data.max()\n",
    "        vocab_size = self.categorical_embedding.num_embeddings\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            categorical_embedded = self.categorical_embedding(categorical_data)\n",
    "        \n",
    "        # Flatten\n",
    "        window_size, num_categorical, embed_dim = categorical_embedded.shape\n",
    "        categorical_flattened = categorical_embedded.view(window_size, num_categorical * embed_dim)\n",
    "        \n",
    "        if continuous_data.numel() > 0:\n",
    "            combined_features = torch.cat([continuous_data, categorical_flattened], dim=-1)\n",
    "        else:\n",
    "            combined_features = categorical_flattened\n",
    "        \n",
    "        targets = torch.tensor(sample['target_data'])\n",
    "\n",
    "        # # # 디버깅 정보 (처음 몇 개 샘플만)\n",
    "        # debug_mode = idx < 3\n",
    "        # if debug_mode:\n",
    "        #     print(f\"\\n=== 샘플 {idx} 디버깅 ===\")\n",
    "        #     print(f\"timekey_hr: {sample['timekey_hr']}\")\n",
    "        #     print(f\"actual_length: {sample['actual_length']}\")\n",
    "\n",
    "        # # 1. 연속형 데이터 확인\n",
    "        #     print(f\"연속형 데이터: {continuous_data.shape}\")\n",
    "        #     print(f\"  NaN: {torch.isnan(continuous_data).sum()}\")\n",
    "        #     print(f\"  Inf: {torch.isinf(continuous_data).sum()}\")\n",
    "        #     print(f\"  범위: [{continuous_data.min():.4f}, {continuous_data.max():.4f}]\")\n",
    "        \n",
    "        # # 2. 범주형 데이터 확인  \n",
    "        #     print(f\"범주형 데이터: {categorical_data.shape}\")\n",
    "        #     print(f\"  값 범위: [{categorical_data.min()}, {categorical_data.max()}]\")\n",
    "        #     print(f\"  고유값: {torch.unique(categorical_data).tolist()}\")\n",
    "        \n",
    "        # # 3. 임베딩 적용 (가장 중요한 부분)\n",
    "        # try:\n",
    "        #     if debug_mode:\n",
    "        #         print(f\"임베딩 적용 중...\")\n",
    "        #         print(f\"  임베딩 vocab_size: {self.categorical_embedding.num_embeddings}\")\n",
    "        #         print(f\"  임베딩 embed_dim: {self.categorical_embedding.embedding_dim}\")\n",
    "                \n",
    "        #         if max_index >= vocab_size:\n",
    "        #             print(f\"❌ CRITICAL: 임베딩 범위 초과!\")\n",
    "        #             print(f\"   최대 인덱스: {max_index}, Vocab 크기: {vocab_size}\")\n",
    "        #             print(f\"   범주형 데이터: {categorical_data}\")\n",
    "        #             raise ValueError(f\"Embedding index out of range: {max_index} >= {vocab_size}\")\n",
    "                \n",
    "        #         print(f\"  임베딩 결과: {categorical_embedded.shape}\")\n",
    "        #         print(f\"  임베딩 NaN: {torch.isnan(categorical_embedded).sum()}\")\n",
    "        #         print(f\"  임베딩 Inf: {torch.isinf(categorical_embedded).sum()}\")\n",
    "\n",
    "        #         print(f\"  Flatten 후: {categorical_flattened.shape}\")\n",
    "        #         print(f\"  Flatten NaN: {torch.isnan(categorical_flattened).sum()}\")\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"❌ 임베딩 적용 실패 (샘플 {idx}): {e}\")\n",
    "        #     print(f\"   categorical_data 상세:\")\n",
    "        #     print(f\"   형태: {categorical_data.shape}\")\n",
    "        #     print(f\"   타입: {categorical_data.dtype}\")  \n",
    "        #     print(f\"   값: {categorical_data}\")\n",
    "        #     raise e\n",
    "        \n",
    "        \n",
    "        # if debug_mode:\n",
    "            \n",
    "        #     # 4. 연속형과 범주형 결합\n",
    "        #     print(f\"최종 결합: {combined_features.shape}\")\n",
    "        #     print(f\"  결합 NaN: {torch.isnan(combined_features).sum()}\")\n",
    "        #     print(f\"  결합 Inf: {torch.isinf(combined_features).sum()}\")\n",
    "        #     if torch.isnan(combined_features).sum() == 0 and torch.isinf(combined_features).sum() == 0:\n",
    "        #         print(f\"  결합 범위: [{combined_features.min():.4f}, {combined_features.max():.4f}]\")\n",
    "        \n",
    "        #     # 5. 타겟 데이터 확인\n",
    "        #     print(f\"타겟 데이터: {targets.shape}\")\n",
    "        #     print(f\"  타겟 NaN: {torch.isnan(targets).sum()}\")\n",
    "        #     print(f\"  타겟 범위: [{targets.min():.4f}, {targets.max():.4f}]\")\n",
    "        \n",
    "        # 최종 반환값에 NaN/Inf가 있는지 체크\n",
    "        final_nan = (torch.isnan(combined_features).sum() + \n",
    "                    torch.isnan(targets).sum() +\n",
    "                    torch.isinf(combined_features).sum() + \n",
    "                    torch.isinf(targets).sum())\n",
    "        \n",
    "        if final_nan > 0:\n",
    "            print(f\"❌ CRITICAL: 샘플 {idx}에서 최종 NaN/Inf 발견!\")\n",
    "            print(f\"   Features NaN: {torch.isnan(combined_features).sum()}\")\n",
    "            print(f\"   Features Inf: {torch.isinf(combined_features).sum()}\")  \n",
    "            print(f\"   Targets NaN: {torch.isnan(targets).sum()}\")\n",
    "            print(f\"   Targets Inf: {torch.isinf(targets).sum()}\")\n",
    "            \n",
    "            # 어디서 NaN이 생겼는지 추적\n",
    "            if torch.isnan(continuous_data).sum() > 0:\n",
    "                print(f\"   → 연속형 데이터에서 NaN 발생\")\n",
    "            if torch.isnan(categorical_flattened).sum() > 0:\n",
    "                print(f\"   → 범주형 임베딩에서 NaN 발생\")\n",
    "        \n",
    "            print(f\"=== 샘플 {idx} 디버깅 완료 ===\\n\")\n",
    "        \n",
    "        return {\n",
    "            'features': combined_features,\n",
    "            'targets': targets,\n",
    "            'masks': torch.tensor(sample['mask']),\n",
    "            'actual_length': sample['actual_length'],\n",
    "            'timekey_hr': sample['timekey_hr'],\n",
    "            'oper_ids_list': sample['oper_ids_list'],\n",
    "            'first_oper_id': sample['first_oper_id'],\n",
    "            'last_oper_id': sample['last_oper_id'],\n",
    "            'continuous_data': continuous_data,\n",
    "            'categorical_data': categorical_data\n",
    "        }\n",
    "    \n",
    "    def get_embedding_layer(self) -> nn.Embedding:\n",
    "        \"\"\"임베딩 레이어 반환 (모델에서 사용)\"\"\"\n",
    "        return self.categorical_embedding\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"최종 특성 차원 반환\"\"\"\n",
    "        return self.feature_dim\n",
    "\n",
    "\n",
    "def split_data_by_days(\n",
    "    df: pd.DataFrame, \n",
    "    train_ratio: float = 0.8, \n",
    "    val_ratio: float = 0.1, \n",
    "    test_ratio: float = 0.1\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"날짜 기준으로 데이터를 분할\"\"\"\n",
    "    \n",
    "    # timekey_hr에서 날짜(day) 추출\n",
    "    df['date'] = (df['timekey_hr'].astype(int) // 100).astype(int)\n",
    "    \n",
    "    # 고유한 날짜들을 시간순으로 정렬\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "    total_days = len(unique_dates)\n",
    "    \n",
    "    # 날짜 기준으로 분할 인덱스 계산\n",
    "    train_days = int(total_days * train_ratio)\n",
    "    val_days = int(total_days * val_ratio)\n",
    "    \n",
    "    train_dates = unique_dates[:train_days]\n",
    "    val_dates = unique_dates[train_days:train_days + val_days]\n",
    "    test_dates = unique_dates[train_days + val_days:]\n",
    "    \n",
    "    # 각 분할에 해당하는 데이터 추출\n",
    "    train_df = df[df['date'].isin(train_dates)].copy()\n",
    "    val_df = df[df['date'].isin(val_dates)].copy()\n",
    "    test_df = df[df['date'].isin(test_dates)].copy()\n",
    "    \n",
    "    logger.info(f\"날짜 기준 데이터 분할 완료:\")\n",
    "    logger.info(f\"  - 총 날짜 수: {total_days}일\")\n",
    "    logger.info(f\"  - Train: {len(train_dates)}일 ({len(train_df):,}행)\")\n",
    "    logger.info(f\"  - Validation: {len(val_dates)}일 ({len(val_df):,}행)\")  \n",
    "    logger.info(f\"  - Test: {len(test_dates)}일 ({len(test_df):,}행)\")\n",
    "    logger.info(f\"  - Train 날짜 범위: {min(train_dates)} ~ {max(train_dates)}\")\n",
    "    logger.info(f\"  - Val 날짜 범위: {min(val_dates)} ~ {max(val_dates)}\")\n",
    "    logger.info(f\"  - Test 날짜 범위: {min(test_dates)} ~ {max(test_dates)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def sequence_collate_fn(batch):\n",
    "    \"\"\"시퀀스 배치 collate 함수\"\"\"\n",
    "    \n",
    "    # 주요 텐서들 추출\n",
    "    features = torch.stack([sample['features'] for sample in batch])\n",
    "    targets = torch.stack([sample['targets'] for sample in batch])\n",
    "    masks = torch.stack([sample['masks'] for sample in batch])\n",
    "    \n",
    "    # 메타 정보들\n",
    "    actual_lengths = [sample['actual_length'] for sample in batch]\n",
    "    timekey_hrs = [sample['timekey_hr'] for sample in batch]\n",
    "    oper_ids_lists = [sample['oper_ids_list'] for sample in batch]\n",
    "    first_oper_ids = [sample['first_oper_id'] for sample in batch]\n",
    "    last_oper_ids = [sample['last_oper_id'] for sample in batch]\n",
    "    \n",
    "    # 디버깅용 원본 데이터\n",
    "    continuous_data = torch.stack([sample['continuous_data'] for sample in batch])\n",
    "    categorical_data = torch.stack([sample['categorical_data'] for sample in batch])\n",
    "    \n",
    "    return {\n",
    "        'features': features,  # 최종 결합된 특성 [batch_size, window_size, total_feature_dim]\n",
    "        'targets': targets,\n",
    "        'masks': masks,\n",
    "        'actual_lengths': actual_lengths,\n",
    "        'timekey_hrs': timekey_hrs,\n",
    "        'oper_ids_lists': oper_ids_lists,\n",
    "        'first_oper_ids': first_oper_ids,\n",
    "        'last_oper_ids': last_oper_ids,\n",
    "        # 디버깅용\n",
    "        'continuous_data': continuous_data,\n",
    "        'categorical_data': categorical_data\n",
    "    }\n",
    "\n",
    "def create_dataloaders(dataset_config: Dict) -> Tuple[DataLoader, DataLoader, DataLoader, CategoricalProcessor, nn.Embedding]:\n",
    "    \"\"\"데이터로더 생성 - 날짜 기준 분할 + 통합 임베딩 + Window sliding\"\"\"\n",
    "    \n",
    "    # 데이터 로드\n",
    "    data_path = dataset_config[\"file_path\"]\n",
    "    excel = pd.read_excel(data_path, sheet_name=None, header=1)\n",
    "    sheet_names = dataset_config[\"sheet_names\"]\n",
    "    \n",
    "    total_df = pd.concat([excel[sheet_name] for sheet_name in sheet_names])\n",
    "    \n",
    "    # 기본 전처리\n",
    "    if \"Unnamed: 0\" in total_df.columns:\n",
    "        total_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    \n",
    "    # y값 결측치 제거\n",
    "    df = total_df[~total_df[dataset_config[\"target_column\"]].isna()].copy()\n",
    "    logger.info(f\"y값 제거 후: {len(df)}행\")\n",
    "\n",
    "    # ✅ Inf 값 확인 및 처리 추가\n",
    "    logger.info(\"\\nInf 값 확인:\")\n",
    "    continuous_cols = dataset_config[\"continuous_columns\"]\n",
    "    inf_found = False\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        if col in df.columns:\n",
    "            inf_count = np.isinf(df[col]).sum()\n",
    "            if inf_count > 0:\n",
    "                print(f\"  ❌ {col}: {inf_count}개 Inf 값\")\n",
    "                inf_found = True\n",
    "                \n",
    "                # Inf 값을 NaN으로 변환 후 적절한 값으로 대체\n",
    "                df.loc[np.isinf(df[col]), col] = np.nan\n",
    "                df[col] = df[col].fillna(1e+5)  # 중간값으로 대체\n",
    "                logger.info(f\"     → 아주 큰 값({1e+5})으로 대체\")\n",
    "    \n",
    "    if not inf_found:\n",
    "        print(\"  ✅ 연속형 변수에 Inf 없음\")\n",
    "\n",
    "\n",
    "    # 불필요한 컬럼 제거\n",
    "    drop_columns = dataset_config.get(\"additional_drop_columns\", [])\n",
    "    if drop_columns:\n",
    "        existing_drops = [col for col in drop_columns if col in df.columns]\n",
    "        if existing_drops:\n",
    "            df = df.drop(columns=existing_drops)\n",
    "\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    logger.info(f\"원본 데이터 로드 완료:\")\n",
    "    logger.info(f\"  - 총 행 수: {len(df):,}개\")\n",
    "    logger.info(f\"  - 고유 timekey_hr: {df['timekey_hr'].nunique()}개\")\n",
    "    \n",
    "    # 통합 범주형 처리기 생성 및 학습\n",
    "    categorical_processor = CategoricalProcessor(\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),  # 새로운 파라미터\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8)\n",
    "    )\n",
    "    categorical_processor.fit(df)\n",
    "    \n",
    "    # 날짜 기준으로 데이터 분할\n",
    "    train_df, val_df, test_df = split_data_by_days(\n",
    "        df,\n",
    "        train_ratio=dataset_config.get(\"train_ratio\", 0.8),\n",
    "        val_ratio=dataset_config.get(\"val_ratio\", 0.1),\n",
    "        test_ratio=dataset_config.get(\"test_ratio\", 0.1)\n",
    "    )\n",
    "    \n",
    "    # Window sliding 파라미터\n",
    "    window_size = dataset_config.get(\"window_size\", 30)\n",
    "    stride = dataset_config.get(\"stride\", None)\n",
    "    padding_value = dataset_config.get(\"padding_value\", -9999.0)\n",
    "    \n",
    "    logger.info(f\"\\nWindow sliding 설정:\")\n",
    "    logger.info(f\"  - Window 크기: {window_size}\")\n",
    "    logger.info(f\"  - Stride: {stride if stride is not None else window_size} (비겹침)\")\n",
    "    logger.info(f\"  - Padding 값: {padding_value}\")\n",
    "    \n",
    "    # 데이터셋 생성\n",
    "    train_dataset = SequenceOperDataset(\n",
    "        df=train_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=padding_value\n",
    "    )\n",
    "    \n",
    "    val_dataset = SequenceOperDataset(\n",
    "        df=val_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=padding_value\n",
    "    )\n",
    "    \n",
    "    test_dataset = SequenceOperDataset(\n",
    "        df=test_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=padding_value\n",
    "    )\n",
    "    \n",
    "    # 임베딩 레이어 추출 (모델에서 사용할 용도)\n",
    "    embedding_layer = train_dataset.get_embedding_layer()\n",
    "    \n",
    "    # 데이터로더 생성\n",
    "    batch_size = dataset_config.get(\"batch_size\", 32)\n",
    "    num_workers = dataset_config.get(\"num_workers\", 4)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=sequence_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=sequence_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=sequence_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"\\n데이터로더 생성 완료:\")\n",
    "    logger.info(f\"  - 배치 크기: {batch_size}\")\n",
    "    logger.info(f\"  - Train 배치 수: {len(train_loader)}\")\n",
    "    logger.info(f\"  - Val 배치 수: {len(val_loader)}\")\n",
    "    logger.info(f\"  - Test 배치 수: {len(test_loader)}\")\n",
    "    \n",
    "    # 특성 차원 정보 출력\n",
    "    feature_dim = train_dataset.get_feature_dim()\n",
    "    logger.info(f\"  - 최종 특성 차원: {feature_dim}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, categorical_processor, embedding_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ 모델 아키텍처들\n",
    "\n",
    "### RNN 기본 모델 (models/rnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"기본 RNN/LSTM/GRU 모델 - 통합 임베딩 데이터셋용\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # 이미 결합된 특성 차원 (연속형 + 범주형 임베딩)\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: bool = True,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # RNN 레이어 - 입력 차원이 이미 결합된 feature_dim\n",
    "        if rnn_type.upper() == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type.upper() == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # 출력 차원 계산\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - 이미 결합된 특성 (연속형 + 범주형 임베딩)\n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "            **kwargs: 호환성을 위한 추가 인자들 (무시됨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = features.shape[:2]\n",
    "        \n",
    "        # 패딩된 위치를 마스킹\n",
    "        masked_features = features.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # RNN forward\n",
    "        rnn_output, _ = self.rnn(masked_features)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        predictions = self.output_layer(rnn_output).squeeze(-1)\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN + Self-Attention 모델 (models/attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention 메커니즘\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, hidden_dim]\n",
    "            mask: [batch_size, seq_len] (True = 패딩)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Multi-head attention\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_mask = mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended = torch.matmul(attention_weights, V)\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Residual connection + Layer norm\n",
    "        output = self.layer_norm(x + attended)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class RNNAttentionModel(nn.Module):\n",
    "    \"\"\"RNN + Self-Attention 모델 - 통합 임베딩 데이터셋용\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # 이미 결합된 특성 차원 (연속형 + 범주형 임베딩)\n",
    "        rnn_type: str = \"LSTM\", \n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: bool = True,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # RNN 레이어 - 입력 차원이 이미 결합된 feature_dim\n",
    "        if rnn_type.upper() == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type.upper() == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # RNN 출력 차원\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        # RNN 출력을 어텐션 입력 차원으로 변환\n",
    "        self.rnn_projection = nn.Linear(rnn_output_dim, hidden_dim)\n",
    "        \n",
    "        # Self-Attention\n",
    "        self.self_attention = SelfAttention(\n",
    "            hidden_dim, num_attention_heads, dropout\n",
    "        )\n",
    "        \n",
    "        # 출력 레이어\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - 이미 결합된 특성 (연속형 + 범주형 임베딩)\n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "            **kwargs: 호환성을 위한 추가 인자들 (무시됨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = features.shape[:2]\n",
    "        \n",
    "        # 패딩된 위치를 마스킹\n",
    "        masked_features = features.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # RNN forward\n",
    "        rnn_output, _ = self.rnn(masked_features)\n",
    "        \n",
    "        # RNN 출력 차원 변환\n",
    "        projected_output = self.rnn_projection(rnn_output)\n",
    "        \n",
    "        # Self-Attention 적용\n",
    "        attended_output = self.self_attention(projected_output, masks)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        predictions = self.output_layer(attended_output).squeeze(-1)\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1D 모델 (models/cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    \"\"\"1D CNN 모델 (다중 커널) - 통합 임베딩 데이터셋용\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # 이미 결합된 특성 차원 (연속형 + 범주형 임베딩)\n",
    "        kernel_sizes: List[int] = [3, 5, 7],\n",
    "        num_filters: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # 다중 커널 1D Conv 레이어들 - 입력 차원이 이미 결합된 feature_dim\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(feature_dim, num_filters, kernel_size, padding=kernel_size//2)\n",
    "            for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(num_filters) for _ in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # 출력 레이어\n",
    "        total_filters = len(kernel_sizes) * num_filters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters, total_filters // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - 이미 결합된 특성 (연속형 + 범주형 임베딩)\n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "            **kwargs: 호환성을 위한 추가 인자들 (무시됨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = features.shape[:2]\n",
    "        \n",
    "        # 패딩된 위치를 마스킹\n",
    "        masked_features = features.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # Conv1d를 위해 차원 변환: [batch, seq_len, features] -> [batch, features, seq_len]\n",
    "        conv_input = masked_features.transpose(1, 2)\n",
    "        \n",
    "        # 다중 커널 Conv1D 적용\n",
    "        conv_outputs = []\n",
    "        for conv, bn in zip(self.conv_layers, self.batch_norms):\n",
    "            conv_out = F.relu(bn(conv(conv_input)))  # [batch, filters, seq_len]\n",
    "            conv_outputs.append(conv_out)\n",
    "        \n",
    "        # 모든 커널 출력 결합\n",
    "        combined_conv = torch.cat(conv_outputs, dim=1)  # [batch, total_filters, seq_len]\n",
    "        \n",
    "        # 다시 원래 차원으로: [batch, total_filters, seq_len] -> [batch, seq_len, total_filters]\n",
    "        combined_conv = combined_conv.transpose(1, 2)\n",
    "        \n",
    "        # 출력 레이어\n",
    "        predictions = self.output_layer(combined_conv).squeeze(-1)\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer(Encoder) 모델(models/transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"사인/코사인 위치 인코딩\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # 위치 인코딩 생성\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 짝수 인덱스\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 홀수 인덱스\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:seq_len, :, :].transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Point-wise Transformer 모델 (인코더만 사용) - 통합 임베딩 데이터셋용\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # 입력 특성 차원 (연속형 + 범주형 임베딩 flatten 결과)\n",
    "        d_model: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        use_positional_encoding: bool = True,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.padding_value = padding_value\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        \n",
    "        # 입력 특성을 d_model 차원으로 projection\n",
    "        self.input_projection = nn.Linear(feature_dim, d_model)\n",
    "        \n",
    "        # 위치 인코딩 (선택적)\n",
    "        if use_positional_encoding:\n",
    "            self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Transformer 인코더 레이어\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,  # [batch_size, seq_len, d_model]\n",
    "            norm_first=False   # Post-norm (표준)\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Point-wise 출력 레이어 (각 위치별로 독립적 예측)\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # 파라미터 초기화\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Xavier uniform 초기화\"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - 이미 결합된 특성 (연속형 + 범주형 임베딩)\n",
    "            masks: [batch_size, seq_len] (True = 패딩 위치)\n",
    "            **kwargs: 호환성을 위한 추가 인자들 (무시됨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len] - 각 위치별 예측값\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = features.shape\n",
    "        \n",
    "        # 입력 특성을 Transformer 차원으로 projection\n",
    "        x = self.input_projection(features)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 위치 인코딩 추가 (선택적)\n",
    "        if self.use_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer 인코더 적용\n",
    "        # src_key_padding_mask: True인 위치는 attention에서 무시\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            x, \n",
    "            src_key_padding_mask=masks\n",
    "        )  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Point-wise 예측 (각 위치별로 독립적)\n",
    "        predictions = self.output_projection(transformer_output).squeeze(-1)  # [batch_size, seq_len]\n",
    "        \n",
    "        # 패딩된 위치는 0으로 마스킹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 팩토리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_config: Dict, feature_dim: int):\n",
    "    \"\"\"설정에 따른 모델 생성 - 통합 임베딩 데이터셋용 (모든 모델 타입 지원)\"\"\"\n",
    "    \n",
    "    model_type = model_config.get(\"model_type\", \"lstm\").lower()\n",
    "    dropout = model_config.get(\"dropout\", 0.1)\n",
    "    padding_value = model_config.get(\"padding_value\", -9999.0)\n",
    "    hidden_dim = model_config.get(\"hidden_dim\", 128)\n",
    "    num_layers = model_config.get(\"num_layers\", 2)\n",
    "    bidirectional = model_config.get(\"bidirectional\", True)\n",
    "    \n",
    "    logger.info(f\"모델 생성 중:\")\n",
    "    logger.info(f\"  - 모델 타입: {model_type}\")\n",
    "    logger.info(f\"  - 입력 특성 차원: {feature_dim}\")\n",
    "    logger.info(f\"  - 히든 차원: {hidden_dim}\")\n",
    "    logger.info(f\"  - 레이어 수: {num_layers}\")\n",
    "    logger.info(f\"  - 드롭아웃: {dropout}\")\n",
    "    \n",
    "    if model_type == \"transformer\":\n",
    "        # Transformer 모델\n",
    "        d_model = model_config.get(\"d_model\", 256)\n",
    "        num_heads = model_config.get(\"num_heads\", 8)\n",
    "        dim_feedforward = model_config.get(\"dim_feedforward\", 1024)\n",
    "        activation = model_config.get(\"activation\", \"relu\")\n",
    "        use_positional_encoding = model_config.get(\"use_positional_encoding\", True)\n",
    "        \n",
    "        model = TransformerModel(\n",
    "            feature_dim=feature_dim,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - d_model: {d_model}\")\n",
    "        logger.info(f\"  - num_heads: {num_heads}\")\n",
    "        logger.info(f\"  - dim_feedforward: {dim_feedforward}\")\n",
    "        logger.info(f\"  - 위치 인코딩: {'사용' if use_positional_encoding else '미사용'}\")\n",
    "        \n",
    "    elif model_type in [\"rnn\", \"lstm\", \"gru\"]:\n",
    "        # 기본 RNN/LSTM/GRU 모델\n",
    "        model = RNNModel(\n",
    "            feature_dim=feature_dim,\n",
    "            rnn_type=model_type.upper(),\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - RNN 타입: {model_type.upper()}\")\n",
    "        logger.info(f\"  - 양방향: {'Yes' if bidirectional else 'No'}\")\n",
    "        \n",
    "    elif model_type in [\"rnn_attention\", \"lstm_attention\", \"gru_attention\"]:\n",
    "        # RNN + Self-Attention 모델\n",
    "        rnn_type = model_type.replace(\"_attention\", \"\").upper()\n",
    "        num_attention_heads = model_config.get(\"num_attention_heads\", 8)\n",
    "        \n",
    "        model = RNNAttentionModel(\n",
    "            feature_dim=feature_dim,\n",
    "            rnn_type=rnn_type,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - RNN 타입: {rnn_type}\")\n",
    "        logger.info(f\"  - 양방향: {'Yes' if bidirectional else 'No'}\")\n",
    "        logger.info(f\"  - Attention heads: {num_attention_heads}\")\n",
    "        \n",
    "    elif model_type == \"cnn1d\":\n",
    "        # CNN 1D 모델\n",
    "        kernel_sizes = model_config.get(\"kernel_sizes\", [3, 5, 7])\n",
    "        num_filters = model_config.get(\"num_filters\", 64)\n",
    "        \n",
    "        model = CNN1DModel(\n",
    "            feature_dim=feature_dim,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            num_filters=num_filters,\n",
    "            dropout=dropout,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - 커널 크기들: {kernel_sizes}\")\n",
    "        logger.info(f\"  - 필터 수: {num_filters}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    # 파라미터 수 계산 및 출력\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    logger.info(f\"  - 총 파라미터 수: {total_params:,}\")\n",
    "    logger.info(f\"  - 학습 가능한 파라미터 수: {trainable_params:,}\")\n",
    "    logger.info(f\"모델 생성 완료!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_from_config_and_dataloader(model_config: Dict, train_loader: DataLoader) -> nn.Module:\n",
    "    \"\"\"데이터로더에서 특성 차원을 추출하여 모델 생성하는 헬퍼 함수\"\"\"\n",
    "    \n",
    "    # 첫 번째 배치에서 특성 차원 추출\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    feature_dim = sample_batch['features'].shape[-1]  # [batch_size, seq_len, feature_dim]\n",
    "    \n",
    "    logger.info(f\"데이터로더에서 추출한 특성 차원: {feature_dim}\")\n",
    "    \n",
    "    return create_model(model_config, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚂 훈련 및 평가 함수들\n",
    "\n",
    "### 마스크 기반 손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"패딩을 고려한 안전한 MSE Loss - NaN 방지\"\"\"\n",
    "    \n",
    "    def __init__(self, padding_value: float = -9999.0):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "    \n",
    "    def forward(self, predictions, targets, masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: [batch_size, seq_len]\n",
    "            targets: [batch_size, seq_len]  \n",
    "            masks: [batch_size, seq_len] (True = 패딩)\n",
    "        \"\"\"\n",
    "        # 패딩되지 않은 위치만 선택\n",
    "        valid_mask = ~masks\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            print(\"⚠️ 경고: 유효한 데이터가 없습니다!\")\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        valid_predictions = predictions[valid_mask]\n",
    "        valid_targets = targets[valid_mask]\n",
    "        \n",
    "        # NaN/Inf 체크 및 제거\n",
    "        finite_mask = torch.isfinite(valid_predictions) & torch.isfinite(valid_targets)\n",
    "        \n",
    "        if finite_mask.sum() == 0:\n",
    "            print(\"⚠️ 경고: finite한 값이 없습니다!\")\n",
    "            return torch.tensor(1e6, device=predictions.device, requires_grad=True)  # 큰 손실값 반환\n",
    "        \n",
    "        valid_predictions = valid_predictions[finite_mask]\n",
    "        valid_targets = valid_targets[finite_mask]\n",
    "        \n",
    "        # MSE 계산\n",
    "        mse = F.mse_loss(valid_predictions, valid_targets)\n",
    "        \n",
    "        # NaN 체크\n",
    "        if torch.isnan(mse) or torch.isinf(mse):\n",
    "            print(f\"⚠️ 경고: MSE가 {mse.item()}입니다!\")\n",
    "            return torch.tensor(1e6, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets, masks, padding_value: float = -9999.0):\n",
    "    \"\"\"패딩을 고려한 메트릭 계산\"\"\"\n",
    "    valid_mask = ~masks\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        return {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_targets = targets[valid_mask]\n",
    "    \n",
    "    # CPU로 변환\n",
    "    valid_predictions = valid_predictions.detach().cpu().numpy()\n",
    "    valid_targets = valid_targets.detach().cpu().numpy()\n",
    "    \n",
    "    mse = np.mean((valid_predictions - valid_targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(valid_predictions - valid_targets))\n",
    "    \n",
    "    # MAPE 계산 (0으로 나누기 방지)\n",
    "    epsilon = 1e-8\n",
    "    abs_targets = np.abs(valid_targets)\n",
    "    abs_errors = np.abs(valid_predictions - valid_targets)\n",
    "    safe_targets = np.maximum(abs_targets, epsilon)\n",
    "    mape = np.mean(abs_errors / safe_targets * 100)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse, \n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"valid_count\": len(valid_predictions)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 에폭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"한 에폭 훈련 - 통합 임베딩 데이터셋용\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_metrics = {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        desc=f\"Epoch {epoch} [Train]\",\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    for batch_idx, batch in pbar:\n",
    "        # 새로운 데이터셋 구조에 맞는 키 사용\n",
    "        features = batch[\"features\"].to(device)  # 이미 결합된 특성\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "        masks = batch[\"masks\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 모든 모델이 동일한 인터페이스 사용\n",
    "        predictions = model(features, masks)\n",
    "        loss = criterion(predictions, targets, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 메트릭 계산\n",
    "        with torch.no_grad():\n",
    "            batch_metrics = compute_metrics(predictions, targets, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "            total_metrics[key] += batch_metrics[key]\n",
    "        total_metrics[\"valid_count\"] += batch_metrics[\"valid_count\"]\n",
    "        \n",
    "        # 진행바 업데이트\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"MAPE\": f\"{batch_metrics['mape']:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # 평균 계산\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "        total_metrics[key] = total_metrics[key] / len(dataloader)\n",
    "    \n",
    "    return avg_loss, total_metrics\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device, epoch=None):\n",
    "    \"\"\"검증 에폭 - 통합 임베딩 데이터셋용\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = {\"mse\": 0, \"rmse\": 0, \"mae\": 0, \"mape\": 0, \"valid_count\": 0}\n",
    "    \n",
    "    desc = f\"Epoch {epoch} [Val]\" if epoch is not None else \"Validation\"\n",
    "    pbar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # 새로운 데이터셋 구조에 맞는 키 사용\n",
    "            features = batch[\"features\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            masks = batch[\"masks\"].to(device)\n",
    "            \n",
    "            # 모든 모델이 동일한 인터페이스 사용\n",
    "            predictions = model(features, masks)\n",
    "            loss = criterion(predictions, targets, masks)\n",
    "            \n",
    "            batch_metrics = compute_metrics(predictions, targets, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "                total_metrics[key] += batch_metrics[key]\n",
    "            total_metrics[\"valid_count\"] += batch_metrics[\"valid_count\"]\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"MAPE\": f\"{batch_metrics['mape']:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "        total_metrics[key] = total_metrics[key] / len(dataloader)\n",
    "    \n",
    "    return avg_loss, total_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 메인 훈련 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, training_config, device, save_path):\n",
    "    \"\"\"메인 훈련 루프 - 통합 임베딩 데이터셋용 (검증 간격 설정 가능)\"\"\"\n",
    "    \n",
    "    num_epochs = training_config.get(\"num_epochs\", 100)\n",
    "    learning_rate = training_config.get(\"learning_rate\", 1e-3)\n",
    "    patience = training_config.get(\"patience\", 20)\n",
    "    padding_value = training_config.get(\"padding_value\", -9999.0)\n",
    "    \n",
    "    # 검증 간격 설정 (새로 추가)\n",
    "    val_interval = training_config.get(\"val_interval\", 1)  # 기본값: 매 에폭마다 검증\n",
    "    log_interval = training_config.get(\"log_interval\", 1)  # 기본값: 매 에폭마다 로깅\n",
    "    \n",
    "    # 손실 함수 및 옵티마이저\n",
    "    criterion = MaskedMSELoss(padding_value)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience//2, verbose=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    last_val_loss = None\n",
    "    last_val_metrics = None\n",
    "    \n",
    "    logger.info(f\"훈련 시작: {num_epochs} 에폭, 학습률 {learning_rate}\")\n",
    "    logger.info(f\"모델 타입: {training_config.get('model_type', 'unknown')}\")\n",
    "    logger.info(f\"검증 간격: {val_interval} 에폭마다\")\n",
    "    logger.info(f\"로깅 간격: {log_interval} 에폭마다\")\n",
    "    \n",
    "    # 에폭 진행바\n",
    "    epoch_pbar = tqdm(range(1, num_epochs + 1), desc=\"Training Progress\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # 훈련은 매 에폭마다 실시\n",
    "        train_loss, train_metrics = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # 검증은 지정된 간격마다 또는 마지막 에폭에서 실시\n",
    "        should_validate = (epoch % val_interval == 0) or (epoch == num_epochs)\n",
    "        \n",
    "        if should_validate:\n",
    "            val_loss, val_metrics = validate_epoch(\n",
    "                model, val_loader, criterion, device, epoch\n",
    "            )\n",
    "            last_val_loss = val_loss\n",
    "            last_val_metrics = val_metrics\n",
    "            \n",
    "            # 스케줄러 업데이트 (검증이 실시된 경우만)\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            # 검증하지 않는 에폭에서는 이전 검증 결과 사용\n",
    "            val_loss = last_val_loss if last_val_loss is not None else float('inf')\n",
    "            val_metrics = last_val_metrics if last_val_metrics is not None else {\n",
    "                \"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0\n",
    "            }\n",
    "        \n",
    "        # 로깅 (지정된 간격마다 또는 검증이 실시된 경우)\n",
    "        should_log = (epoch % log_interval == 0) or should_validate or (epoch == num_epochs)\n",
    "        \n",
    "        if should_log:\n",
    "            if should_validate:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "                    f'Train MAPE={train_metrics[\"mape\"]:.2f}%, Val MAPE={val_metrics[\"mape\"]:.2f}%'\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, \"\n",
    "                    f'Train MAPE={train_metrics[\"mape\"]:.2f}% (검증 생략)'\n",
    "                )\n",
    "        \n",
    "        # 진행바 업데이트\n",
    "        val_status = \"검증됨\" if should_validate else \"이전값\"\n",
    "        epoch_pbar.set_postfix({\n",
    "            \"T_Loss\": f\"{train_loss:.4f}\",\n",
    "            \"V_Loss\": f\"{val_loss:.4f}({val_status})\",\n",
    "            \"V_MAPE\": f'{val_metrics[\"mape\"]:.2f}%',\n",
    "            \"Best\": f\"{best_val_loss:.4f}\",\n",
    "            \"Patience\": f\"{patience_counter}/{patience}\"\n",
    "        })\n",
    "        \n",
    "        # 최고 모델 저장 (검증이 실시된 경우만)\n",
    "        if should_validate and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'train_metrics': train_metrics,\n",
    "                'model_type': training_config.get('model_type', 'unknown'),\n",
    "                'feature_dim': training_config.get('feature_dim', None),\n",
    "                'config': training_config\n",
    "            }, save_path)\n",
    "            \n",
    "            logger.info(f\"  → Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        elif should_validate:\n",
    "            # 검증은 했지만 성능이 개선되지 않은 경우\n",
    "            patience_counter += 1\n",
    "        # 검증하지 않은 경우 patience_counter는 증가시키지 않음\n",
    "        \n",
    "        # 조기 종료 (검증이 실시된 경우만 확인)\n",
    "        if should_validate and patience_counter >= patience:\n",
    "            logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    # 마지막에 한번 더 검증 (마지막 에폭에서 검증하지 않았다면)\n",
    "    if not should_validate:\n",
    "        logger.info(\"최종 검증 실시 중...\")\n",
    "        final_val_loss, final_val_metrics = validate_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        logger.info(f\"최종 검증 결과: Val Loss={final_val_loss:.4f}, Val MAPE={final_val_metrics['mape']:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'total_epochs': epoch,\n",
    "        'early_stopped': patience_counter >= patience,\n",
    "        'validation_count': len([e for e in range(1, epoch + 1) if e % val_interval == 0 or e == epoch])\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader, device, model_path, config):\n",
    "    \"\"\"모델 평가 - 통합 임베딩 데이터셋용 (구조 정보 포함)\"\"\"\n",
    "    logger.info(f\"모델 로드: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    padding_value = config.get(\"padding_value\", -9999.0)\n",
    "    criterion = MaskedMSELoss(padding_value)\n",
    "    \n",
    "    # 구조화된 결과를 위한 리스트들\n",
    "    structured_predictions = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    logger.info(\"테스트 시작\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # collate_fn에서 반환하는 키 이름들 사용\n",
    "            features = batch[\"features\"].to(device)  # 이미 결합된 특성\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            masks = batch[\"masks\"].to(device)\n",
    "            \n",
    "            # 구조 정보 추출 (collate_fn의 키 이름들과 일치)\n",
    "            timekey_hrs = batch[\"timekey_hrs\"]\n",
    "            oper_ids_lists = batch[\"oper_ids_lists\"]\n",
    "            actual_lengths = batch[\"actual_lengths\"]\n",
    "            first_oper_ids = batch[\"first_oper_ids\"]\n",
    "            last_oper_ids = batch[\"last_oper_ids\"]\n",
    "            \n",
    "            # 모델 예측\n",
    "            predictions = model(features, masks)\n",
    "            loss = criterion(predictions, targets, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # CPU로 변환\n",
    "            predictions_cpu = predictions.cpu()\n",
    "            targets_cpu = targets.cpu()\n",
    "            masks_cpu = masks.cpu()\n",
    "            \n",
    "            # 배치 내 각 샘플에 대해 구조화된 결과 생성\n",
    "            batch_size = predictions_cpu.shape[0]\n",
    "            for sample_idx in range(batch_size):\n",
    "                timekey_hr = timekey_hrs[sample_idx]\n",
    "                oper_ids = oper_ids_lists[sample_idx]\n",
    "                actual_length = actual_lengths[sample_idx]\n",
    "                first_oper_id = first_oper_ids[sample_idx]\n",
    "                last_oper_id = last_oper_ids[sample_idx]\n",
    "                \n",
    "                sample_predictions = predictions_cpu[sample_idx]\n",
    "                sample_targets = targets_cpu[sample_idx]\n",
    "                sample_masks = masks_cpu[sample_idx]\n",
    "                \n",
    "                # 각 시퀀스 위치에 대해 (패딩되지 않은 위치만)\n",
    "                for seq_idx in range(actual_length):\n",
    "                    if seq_idx < len(sample_predictions) and not sample_masks[seq_idx]:\n",
    "                        pred_val = sample_predictions[seq_idx].item()\n",
    "                        target_val = sample_targets[seq_idx].item()\n",
    "                        oper_id = oper_ids[seq_idx] if seq_idx < len(oper_ids) else None\n",
    "                        \n",
    "                        # 개별 예측 결과 저장\n",
    "                        structured_predictions.append({\n",
    "                            'timekey_hr': timekey_hr,\n",
    "                            'oper_id': oper_id,\n",
    "                            'seq_position': seq_idx,\n",
    "                            'prediction': pred_val,\n",
    "                            'actual': target_val,\n",
    "                            'first_oper_id': first_oper_id,\n",
    "                            'last_oper_id': last_oper_id,\n",
    "                            'window_length': actual_length\n",
    "                        })\n",
    "                        \n",
    "                        all_predictions.append(pred_val)\n",
    "                        all_targets.append(target_val)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    # 메트릭 계산\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    if len(all_predictions) == 0:\n",
    "        logger.warning(\"예측 결과가 없습니다!\")\n",
    "        metrics = {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    else:\n",
    "        mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "        \n",
    "        # MAPE 계산 (0으로 나누기 방지)\n",
    "        epsilon = 1e-8\n",
    "        abs_targets = np.abs(all_targets)\n",
    "        abs_errors = np.abs(all_predictions - all_targets)\n",
    "        safe_targets = np.maximum(abs_targets, epsilon)\n",
    "        mape = np.mean(abs_errors / safe_targets * 100)\n",
    "        \n",
    "        metrics = {\n",
    "            \"mse\": mse,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"valid_count\": len(all_predictions)\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"테스트 결과: RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}, MAPE={metrics['mape']:.2f}%\")\n",
    "    logger.info(f\"구조화된 예측 결과: {len(structured_predictions):,}개\")\n",
    "    \n",
    "    # 모델 정보 수집\n",
    "    model_info = {\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        \"model_type\": config.get(\"model_type\", \"unknown\")\n",
    "    }\n",
    "    \n",
    "    # checkpoint에서 추가 정보 수집 (있는 경우)\n",
    "    if 'feature_dim' in checkpoint:\n",
    "        model_info['feature_dim'] = checkpoint['feature_dim']\n",
    "    if 'epoch' in checkpoint:\n",
    "        model_info['best_epoch'] = checkpoint['epoch']\n",
    "    if 'val_loss' in checkpoint:\n",
    "        model_info['best_val_loss'] = checkpoint['val_loss']\n",
    "    \n",
    "    return {\n",
    "        \"test_loss\": avg_loss,\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": all_predictions,\n",
    "        \"targets\": all_targets,\n",
    "        \"structured_predictions\": structured_predictions,\n",
    "        \"model_info\": model_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 메인 실행 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 13:52:30,815 - INFO - Using device: cuda:0\n",
      "2025-09-04 13:52:30,815 - INFO - 데이터 로딩 중...\n",
      "2025-09-04 14:07:57,271 - INFO - y값 제거 후: 1671457행\n",
      "2025-09-04 14:07:57,272 - INFO - \n",
      "Inf 값 확인:\n",
      "2025-09-04 14:07:57,285 - INFO -      → 아주 큰 값(100000.0)으로 대체\n",
      "2025-09-04 14:07:57,478 - INFO - 원본 데이터 로드 완료:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ❌ x5: 144개 Inf 값\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:07:57,479 - INFO -   - 총 행 수: 1,671,457개\n",
      "2025-09-04 14:07:57,488 - INFO -   - 고유 timekey_hr: 2136개\n",
      "2025-09-04 14:07:57,488 - INFO - 통합된 범주형 변수 설정:\n",
      "2025-09-04 14:07:57,488 - INFO -   - 변수별 카테고리 수: {'oper_group': 277, 'days': 7, 'shift': 3, 'x1': 20}\n",
      "2025-09-04 14:07:57,489 - INFO -   - 총 vocabulary 크기: 307\n",
      "2025-09-04 14:07:57,600 - INFO -   - oper_group: 인덱스 0~276 (277개)\n",
      "2025-09-04 14:07:57,693 - INFO -   - days: 인덱스 277~283 (7개)\n",
      "2025-09-04 14:07:57,783 - INFO -   - shift: 인덱스 284~286 (3개)\n",
      "2025-09-04 14:07:57,886 - INFO -   - x1: 인덱스 287~306 (20개)\n",
      "2025-09-04 14:07:57,886 - INFO -   - 최종 통합 vocabulary 크기: 307\n",
      "2025-09-04 14:07:58,507 - INFO - 날짜 기준 데이터 분할 완료:\n",
      "2025-09-04 14:07:58,508 - INFO -   - 총 날짜 수: 90일\n",
      "2025-09-04 14:07:58,508 - INFO -   - Train: 72일 (1,342,808행)\n",
      "2025-09-04 14:07:58,509 - INFO -   - Validation: 9일 (170,705행)\n",
      "2025-09-04 14:07:58,509 - INFO -   - Test: 9일 (157,944행)\n",
      "2025-09-04 14:07:58,510 - INFO -   - Train 날짜 범위: 20250503 ~ 20250713\n",
      "2025-09-04 14:07:58,511 - INFO -   - Val 날짜 범위: 20250714 ~ 20250722\n",
      "2025-09-04 14:07:58,511 - INFO -   - Test 날짜 범위: 20250723 ~ 20250731\n",
      "2025-09-04 14:07:58,512 - INFO - \n",
      "Window sliding 설정:\n",
      "2025-09-04 14:07:58,513 - INFO -   - Window 크기: 50\n",
      "2025-09-04 14:07:58,513 - INFO -   - Stride: 50 (비겹침)\n",
      "2025-09-04 14:07:58,514 - INFO -   - Padding 값: -9999.0\n",
      "2025-09-04 14:08:31,958 - INFO - Window sliding 결과:\n",
      "2025-09-04 14:08:31,958 - INFO -   - 총 샘플 수: 27424\n",
      "2025-09-04 14:08:31,962 - INFO -   - 평균 실제 길이: 49.0\n",
      "2025-09-04 14:08:31,965 - INFO -   - 최대 실제 길이: 50\n",
      "2025-09-04 14:08:31,966 - INFO -   - 최소 실제 길이: 1\n",
      "2025-09-04 14:08:31,970 - INFO -   - 고유한 timekey_hr: 1721개\n",
      "2025-09-04 14:08:31,971 - INFO -   - timekey_hr당 평균 샘플 수: 15.9개\n",
      "2025-09-04 14:08:31,975 - INFO - 시퀀스 데이터셋 구성 완료:\n",
      "2025-09-04 14:08:31,976 - INFO -   - 총 샘플 수: 27424\n",
      "2025-09-04 14:08:31,976 - INFO -   - Window 크기: 50\n",
      "2025-09-04 14:08:31,977 - INFO -   - Stride: 50\n",
      "2025-09-04 14:08:31,977 - INFO -   - 연속형 차원: 20\n",
      "2025-09-04 14:08:31,978 - INFO -   - 범주형 변수 수: 4\n",
      "2025-09-04 14:08:31,979 - INFO -   - 임베딩 차원: 8\n",
      "2025-09-04 14:08:31,979 - INFO -   - 최종 특성 차원: 52\n",
      "2025-09-04 14:08:31,980 - INFO -   - 패딩값: -9999.0\n",
      "2025-09-04 14:08:36,229 - INFO - Window sliding 결과:\n",
      "2025-09-04 14:08:36,229 - INFO -   - 총 샘플 수: 3458\n",
      "2025-09-04 14:08:36,231 - INFO -   - 평균 실제 길이: 49.4\n",
      "2025-09-04 14:08:36,231 - INFO -   - 최대 실제 길이: 50\n",
      "2025-09-04 14:08:36,232 - INFO -   - 최소 실제 길이: 1\n",
      "2025-09-04 14:08:36,233 - INFO -   - 고유한 timekey_hr: 216개\n",
      "2025-09-04 14:08:36,234 - INFO -   - timekey_hr당 평균 샘플 수: 16.0개\n",
      "2025-09-04 14:08:36,235 - INFO - 시퀀스 데이터셋 구성 완료:\n",
      "2025-09-04 14:08:36,236 - INFO -   - 총 샘플 수: 3458\n",
      "2025-09-04 14:08:36,236 - INFO -   - Window 크기: 50\n",
      "2025-09-04 14:08:36,237 - INFO -   - Stride: 50\n",
      "2025-09-04 14:08:36,237 - INFO -   - 연속형 차원: 20\n",
      "2025-09-04 14:08:36,238 - INFO -   - 범주형 변수 수: 4\n",
      "2025-09-04 14:08:36,238 - INFO -   - 임베딩 차원: 8\n",
      "2025-09-04 14:08:36,239 - INFO -   - 최종 특성 차원: 52\n",
      "2025-09-04 14:08:36,239 - INFO -   - 패딩값: -9999.0\n",
      "2025-09-04 14:08:40,041 - INFO - Window sliding 결과:\n",
      "2025-09-04 14:08:40,041 - INFO -   - 총 샘플 수: 3195\n",
      "2025-09-04 14:08:40,042 - INFO -   - 평균 실제 길이: 49.4\n",
      "2025-09-04 14:08:40,043 - INFO -   - 최대 실제 길이: 50\n",
      "2025-09-04 14:08:40,044 - INFO -   - 최소 실제 길이: 1\n",
      "2025-09-04 14:08:40,045 - INFO -   - 고유한 timekey_hr: 199개\n",
      "2025-09-04 14:08:40,046 - INFO -   - timekey_hr당 평균 샘플 수: 16.1개\n",
      "2025-09-04 14:08:40,047 - INFO - 시퀀스 데이터셋 구성 완료:\n",
      "2025-09-04 14:08:40,047 - INFO -   - 총 샘플 수: 3195\n",
      "2025-09-04 14:08:40,048 - INFO -   - Window 크기: 50\n",
      "2025-09-04 14:08:40,048 - INFO -   - Stride: 50\n",
      "2025-09-04 14:08:40,049 - INFO -   - 연속형 차원: 20\n",
      "2025-09-04 14:08:40,049 - INFO -   - 범주형 변수 수: 4\n",
      "2025-09-04 14:08:40,050 - INFO -   - 임베딩 차원: 8\n",
      "2025-09-04 14:08:40,050 - INFO -   - 최종 특성 차원: 52\n",
      "2025-09-04 14:08:40,051 - INFO -   - 패딩값: -9999.0\n",
      "2025-09-04 14:08:40,051 - INFO - \n",
      "데이터로더 생성 완료:\n",
      "2025-09-04 14:08:40,052 - INFO -   - 배치 크기: 16\n",
      "2025-09-04 14:08:40,052 - INFO -   - Train 배치 수: 1714\n",
      "2025-09-04 14:08:40,053 - INFO -   - Val 배치 수: 217\n",
      "2025-09-04 14:08:40,053 - INFO -   - Test 배치 수: 200\n",
      "2025-09-04 14:08:40,054 - INFO -   - 최종 특성 차원: 52\n",
      "2025-09-04 14:08:40,153 - INFO - 모델 생성 중...\n",
      "2025-09-04 14:08:41,721 - INFO - 데이터로더에서 추출한 특성 차원: 52\n",
      "2025-09-04 14:08:41,722 - INFO - 모델 생성 중:\n",
      "2025-09-04 14:08:41,722 - INFO -   - 모델 타입: lstm\n",
      "2025-09-04 14:08:41,723 - INFO -   - 입력 특성 차원: 52\n",
      "2025-09-04 14:08:41,723 - INFO -   - 히든 차원: 128\n",
      "2025-09-04 14:08:41,724 - INFO -   - 레이어 수: 2\n",
      "2025-09-04 14:08:41,724 - INFO -   - 드롭아웃: 0.1\n",
      "2025-09-04 14:08:41,730 - INFO -   - RNN 타입: LSTM\n",
      "2025-09-04 14:08:41,730 - INFO -   - 양방향: Yes\n",
      "2025-09-04 14:08:41,731 - INFO -   - 총 파라미터 수: 614,657\n",
      "2025-09-04 14:08:41,731 - INFO -   - 학습 가능한 파라미터 수: 614,657\n",
      "2025-09-04 14:08:41,732 - INFO - 모델 생성 완료!\n",
      "2025-09-04 14:08:41,732 - INFO - 훈련 시작...\n",
      "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2025-09-04 14:08:43,014 - INFO - 훈련 시작: 10 에폭, 학습률 0.0001\n",
      "2025-09-04 14:08:43,015 - INFO - 모델 타입: lstm\n",
      "2025-09-04 14:08:43,015 - INFO - 검증 간격: 2 에폭마다\n",
      "2025-09-04 14:08:43,016 - INFO - 로깅 간격: 1 에폭마다\n",
      "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  27%|██▋       | 468/1714 [00:05<00:11, 105.64it/s, Loss=0.0377, MAPE=62.83%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  27%|██▋       | 468/1714 [00:05<00:11, 105.64it/s, Loss=0.0256, MAPE=62.13%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  27%|██▋       | 468/1714 [00:05<00:11, 105.64it/s, Loss=0.0191, MAPE=71.60%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  75%|███████▍  | 1282/1714 [00:13<00:04, 103.81it/s, Loss=0.0315, MAPE=60.68%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "최종 결합: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  75%|███████▍  | 1282/1714 [00:13<00:04, 103.81it/s, Loss=0.0183, MAPE=59.88%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  75%|███████▌  | 1293/1714 [00:13<00:04, 100.78it/s, Loss=0.0183, MAPE=59.88%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  85%|████████▌ | 1458/1714 [00:15<00:02, 104.94it/s, Loss=0.0145, MAPE=63.11%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0  임베딩 Inf: 0\n",
      "\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  85%|████████▌ | 1458/1714 [00:15<00:02, 104.94it/s, Loss=0.0156, MAPE=65.41%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  85%|████████▌ | 1458/1714 [00:15<00:02, 104.94it/s, Loss=0.0144, MAPE=54.38%]\u001b[A2025-09-04 14:09:01,401 - INFO - Epoch   1: Train Loss=0.0267, Train MAPE=60036.54% (검증 생략)\n",
      "Training Progress:  10%|█         | 1/10 [00:18<02:45, 18.38s/it, T_Loss=0.0267, V_Loss=inf(이전값), V_MAPE=0.00%, Best=inf, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  21%|██        | 364/1714 [00:03<00:13, 101.13it/s, Loss=0.0321, MAPE=131.03%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  21%|██        | 364/1714 [00:03<00:13, 101.13it/s, Loss=0.0110, MAPE=81.75%] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten 후: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  78%|███████▊  | 1344/1714 [00:13<00:03, 103.81it/s, Loss=0.0235, MAPE=54.70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0최종 결합: torch.Size([50, 52])\n",
      "\n",
      "  결합 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  78%|███████▊  | 1344/1714 [00:13<00:03, 103.81it/s, Loss=0.0362, MAPE=52.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  97%|█████████▋| 1661/1714 [00:16<00:00, 104.06it/s, Loss=0.0158, MAPE=50.23%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "\n",
      "  임베딩 NaN: 0  임베딩 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten 후: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  97%|█████████▋| 1661/1714 [00:16<00:00, 104.06it/s, Loss=0.0091, MAPE=49.57%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  97%|█████████▋| 1661/1714 [00:16<00:00, 104.06it/s, Loss=0.0412, MAPE=55.35%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 범위: [-3.8325, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 5749.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [0, 289]\n",
      "  고유값: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 5749.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0150, 1.0080]\n",
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4450.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 291]\n",
      "  고유값: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 4450.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.3440]\n",
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 7433.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [12, 292]\n",
      "  고유값: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7486, 7433.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:09:20,625 - INFO - Epoch   2: Train Loss=0.0248, Val Loss=0.0205, Train MAPE=57499.59%, Val MAPE=16735.68%\n",
      "Training Progress:  10%|█         | 1/10 [00:37<02:45, 18.38s/it, T_Loss=0.0248, V_Loss=0.0205(검증됨), V_MAPE=16735.68%, Best=inf, Patience=0/20]2025-09-04 14:09:20,640 - INFO -   → Best model saved! (Val Loss: 0.0205)\n",
      "Training Progress:  20%|██        | 2/10 [00:37<02:31, 18.89s/it, T_Loss=0.0248, V_Loss=0.0205(검증됨), V_MAPE=16735.68%, Best=inf, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 데이터: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  51%|█████     | 867/1714 [00:08<00:08, 101.54it/s, Loss=0.0290, MAPE=58.23%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  51%|█████     | 867/1714 [00:08<00:08, 101.54it/s, Loss=0.0189, MAPE=58.40%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten 후: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 [Train]:  56%|█████▋    | 966/1714 [00:09<00:07, 100.86it/s, Loss=0.0213, MAPE=64.06%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 embed_dim: 8  임베딩 결과: torch.Size([50, 4, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  56%|█████▋    | 966/1714 [00:09<00:07, 100.86it/s, Loss=0.0193, MAPE=71.04%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 [Train]:  80%|████████  | 1372/1714 [00:13<00:03, 103.02it/s, Loss=0.0104, MAPE=55.11%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 [Train]:  80%|████████  | 1372/1714 [00:13<00:03, 103.02it/s, Loss=0.0099, MAPE=61.52%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "\n",
      "타겟 데이터: torch.Size([50])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  타겟 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  80%|████████  | 1372/1714 [00:13<00:03, 103.02it/s, Loss=0.0395, MAPE=52.20%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:09:37,794 - INFO - Epoch   3: Train Loss=0.0247, Train MAPE=56787.93% (검증 생략)\n",
      "Training Progress:  30%|███       | 3/10 [00:54<02:06, 18.10s/it, T_Loss=0.0247, V_Loss=0.0205(이전값), V_MAPE=16735.68%, Best=0.0205, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:   6%|▌         | 95/1714 [00:00<00:16, 100.47it/s, Loss=0.0208, MAPE=59.53%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:   6%|▌         | 95/1714 [00:01<00:16, 100.47it/s, Loss=0.0197, MAPE=64.22%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  72%|███████▏  | 1230/1714 [00:12<00:04, 103.92it/s, Loss=0.0120, MAPE=63.70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결합: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  72%|███████▏  | 1230/1714 [00:12<00:04, 103.92it/s, Loss=0.0165, MAPE=52.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 [Train]:  87%|████████▋ | 1494/1714 [00:14<00:02, 102.92it/s, Loss=0.0087, MAPE=55.82%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 결과: torch.Size([50, 4, 8])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  87%|████████▋ | 1494/1714 [00:14<00:02, 102.92it/s, Loss=0.1138, MAPE=53.33%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 5749.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [0, 289]\n",
      "  고유값: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "\n",
      "  결합 범위: [-2.7773, 5749.0000]타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0150, 1.0080]\n",
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4450.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 291]\n",
      "  고유값: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 4450.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.3440]\n",
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 7433.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [12, 292]\n",
      "  고유값: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7486, 7433.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:09:56,790 - INFO - Epoch   4: Train Loss=0.0244, Val Loss=0.0202, Train MAPE=61718.76%, Val MAPE=20951.88%\n",
      "Training Progress:  30%|███       | 3/10 [01:13<02:06, 18.10s/it, T_Loss=0.0244, V_Loss=0.0202(검증됨), V_MAPE=20951.88%, Best=0.0205, Patience=0/20]2025-09-04 14:09:56,816 - INFO -   → Best model saved! (Val Loss: 0.0202)\n",
      "Training Progress:  40%|████      | 4/10 [01:13<01:50, 18.46s/it, T_Loss=0.0244, V_Loss=0.0202(검증됨), V_MAPE=20951.88%, Best=0.0205, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  26%|██▋       | 450/1714 [00:04<00:12, 103.57it/s, Loss=0.1339, MAPE=75.13%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "\n",
      "  임베딩 결과: torch.Size([50, 4, 8])  임베딩 NaN: 0  임베딩 Inf: 0\n",
      "\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  26%|██▋       | 450/1714 [00:04<00:12, 103.57it/s, Loss=0.0182, MAPE=77.79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 범위: [-2.5095, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  27%|██▋       | 461/1714 [00:04<00:12, 97.85it/s, Loss=0.0182, MAPE=77.79%] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  43%|████▎     | 731/1714 [00:07<00:09, 102.44it/s, Loss=0.0164, MAPE=55.96%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 데이터: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  43%|████▎     | 742/1714 [00:07<00:09, 102.68it/s, Loss=0.0164, MAPE=55.96%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  43%|████▎     | 742/1714 [00:07<00:09, 102.68it/s, Loss=0.0183, MAPE=61.68%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  48%|████▊     | 818/1714 [00:08<00:08, 102.54it/s, Loss=0.0137, MAPE=62.32%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  48%|████▊     | 818/1714 [00:08<00:08, 102.54it/s, Loss=0.0151, MAPE=55.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:10:14,087 - INFO - Epoch   5: Train Loss=0.0243, Train MAPE=66567.06% (검증 생략)\n",
      "Training Progress:  50%|█████     | 5/10 [01:31<01:30, 18.03s/it, T_Loss=0.0243, V_Loss=0.0202(이전값), V_MAPE=20951.88%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 [Train]:  48%|████▊     | 831/1714 [00:08<00:08, 101.70it/s, Loss=0.0503, MAPE=61.21%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  48%|████▊     | 831/1714 [00:08<00:08, 101.70it/s, Loss=0.0126, MAPE=69.67%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  70%|███████   | 1205/1714 [00:11<00:04, 102.05it/s, Loss=0.0154, MAPE=67.85%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307  임베딩 embed_dim: 8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  70%|███████   | 1205/1714 [00:11<00:04, 102.05it/s, Loss=0.0130, MAPE=64.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  97%|█████████▋| 1656/1714 [00:16<00:00, 101.68it/s, Loss=0.0077, MAPE=53.02%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 데이터: torch.Size([50, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  값 범위: [3, 296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 [Train]:  97%|█████████▋| 1667/1714 [00:16<00:00, 100.70it/s, Loss=0.0077, MAPE=53.02%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  타겟 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  97%|█████████▋| 1667/1714 [00:16<00:00, 100.70it/s, Loss=0.0335, MAPE=52.16%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 5749.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [0, 289]\n",
      "  고유값: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 5749.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0150, 1.0080]\n",
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4450.0000]\n",
      "\n",
      "범주형 데이터: torch.Size([50, 4])  값 범위: [1, 291]\n",
      "  고유값: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 4450.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.3440]\n",
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 7433.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [12, 292]\n",
      "  고유값: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7486, 7433.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:10:33,191 - INFO - Epoch   6: Train Loss=0.0242, Val Loss=0.0202, Train MAPE=66663.10%, Val MAPE=17262.90%\n",
      "Training Progress:  50%|█████     | 5/10 [01:50<01:30, 18.03s/it, T_Loss=0.0242, V_Loss=0.0202(검증됨), V_MAPE=17262.90%, Best=0.0202, Patience=0/20]2025-09-04 14:10:33,219 - INFO -   → Best model saved! (Val Loss: 0.0202)\n",
      "Training Progress:  60%|██████    | 6/10 [01:50<01:13, 18.41s/it, T_Loss=0.0242, V_Loss=0.0202(검증됨), V_MAPE=17262.90%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  36%|███▌      | 611/1714 [00:06<00:10, 101.82it/s, Loss=0.0169, MAPE=58.79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "최종 결합: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  36%|███▌      | 611/1714 [00:06<00:10, 101.82it/s, Loss=0.0460, MAPE=114.58%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 [Train]:  79%|███████▉  | 1356/1714 [00:13<00:03, 101.52it/s, Loss=0.0218, MAPE=66.65%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  값 범위: [1, 293]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  79%|███████▉  | 1356/1714 [00:13<00:03, 101.52it/s, Loss=0.0767, MAPE=59.25%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 Inf: 0최종 결합: torch.Size([50, 52])  결합 NaN: 0\n",
      "\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 [Train]:  87%|████████▋ | 1486/1714 [00:14<00:02, 100.94it/s, Loss=0.0218, MAPE=74.92%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  임베딩 결과: torch.Size([50, 4, 8])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  87%|████████▋ | 1486/1714 [00:14<00:02, 100.94it/s, Loss=0.0152, MAPE=60.60%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "\n",
      "  타겟 범위: [0.0060, 0.0380]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:10:50,461 - INFO - Epoch   7: Train Loss=0.0239, Train MAPE=68148.45% (검증 생략)\n",
      "Training Progress:  70%|███████   | 7/10 [02:07<00:54, 18.03s/it, T_Loss=0.0239, V_Loss=0.0202(이전값), V_MAPE=17262.90%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]:  14%|█▍        | 239/1714 [00:02<00:14, 101.50it/s, Loss=0.0319, MAPE=55.30%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  14%|█▍        | 239/1714 [00:02<00:14, 101.50it/s, Loss=0.0183, MAPE=59.58%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  48%|████▊     | 820/1714 [00:08<00:08, 102.20it/s, Loss=0.0193, MAPE=58.45%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]:  48%|████▊     | 820/1714 [00:08<00:08, 102.20it/s, Loss=0.0079, MAPE=59.60%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  82%|████████▏ | 1409/1714 [00:13<00:03, 101.37it/s, Loss=0.0191, MAPE=78.16%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  82%|████████▏ | 1409/1714 [00:13<00:03, 101.37it/s, Loss=0.0183, MAPE=82.70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 vocab_size: 307  임베딩 Inf: 0\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 5749.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [0, 289]\n",
      "  고유값: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 5749.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0150, 1.0080]\n",
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4450.0000]범주형 데이터: torch.Size([50, 4])\n",
      "\n",
      "  값 범위: [1, 291]\n",
      "  고유값: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])  임베딩 NaN: 0\n",
      "\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 4450.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.3440]\n",
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 7433.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [12, 292]\n",
      "  고유값: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7486, 7433.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:11:09,772 - INFO - Epoch   8: Train Loss=0.0238, Val Loss=0.0206, Train MAPE=64260.82%, Val MAPE=22810.22%\n",
      "Training Progress:  80%|████████  | 8/10 [02:26<00:36, 18.43s/it, T_Loss=0.0238, V_Loss=0.0206(검증됨), V_MAPE=22810.22%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 [Train]:  18%|█▊        | 304/1714 [00:03<00:13, 101.85it/s, Loss=0.0203, MAPE=59.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  18%|█▊        | 315/1714 [00:03<00:13, 100.58it/s, Loss=0.0203, MAPE=59.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 결합: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  18%|█▊        | 315/1714 [00:03<00:13, 100.58it/s, Loss=0.0149, MAPE=67.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  20%|█▉        | 336/1714 [00:03<00:14, 95.39it/s, Loss=0.0330, MAPE=55.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  20%|█▉        | 336/1714 [00:03<00:14, 95.39it/s, Loss=0.0076, MAPE=63.79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 [Train]:  58%|█████▊    | 993/1714 [00:09<00:07, 102.47it/s, Loss=0.1532, MAPE=54.32%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  58%|█████▊    | 993/1714 [00:09<00:07, 102.47it/s, Loss=0.0346, MAPE=58.47%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:11:27,036 - INFO - Epoch   9: Train Loss=0.0237, Train MAPE=68876.10% (검증 생략)\n",
      "Training Progress:  90%|█████████ | 9/10 [02:44<00:18, 18.07s/it, T_Loss=0.0237, V_Loss=0.0206(이전값), V_MAPE=22810.22%, Best=0.0202, Patience=1/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:   4%|▎         | 61/1714 [00:00<00:16, 100.33it/s, Loss=0.0100, MAPE=57.07%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [3, 296]\n",
      "  고유값: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  결합 Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:   4%|▎         | 61/1714 [00:00<00:16, 100.33it/s, Loss=0.0223, MAPE=67.05%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  결합 범위: [-2.5095, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 293]\n",
      "  고유값: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 결과: torch.Size([50, 4, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:  44%|████▍     | 753/1714 [00:07<00:09, 102.07it/s, Loss=0.0141, MAPE=52.98%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.9151, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0030, 0.0400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:  44%|████▍     | 753/1714 [00:07<00:09, 102.07it/s, Loss=0.0520, MAPE=523899.81%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "연속형 데이터: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:  85%|████████▍ | 1456/1714 [00:14<00:02, 102.40it/s, Loss=0.0216, MAPE=54.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4175.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [9, 301]\n",
      "  고유값: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 [Train]:  85%|████████▍ | 1456/1714 [00:14<00:02, 102.40it/s, Loss=0.0239, MAPE=58.39%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-3.8325, 4175.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 5749.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [0, 289]\n",
      "  고유값: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307  임베딩 embed_dim: 8\n",
      "\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 5749.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0150, 1.0080]\n",
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4450.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [1, 291]\n",
      "  고유값: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7773, 4450.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.3440]\n",
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 7433.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [12, 292]\n",
      "  고유값: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.7486, 7433.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:11:46,228 - INFO - Epoch  10: Train Loss=0.0234, Val Loss=0.0204, Train MAPE=64784.27%, Val MAPE=20515.76%\n",
      "Training Progress: 100%|██████████| 10/10 [03:03<00:00, 18.32s/it, T_Loss=0.0234, V_Loss=0.0204(검증됨), V_MAPE=20515.76%, Best=0.0202, Patience=1/20]\n",
      "2025-09-04 14:11:46,231 - INFO - 훈련 완료, 테스트 시작...\n",
      "2025-09-04 14:11:46,231 - INFO - 모델 로드: 250904_test/lstm_20250904_135229.pth\n",
      "2025-09-04 14:11:46,246 - INFO - 테스트 시작\n",
      "Testing:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 샘플 0 디버깅 ===\n",
      "timekey_hr: 2025072300\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 6911.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [0, 289]\n",
      "  고유값: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 282, 286, 287, 288, 289]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0  결합 범위: [-2.2500, 6911.0000]\n",
      "\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.2450]\n",
      "\n",
      "=== 샘플 1 디버깅 ===\n",
      "timekey_hr: 2025072300\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4500.0000]\n",
      "\n",
      "범주형 데이터: torch.Size([50, 4])  값 범위: [1, 291]\n",
      "  고유값: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 282, 286, 289, 290, 291]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.4695, 4500.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0110, 0.8490]\n",
      "\n",
      "=== 샘플 2 디버깅 ===\n",
      "timekey_hr: 2025072300\n",
      "actual_length: 50\n",
      "연속형 데이터: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  범위: [0.0000, 4721.0000]\n",
      "범주형 데이터: torch.Size([50, 4])\n",
      "  값 범위: [12, 292]\n",
      "  고유값: [12, 34, 56, 78, 89, 100, 112, 145, 200, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 266, 282, 286, 291, 292]\n",
      "임베딩 적용 중...\n",
      "  임베딩 vocab_size: 307\n",
      "  임베딩 embed_dim: 8\n",
      "  임베딩 결과: torch.Size([50, 4, 8])\n",
      "  임베딩 NaN: 0\n",
      "  임베딩 Inf: 0\n",
      "  Flatten 후: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "최종 결합: torch.Size([50, 52])\n",
      "  결합 NaN: 0\n",
      "  결합 Inf: 0\n",
      "  결합 범위: [-2.3043, 4721.0000]\n",
      "타겟 데이터: torch.Size([50])\n",
      "  타겟 NaN: 0\n",
      "  타겟 범위: [0.0130, 1.8050]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 200/200 [00:03<00:00, 59.86it/s]\n",
      "2025-09-04 14:11:49,611 - INFO - 테스트 결과: RMSE=0.1503, MAE=0.0523, MAPE=8027.00%\n",
      "2025-09-04 14:11:49,611 - INFO - 구조화된 예측 결과: 157,944개\n",
      "2025-09-04 14:11:49,613 - INFO - 결과 저장 완료: 250904_test/lstm_20250904_135229_results.json\n",
      "2025-09-04 14:11:51,202 - INFO - 구조화된 예측 결과 저장:\n",
      "2025-09-04 14:11:51,203 - INFO -   - 파일 경로: 250904_test/lstm_20250904_135229_predictions.csv\n",
      "2025-09-04 14:11:51,203 - INFO -   - 저장된 예측 개수: 157,944개\n",
      "2025-09-04 14:11:51,205 - INFO -   - 고유한 timekey_hr: 199개\n",
      "2025-09-04 14:11:51,215 - INFO -   - 고유한 oper_id: 813개\n",
      "2025-09-04 14:11:51,228 - INFO -   - 고유한 first_oper_id: 255개\n",
      "2025-09-04 14:11:51,236 - INFO -   - 고유한 last_oper_id: 249개\n",
      "2025-09-04 14:11:51,237 - INFO -   - 평균 윈도우 길이: 49.7\n",
      "2025-09-04 14:11:51,237 - INFO - ==================================================\n",
      "2025-09-04 14:11:51,238 - INFO - 실험 완료: lstm_20250904_135229\n",
      "2025-09-04 14:11:51,239 - INFO - 모델 타입: lstm\n",
      "2025-09-04 14:11:51,239 - INFO - 테스트 RMSE: 0.1503\n",
      "2025-09-04 14:11:51,240 - INFO - 테스트 MAE: 0.0523\n",
      "2025-09-04 14:11:51,240 - INFO - 테스트 MAPE: 8027.00%\n",
      "2025-09-04 14:11:51,241 - INFO - 총 파라미터 수: 614,657\n",
      "2025-09-04 14:11:51,241 - INFO - 학습 가능한 파라미터 수: 614,657\n",
      "2025-09-04 14:11:51,242 - INFO - 모델 저장 위치: 250904_test/lstm_20250904_135229.pth\n",
      "2025-09-04 14:11:51,242 - INFO - 결과 저장 위치: 250904_test/lstm_20250904_135229_results.json\n",
      "2025-09-04 14:11:51,243 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"메인 실행 함수\"\"\"\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"시계열 시퀀스 모델링\")\n",
    "    parser.add_argument(\"--config-dir\", default=\"configs\", help=\"설정 파일 디렉토리\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"train\", \"eval\"], default=\"train\", help=\"실행 모드\")\n",
    "    parser.add_argument(\"--model-path\", default=None, help=\"평가용 모델 경로\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU 번호\")\n",
    "    parser.add_argument(\"--exp-name\", default=None, help=\"실험명\")\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    # 설정 로드\n",
    "    config = load_config(args.config_dir)\n",
    "    set_random_seeds(42)\n",
    "\n",
    "    # 실험명 설정\n",
    "    if args.exp_name:\n",
    "        exp_name = args.exp_name\n",
    "    else:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_type = config.get(\"model_type\", \"lstm\")\n",
    "        exp_name = f\"{model_type}_{timestamp}\"\n",
    "\n",
    "    # 저장 디렉토리\n",
    "    save_dir = config.get(\"save_dir\", \"models\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_save_path = os.path.join(save_dir, f\"{exp_name}.pth\")\n",
    "\n",
    "    # 로깅 설정\n",
    "    logger = setup_logging(os.path.join(save_dir, config.get(\"log_file\", f\"{exp_name}.log\")))\n",
    "\n",
    "    # 디바이스 설정\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # 데이터로더 생성 (통합 임베딩 방식)\n",
    "    logger.info(\"데이터 로딩 중...\")\n",
    "    train_loader, val_loader, test_loader, categorical_processor, embedding_layer = create_dataloaders(config)\n",
    "\n",
    "    # 모델 생성 (새로운 방식)\n",
    "    logger.info(\"모델 생성 중...\")\n",
    "    model = create_model_from_config_and_dataloader(config, train_loader)\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        # 훈련\n",
    "        logger.info(\"훈련 시작...\")\n",
    "        train_results = train_model(\n",
    "            model, train_loader, val_loader, config, device, model_save_path\n",
    "        )\n",
    "        \n",
    "        logger.info(\"훈련 완료, 테스트 시작...\")\n",
    "        test_results = evaluate_model(\n",
    "            model, test_loader, device, model_save_path, config\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        # 평가\n",
    "        if not args.model_path:\n",
    "            raise ValueError(\"--model-path must be provided in eval mode\")\n",
    "        test_results = evaluate_model(\n",
    "            model, test_loader, device, args.model_path, config\n",
    "        )\n",
    "\n",
    "    # 결과 저장\n",
    "    results = {\n",
    "        \"exp_name\": exp_name,\n",
    "        \"config\": config,\n",
    "        \"test_metrics\": test_results[\"metrics\"],\n",
    "        \"model_info\": test_results.get(\"model_info\", {\n",
    "            \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "            \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            \"model_type\": config.get(\"model_type\", \"unknown\")\n",
    "        })\n",
    "    }\n",
    "\n",
    "    results_path = os.path.join(save_dir, f\"{exp_name}_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "    logger.info(f\"결과 저장 완료: {results_path}\")\n",
    "\n",
    "    # 예측 결과 저장\n",
    "    if \"structured_predictions\" in test_results and test_results[\"structured_predictions\"]:\n",
    "        # 구조화된 예측 결과 저장 (timekey_hr, oper_id 포함)\n",
    "        structured_df = pd.DataFrame(test_results[\"structured_predictions\"])\n",
    "        \n",
    "        # 컬럼명 확인 후 에러 계산 (prediction vs predicted 통일)\n",
    "        pred_col = \"prediction\" if \"prediction\" in structured_df.columns else \"predicted\"\n",
    "        \n",
    "        structured_df[\"error\"] = structured_df[pred_col] - structured_df[\"actual\"]\n",
    "        structured_df[\"abs_error\"] = structured_df[\"error\"].abs()\n",
    "        structured_df[\"abs_percent_error\"] = (\n",
    "            structured_df[\"abs_error\"] / structured_df[\"actual\"].abs().clip(lower=1e-8) * 100\n",
    "        )\n",
    "        \n",
    "        # 구조화된 결과를 메인 예측 파일로 저장\n",
    "        predictions_path = os.path.join(save_dir, f\"{exp_name}_predictions.csv\")\n",
    "        structured_df.to_csv(predictions_path, index=False)\n",
    "        \n",
    "        logger.info(f\"구조화된 예측 결과 저장:\")\n",
    "        logger.info(f\"  - 파일 경로: {predictions_path}\")\n",
    "        logger.info(f\"  - 저장된 예측 개수: {len(structured_df):,}개\")\n",
    "        logger.info(f\"  - 고유한 timekey_hr: {structured_df['timekey_hr'].nunique()}개\")\n",
    "        \n",
    "        # oper_id 정보 출력 (있는 경우)\n",
    "        if 'oper_id' in structured_df.columns:\n",
    "            logger.info(f\"  - 고유한 oper_id: {structured_df['oper_id'].nunique()}개\")\n",
    "        \n",
    "        # 추가 구조 정보 출력 (있는 경우)\n",
    "        if 'first_oper_id' in structured_df.columns:\n",
    "            logger.info(f\"  - 고유한 first_oper_id: {structured_df['first_oper_id'].nunique()}개\")\n",
    "        if 'last_oper_id' in structured_df.columns:\n",
    "            logger.info(f\"  - 고유한 last_oper_id: {structured_df['last_oper_id'].nunique()}개\")\n",
    "        if 'window_length' in structured_df.columns:\n",
    "            avg_window = structured_df['window_length'].mean()\n",
    "            logger.info(f\"  - 평균 윈도우 길이: {avg_window:.1f}\")\n",
    "        \n",
    "    else:\n",
    "        # 구조화된 정보가 없는 경우 기본 방식으로 저장 (호환성 유지)\n",
    "        if \"predictions\" in test_results and \"targets\" in test_results:\n",
    "            predictions_df = pd.DataFrame({\n",
    "                \"actual\": test_results[\"targets\"],\n",
    "                \"predicted\": test_results[\"predictions\"],\n",
    "                \"residual\": test_results[\"targets\"] - test_results[\"predictions\"],\n",
    "                \"abs_error\": np.abs(test_results[\"targets\"] - test_results[\"predictions\"]),\n",
    "                \"abs_percent_error\": (\n",
    "                    np.abs(test_results[\"targets\"] - test_results[\"predictions\"]) / \n",
    "                    np.maximum(np.abs(test_results[\"targets\"]), 1e-8) * 100\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            predictions_path = os.path.join(save_dir, f\"{exp_name}_predictions.csv\")\n",
    "            predictions_df.to_csv(predictions_path, index=False)\n",
    "            \n",
    "            logger.info(f\"기본 예측 결과 저장:\")\n",
    "            logger.info(f\"  - 파일 경로: {predictions_path}\")\n",
    "            logger.info(f\"  - 저장된 예측 개수: {len(predictions_df):,}개\")\n",
    "        else:\n",
    "            logger.warning(\"예측 결과 데이터가 없어 CSV 파일을 저장할 수 없습니다.\")\n",
    "\n",
    "    # 최종 결과 요약\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"실험 완료: {exp_name}\")\n",
    "    logger.info(f\"모델 타입: {config.get('model_type')}\")\n",
    "    logger.info(f\"테스트 RMSE: {test_results['metrics']['rmse']:.4f}\")\n",
    "    logger.info(f\"테스트 MAE: {test_results['metrics']['mae']:.4f}\")\n",
    "    logger.info(f\"테스트 MAPE: {test_results['metrics']['mape']:.2f}%\")\n",
    "\n",
    "    # 모델 정보 출력\n",
    "    model_info = results[\"model_info\"]\n",
    "    if \"total_parameters\" in model_info:\n",
    "        logger.info(f\"총 파라미터 수: {model_info['total_parameters']:,}\")\n",
    "    if \"trainable_parameters\" in model_info:\n",
    "        logger.info(f\"학습 가능한 파라미터 수: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "    logger.info(f\"모델 저장 위치: {model_save_path}\")\n",
    "    logger.info(f\"결과 저장 위치: {results_path}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
