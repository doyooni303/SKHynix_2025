{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â— 5. SKHynix PBL ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ â—\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Œ ê°œìš”\n",
    "\n",
    "ì‹œê°„ëŒ€(timekey_hr) ë‚´ì—ì„œ ê³µì • ìˆœì„œ(oper_id)ë¥¼ ê³ ë ¤í•œ ì‹œí€€ìŠ¤ ê¸°ë°˜ TAT ì˜ˆì¸¡ ëª¨ë¸ì…ë‹ˆë‹¤. ë™ì¼í•œ timekey_hr ë‚´ì˜ oper_idë“¤ì„ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬ ì‹œí€€ìŠ¤ ë°ì´í„°ë¡œ êµ¬ì„±í•˜ê³ , ê° operë³„ ê°œë³„ ì˜ˆì¸¡(sequence-to-sequence)ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë°ì´í„° êµ¬ì¡°**: `[batch_size, sequence_length, feature_dim]`\n",
    "- **sequence_length**: timekey_hr ë‚´ ìµœëŒ€ oper_id ê°œìˆ˜ (í•˜ì´í¼íŒŒë¼ë¯¸í„°)\n",
    "- **feature_dim**: ì—°ì†í˜• ë³€ìˆ˜ ê°œìˆ˜ + ë²”ì£¼í˜• ë³€ìˆ˜ ê°œìˆ˜ Ã— ì„ë² ë”© ì°¨ì›\n",
    "\n",
    "**ì§€ì› ëª¨ë¸**:\n",
    "1. **RNN/LSTM/GRU**: ê¸°ë³¸ ìˆœí™˜ ì‹ ê²½ë§\n",
    "2. **RNN + Self-Attention**: ìˆœí™˜ ì‹ ê²½ë§ì— ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ ì¶”ê°€\n",
    "3. **CNN 1D**: ë‹¤ì¤‘ ì»¤ë„ 1ì°¨ì› í•©ì„±ê³± ì‹ ê²½ë§\n",
    "4. **Transformer Encoder**: Transformer Encoder ê¸°ë°˜ì˜ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
    "\n",
    "### ì„¤ì • ë¡œë”© ë° ì‹œë“œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_dir: str = \"configs\") -> Dict:\n",
    "    \"\"\"YAML ì„¤ì • íŒŒì¼ë“¤ì„ í†µí•©í•˜ì—¬ ë¡œë“œ\"\"\"\n",
    "    configs = {}\n",
    "    config_files = [\"dataset\", \"model\", \"training\"]\n",
    "\n",
    "    for file in config_files:\n",
    "        config_path = os.path.join(config_dir, f\"{file}.yaml\")\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "            configs.update(config)\n",
    "\n",
    "    return configs\n",
    "\n",
    "\n",
    "def set_random_seeds(seed: int = 42):\n",
    "    \"\"\"ì¬í˜„ì„±ì„ ìœ„í•œ ëœë¤ ì‹œë“œ ì„¤ì •\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Global logger ë³€ìˆ˜\n",
    "logger = None\n",
    "\n",
    "def setup_logging(log_file: str = \"training.log\"):\n",
    "    \"\"\"ë¡œê¹… ì„¤ì • ë° global logger ì„¤ì •\"\"\"\n",
    "    global logger\n",
    "    \n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file),\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True  # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±° í›„ ìƒˆë¡œ ì„¤ì •\n",
    "    )\n",
    "    \n",
    "    logger = logging.getLogger(__name__)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalProcessor:\n",
    "    \"\"\"ë²”ì£¼í˜• ë³€ìˆ˜ ì„ë² ë”©ì„ ìœ„í•œ ì²˜ë¦¬ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim: int = 8):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.label_encoders = {}\n",
    "        self.vocab_sizes = {}\n",
    "        self.categorical_columns = []\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, categorical_columns: List[str]):\n",
    "        \"\"\"ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ë²”ì£¼í˜• ì¸ì½”ë” í•™ìŠµ\"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        \n",
    "        for col in categorical_columns:\n",
    "            unique_values = df[col].astype(str).unique()\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(unique_values)\n",
    "            \n",
    "            self.label_encoders[col] = encoder\n",
    "            self.vocab_sizes[col] = len(encoder.classes_)\n",
    "        \n",
    "        logger.info(f\"ë²”ì£¼í˜• ë³€ìˆ˜ë³„ ê³ ìœ ê°’ ê°œìˆ˜:\")\n",
    "        for col in categorical_columns:\n",
    "            logger.info(f\"  {col}: {self.vocab_sizes[col]}ê°œ\")\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"DataFrameì˜ ë²”ì£¼í˜• ì»¬ëŸ¼ë“¤ì„ ìˆ«ìë¡œ ë³€í™˜\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            df_encoded[col] = self.label_encoders[col].transform(\n",
    "                df_encoded[col].astype(str)\n",
    "            )\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def get_vocab_sizes(self) -> List[int]:\n",
    "        \"\"\"ê° ë²”ì£¼í˜• ë³€ìˆ˜ì˜ vocab_size ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "        return [self.vocab_sizes[col] for col in self.categorical_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—‚ï¸ ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤\n",
    "\n",
    "### ë©”ì¸ ë°ì´í„°ì…‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# PyTorch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "def extract_oper_number(oper_id):\n",
    "    \"\"\"oper_idì—ì„œ ìˆ«ì ë¶€ë¶„ ì¶”ì¶œ (ì˜ˆ: 'oper123' -> 123)\"\"\"\n",
    "    match = re.search(r'\\d+', str(oper_id))\n",
    "    return int(match.group()) if match else 0\n",
    "\n",
    "\n",
    "class CategoricalProcessor:\n",
    "    \"\"\"í†µí•©ëœ ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬ê¸° - ëª¨ë“  ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ í•˜ë‚˜ì˜ vocabularyë¡œ í†µí•©\"\"\"\n",
    "    \n",
    "    def __init__(self, categorical_columns: List[str], categories: List[int], embedding_dim: int = 8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            categorical_columns: ë²”ì£¼í˜• ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [\"oper_group\", \"days\", \"shift\", \"x1\"])\n",
    "            categories: ê° ë²”ì£¼í˜• ë³€ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ìˆ˜ (ì˜ˆ: [277, 7, 3, 20])\n",
    "            embedding_dim: ì„ë² ë”© ì°¨ì›\n",
    "        \"\"\"\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.categories = categories\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # ê° ë³€ìˆ˜ë³„ ì¸ì½”ë”ì™€ ì˜¤í”„ì…‹ ê³„ì‚°\n",
    "        self.label_encoders = {}\n",
    "        self.category_offsets = {}  # ê° ë³€ìˆ˜ì˜ ì‹œì‘ ì¸ë±ìŠ¤\n",
    "        self.category_ranges = {}   # ê° ë³€ìˆ˜ì˜ (start, end) ë²”ìœ„\n",
    "        \n",
    "        # í†µí•© vocabulary í¬ê¸° ê³„ì‚°\n",
    "        self.total_vocab_size = sum(categories)\n",
    "        \n",
    "        logger.info(f\"í†µí•©ëœ ë²”ì£¼í˜• ë³€ìˆ˜ ì„¤ì •:\")\n",
    "        logger.info(f\"  - ë³€ìˆ˜ë³„ ì¹´í…Œê³ ë¦¬ ìˆ˜: {dict(zip(categorical_columns, categories))}\")\n",
    "        logger.info(f\"  - ì´ vocabulary í¬ê¸°: {self.total_vocab_size}\")\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame):\n",
    "        \"\"\"ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ í†µí•© ë²”ì£¼í˜• ì¸ì½”ë” í•™ìŠµ\"\"\"\n",
    "        \n",
    "        current_offset = 0\n",
    "        \n",
    "        for i, col in enumerate(self.categorical_columns):\n",
    "            # í•´ë‹¹ ë³€ìˆ˜ì˜ ê³ ìœ ê°’ë“¤ ì¶”ì¶œ\n",
    "            unique_values = df[col].astype(str).unique()\n",
    "            \n",
    "            # LabelEncoder í•™ìŠµ (0ë¶€í„° ì‹œì‘)\n",
    "            encoder = LabelEncoder()\n",
    "            encoder.fit(unique_values)\n",
    "            \n",
    "            # ì‹¤ì œ ê³ ìœ ê°’ ê°œìˆ˜ í™•ì¸\n",
    "            actual_vocab_size = len(encoder.classes_)\n",
    "            expected_vocab_size = self.categories[i]\n",
    "            \n",
    "            if actual_vocab_size != expected_vocab_size:\n",
    "                logger.info(f\"  ê²½ê³ : {col}ì˜ ì‹¤ì œ ì¹´í…Œê³ ë¦¬ ìˆ˜({actual_vocab_size})ê°€ ì„¤ì •ê°’({expected_vocab_size})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤\")\n",
    "                # ì‹¤ì œê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n",
    "                self.categories[i] = actual_vocab_size\n",
    "            \n",
    "            self.label_encoders[col] = encoder\n",
    "            self.category_offsets[col] = current_offset\n",
    "            self.category_ranges[col] = (current_offset, current_offset + actual_vocab_size)\n",
    "            \n",
    "            logger.info(f\"  - {col}: ì¸ë±ìŠ¤ {current_offset}~{current_offset + actual_vocab_size - 1} ({actual_vocab_size}ê°œ)\")\n",
    "            \n",
    "            current_offset += actual_vocab_size\n",
    "        \n",
    "        # ì´ vocabulary í¬ê¸° ì¬ê³„ì‚°\n",
    "        self.total_vocab_size = current_offset\n",
    "        logger.info(f\"  - ìµœì¢… í†µí•© vocabulary í¬ê¸°: {self.total_vocab_size}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"DataFrameì˜ ë²”ì£¼í˜• ì»¬ëŸ¼ë“¤ì„ í†µí•© ì¸ë±ìŠ¤ë¡œ ë³€í™˜\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        for col in self.categorical_columns:\n",
    "            # ë¨¼ì € LabelEncoderë¡œ 0ë¶€í„° ì‹œì‘í•˜ëŠ” ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "            encoded_values = self.label_encoders[col].transform(df_encoded[col].astype(str))\n",
    "            \n",
    "            # ì˜¤í”„ì…‹ ì¶”ê°€í•˜ì—¬ í†µí•© vocabulary ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "            df_encoded[col] = encoded_values + self.category_offsets[col]\n",
    "        \n",
    "        return df_encoded\n",
    "    \n",
    "    def get_total_vocab_size(self) -> int:\n",
    "        \"\"\"í†µí•©ëœ ì´ vocabulary í¬ê¸° ë°˜í™˜\"\"\"\n",
    "        return self.total_vocab_size\n",
    "    \n",
    "    def get_category_info(self) -> Dict:\n",
    "        \"\"\"ë²”ì£¼í˜• ë³€ìˆ˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜\"\"\"\n",
    "        return {\n",
    "            'categorical_columns': self.categorical_columns,\n",
    "            'categories': self.categories,\n",
    "            'category_offsets': self.category_offsets,\n",
    "            'category_ranges': self.category_ranges,\n",
    "            'total_vocab_size': self.total_vocab_size\n",
    "        }\n",
    "    \n",
    "    def decode_categorical_value(self, encoded_value: int) -> Tuple[str, str]:\n",
    "        \"\"\"í†µí•© ì¸ë±ìŠ¤ë¥¼ ì›ë˜ (ë³€ìˆ˜ëª…, ê°’) ìœ¼ë¡œ ë””ì½”ë”©\"\"\"\n",
    "        for col in self.categorical_columns:\n",
    "            start, end = self.category_ranges[col]\n",
    "            if start <= encoded_value < end:\n",
    "                # ì˜¤í”„ì…‹ ì œê±°í•˜ì—¬ ì›ë˜ LabelEncoder ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "                original_idx = encoded_value - self.category_offsets[col]\n",
    "                # ì›ë˜ ê°’ ë³µì›\n",
    "                original_value = self.label_encoders[col].inverse_transform([original_idx])[0]\n",
    "                return col, original_value\n",
    "        \n",
    "        return \"unknown\", \"unknown\"\n",
    "\n",
    "\n",
    "class SequenceOperDataset(Dataset):\n",
    "    \"\"\"ì‹œí€€ìŠ¤ ê¸°ë°˜ oper ë°ì´í„°ì…‹ - í†µí•© ì„ë² ë”© + Window sliding\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        categorical_columns: List[str],\n",
    "        continuous_columns: List[str],\n",
    "        categories: List[int],  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°\n",
    "        target_column: str = \"y\",\n",
    "        categorical_processor: Optional[CategoricalProcessor] = None,\n",
    "        window_size: int = 30,\n",
    "        stride: int = None,\n",
    "        embedding_dim: int = 8,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        self.df = df.copy()\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.continuous_columns = continuous_columns\n",
    "        self.categories = categories\n",
    "        self.target_column = target_column\n",
    "        self.window_size = window_size\n",
    "        self.stride = stride if stride is not None else window_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # í†µí•© ë²”ì£¼í˜• ë°ì´í„° ì²˜ë¦¬ê¸° ì„¤ì •\n",
    "        if categorical_processor is None:\n",
    "            self.categorical_processor = CategoricalProcessor(\n",
    "                categorical_columns, categories, embedding_dim\n",
    "            )\n",
    "            self.categorical_processor.fit(df)\n",
    "        else:\n",
    "            self.categorical_processor = categorical_processor\n",
    "        \n",
    "        # í†µí•© ì„ë² ë”© ë ˆì´ì–´ ìƒì„± (í•™ìŠµ ê°€ëŠ¥)\n",
    "        total_vocab_size = self.categorical_processor.get_total_vocab_size()\n",
    "        self.categorical_embedding = nn.Embedding(\n",
    "            total_vocab_size, embedding_dim, padding_idx=0\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„° ì „ì²˜ë¦¬ ë° ì‹œí€€ìŠ¤ ìƒì„±\n",
    "        self._preprocess_data()\n",
    "        self._create_windowed_sequences()\n",
    "        \n",
    "        logger.info(f\"ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ:\")\n",
    "        logger.info(f\"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(self.samples)}\")\n",
    "        logger.info(f\"  - Window í¬ê¸°: {window_size}\")\n",
    "        logger.info(f\"  - Stride: {self.stride}\")\n",
    "        logger.info(f\"  - ì—°ì†í˜• ì°¨ì›: {len(continuous_columns)}\")\n",
    "        logger.info(f\"  - ë²”ì£¼í˜• ë³€ìˆ˜ ìˆ˜: {len(categorical_columns)}\")\n",
    "        logger.info(f\"  - ì„ë² ë”© ì°¨ì›: {embedding_dim}\")\n",
    "        logger.info(f\"  - ìµœì¢… íŠ¹ì„± ì°¨ì›: {len(continuous_columns) + len(categorical_columns) * embedding_dim}\")\n",
    "        logger.info(f\"  - íŒ¨ë”©ê°’: {padding_value}\")\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        # timekey_hrì—ì„œ ë‚ ì§œ(day) ì¶”ì¶œ\n",
    "        self.df['date'] = (self.df['timekey_hr'] // 100).astype(int)\n",
    "        \n",
    "        # í†µí•© ë²”ì£¼í˜• ë°ì´í„° ì¸ì½”ë”©\n",
    "        if self.categorical_columns:\n",
    "            self.df = self.categorical_processor.transform(self.df)\n",
    "        \n",
    "        # ìµœì¢… íŠ¹ì„± ì°¨ì› ê³„ì‚°\n",
    "        self.continuous_dim = len(self.continuous_columns)\n",
    "        self.categorical_dim = len(self.categorical_columns) * self.embedding_dim\n",
    "        self.feature_dim = self.continuous_dim + self.categorical_dim\n",
    "    \n",
    "    def _create_windowed_sequences(self):\n",
    "        \"\"\"timekey_hrë³„ë¡œ window slidingí•˜ì—¬ ì‹œí€€ìŠ¤ ìƒì„±\"\"\"\n",
    "        self.samples = []\n",
    "        \n",
    "        # timekey_hrë³„ë¡œ ê·¸ë£¹í™”\n",
    "        grouped = self.df.groupby('timekey_hr')\n",
    "        \n",
    "        for timekey_hr, group in grouped:\n",
    "            # oper_id ìˆœì„œë¡œ ì •ë ¬\n",
    "            group_sorted = group.iloc[group['oper_id'].map(extract_oper_number).argsort()].reset_index(drop=True)\n",
    "            \n",
    "            if len(group_sorted) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Window sliding\n",
    "            for start_idx in range(0, len(group_sorted), self.stride):\n",
    "                end_idx = start_idx + self.window_size\n",
    "                \n",
    "                # ìœˆë„ìš° ë°ì´í„° ì¶”ì¶œ\n",
    "                window_data = group_sorted.iloc[start_idx:min(end_idx, len(group_sorted))]\n",
    "                actual_length = len(window_data)\n",
    "                \n",
    "                if actual_length == 0:\n",
    "                    continue\n",
    "                \n",
    "                # ì—°ì†í˜• ë°ì´í„° ì¶”ì¶œ ë° íŒ¨ë”©\n",
    "                if self.continuous_columns:\n",
    "                    continuous_data = window_data[self.continuous_columns].values\n",
    "                    if actual_length < self.window_size:\n",
    "                        padding_rows = self.window_size - actual_length\n",
    "                        padding_matrix = np.full(\n",
    "                            (padding_rows, len(self.continuous_columns)), \n",
    "                            self.padding_value, \n",
    "                            dtype=np.float32\n",
    "                        )\n",
    "                        continuous_data = np.vstack([continuous_data, padding_matrix])\n",
    "                    continuous_data = continuous_data.astype(np.float32)\n",
    "                else:\n",
    "                    continuous_data = np.empty((self.window_size, 0), dtype=np.float32)\n",
    "                \n",
    "                # ë²”ì£¼í˜• ë°ì´í„° ì¶”ì¶œ ë° íŒ¨ë”© (í†µí•© ì¸ë±ìŠ¤, 1ë¶€í„° ì‹œì‘)\n",
    "                if self.categorical_columns:\n",
    "                    categorical_data = window_data[self.categorical_columns].values\n",
    "                    if actual_length < self.window_size:\n",
    "                        padding_rows = self.window_size - actual_length\n",
    "                        # íŒ¨ë”©ì€ 0ìœ¼ë¡œ (íŒ¨ë”©ìš© ì¸ë±ìŠ¤)\n",
    "                        padding_matrix = np.zeros(\n",
    "                            (padding_rows, len(self.categorical_columns)), \n",
    "                            dtype=np.int64\n",
    "                        )\n",
    "                        categorical_data = np.vstack([categorical_data, padding_matrix])\n",
    "                    categorical_data = categorical_data.astype(np.int64)\n",
    "                else:\n",
    "                    categorical_data = np.empty((self.window_size, 0), dtype=np.int64)\n",
    "                \n",
    "                # íƒ€ê²Ÿ ë°ì´í„° ì¶”ì¶œ ë° íŒ¨ë”©\n",
    "                target_data = window_data[self.target_column].values\n",
    "                if actual_length < self.window_size:\n",
    "                    padding_rows = self.window_size - actual_length\n",
    "                    padding_targets = np.full(padding_rows, self.padding_value, dtype=np.float32)\n",
    "                    target_data = np.hstack([target_data, padding_targets])\n",
    "                target_data = target_data.astype(np.float32)\n",
    "                \n",
    "                # ë§ˆìŠ¤í¬ ìƒì„± (True = íŒ¨ë”©ëœ ìœ„ì¹˜)\n",
    "                mask = np.zeros(self.window_size, dtype=bool)\n",
    "                if actual_length < self.window_size:\n",
    "                    mask[actual_length:] = True\n",
    "                \n",
    "                # oper_id ì •ë³´ (ì „ì²´ ë¦¬ìŠ¤íŠ¸)\n",
    "                oper_ids = window_data['oper_id'].values.tolist()\n",
    "                if actual_length < self.window_size:\n",
    "                    oper_ids.extend([None] * (self.window_size - actual_length))\n",
    "                \n",
    "                sample_info = {\n",
    "                    'timekey_hr': timekey_hr,\n",
    "                    'oper_ids_list': oper_ids,\n",
    "                    'continuous_data': continuous_data,\n",
    "                    'categorical_data': categorical_data,\n",
    "                    'target_data': target_data,\n",
    "                    'mask': mask,\n",
    "                    'actual_length': actual_length,\n",
    "                    'first_oper_id': window_data['oper_id'].iloc[0],\n",
    "                    'last_oper_id': window_data['oper_id'].iloc[-1]\n",
    "                }\n",
    "                \n",
    "                self.samples.append(sample_info)\n",
    "        \n",
    "        logger.info(f\"Window sliding ê²°ê³¼:\")\n",
    "        logger.info(f\"  - ì´ ìƒ˜í”Œ ìˆ˜: {len(self.samples)}\")\n",
    "        if self.samples:\n",
    "            actual_lengths = [sample['actual_length'] for sample in self.samples]\n",
    "            logger.info(f\"  - í‰ê·  ì‹¤ì œ ê¸¸ì´: {np.mean(actual_lengths):.1f}\")\n",
    "            logger.info(f\"  - ìµœëŒ€ ì‹¤ì œ ê¸¸ì´: {np.max(actual_lengths)}\")\n",
    "            logger.info(f\"  - ìµœì†Œ ì‹¤ì œ ê¸¸ì´: {np.min(actual_lengths)}\")\n",
    "            \n",
    "            timekey_hrs = [sample['timekey_hr'] for sample in self.samples]\n",
    "            unique_timekey_hrs = len(set(timekey_hrs))\n",
    "            logger.info(f\"  - ê³ ìœ í•œ timekey_hr: {unique_timekey_hrs}ê°œ\")\n",
    "            logger.info(f\"  - timekey_hrë‹¹ í‰ê·  ìƒ˜í”Œ ìˆ˜: {len(self.samples)/unique_timekey_hrs:.1f}ê°œ\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    # def __getitem__(self, idx):\n",
    "    #     sample = self.samples[idx]\n",
    "        \n",
    "    #     # ì—°ì†í˜• ë°ì´í„°\n",
    "    #     continuous_data = torch.tensor(sample['continuous_data'])  # [window_size, continuous_dim]\n",
    "        \n",
    "    #     # ë²”ì£¼í˜• ë°ì´í„° â†’ ì„ë² ë”© ì ìš© â†’ flatten\n",
    "    #     categorical_data = torch.tensor(sample['categorical_data'])  # [window_size, num_categorical]\n",
    "        \n",
    "    #     # ì„ë² ë”© ì ìš©: [window_size, num_categorical, embed_dim]\n",
    "    #     with torch.no_grad():  # ì—¬ê¸°ì„œëŠ” gradient ê³„ì‚° ì•ˆí•¨ (forwardì—ì„œ ê³„ì‚°)\n",
    "    #         categorical_embedded = self.categorical_embedding(categorical_data)\n",
    "        \n",
    "    #     # Flatten: [window_size, num_categorical * embed_dim]\n",
    "    #     window_size, num_categorical, embed_dim = categorical_embedded.shape\n",
    "    #     categorical_flattened = categorical_embedded.view(window_size, num_categorical * embed_dim)\n",
    "        \n",
    "    #     # ì—°ì†í˜• + ë²”ì£¼í˜• ê²°í•©: [window_size, total_feature_dim]\n",
    "    #     if continuous_data.numel() > 0:  # ì—°ì†í˜• ë³€ìˆ˜ê°€ ìˆëŠ” ê²½ìš°\n",
    "    #         combined_features = torch.cat([continuous_data, categorical_flattened], dim=-1)\n",
    "    #     else:  # ì—°ì†í˜• ë³€ìˆ˜ê°€ ì—†ëŠ” ê²½ìš°\n",
    "    #         combined_features = categorical_flattened\n",
    "        \n",
    "    #     return {\n",
    "    #         'features': combined_features,  # ìµœì¢… ê²°í•©ëœ íŠ¹ì„±\n",
    "    #         'targets': torch.tensor(sample['target_data']),\n",
    "    #         'masks': torch.tensor(sample['mask']),\n",
    "    #         'actual_length': sample['actual_length'],\n",
    "    #         'timekey_hr': sample['timekey_hr'],\n",
    "    #         'oper_ids_list': sample['oper_ids_list'],\n",
    "    #         'first_oper_id': sample['first_oper_id'],\n",
    "    #         'last_oper_id': sample['last_oper_id'],\n",
    "    #         # ë””ë²„ê¹…ìš© ì›ë³¸ ë°ì´í„°ë„ í¬í•¨\n",
    "    #         'continuous_data': continuous_data,\n",
    "    #         'categorical_data': categorical_data\n",
    "    #     }\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"ë””ë²„ê¹… ë²„ì „ì˜ __getitem__ ë©”ì†Œë“œ\"\"\"\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "\n",
    "        continuous_data = torch.tensor(sample['continuous_data'])\n",
    "        categorical_data = torch.tensor(sample['categorical_data'])\n",
    "        \n",
    "        # ì„ë² ë”© ì ìš© ì „ ë²”ìœ„ ì²´í¬\n",
    "        max_index = categorical_data.max()\n",
    "        vocab_size = self.categorical_embedding.num_embeddings\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            categorical_embedded = self.categorical_embedding(categorical_data)\n",
    "        \n",
    "        # Flatten\n",
    "        window_size, num_categorical, embed_dim = categorical_embedded.shape\n",
    "        categorical_flattened = categorical_embedded.view(window_size, num_categorical * embed_dim)\n",
    "        \n",
    "        if continuous_data.numel() > 0:\n",
    "            combined_features = torch.cat([continuous_data, categorical_flattened], dim=-1)\n",
    "        else:\n",
    "            combined_features = categorical_flattened\n",
    "        \n",
    "        targets = torch.tensor(sample['target_data'])\n",
    "\n",
    "        # # # ë””ë²„ê¹… ì •ë³´ (ì²˜ìŒ ëª‡ ê°œ ìƒ˜í”Œë§Œ)\n",
    "        # debug_mode = idx < 3\n",
    "        # if debug_mode:\n",
    "        #     print(f\"\\n=== ìƒ˜í”Œ {idx} ë””ë²„ê¹… ===\")\n",
    "        #     print(f\"timekey_hr: {sample['timekey_hr']}\")\n",
    "        #     print(f\"actual_length: {sample['actual_length']}\")\n",
    "\n",
    "        # # 1. ì—°ì†í˜• ë°ì´í„° í™•ì¸\n",
    "        #     print(f\"ì—°ì†í˜• ë°ì´í„°: {continuous_data.shape}\")\n",
    "        #     print(f\"  NaN: {torch.isnan(continuous_data).sum()}\")\n",
    "        #     print(f\"  Inf: {torch.isinf(continuous_data).sum()}\")\n",
    "        #     print(f\"  ë²”ìœ„: [{continuous_data.min():.4f}, {continuous_data.max():.4f}]\")\n",
    "        \n",
    "        # # 2. ë²”ì£¼í˜• ë°ì´í„° í™•ì¸  \n",
    "        #     print(f\"ë²”ì£¼í˜• ë°ì´í„°: {categorical_data.shape}\")\n",
    "        #     print(f\"  ê°’ ë²”ìœ„: [{categorical_data.min()}, {categorical_data.max()}]\")\n",
    "        #     print(f\"  ê³ ìœ ê°’: {torch.unique(categorical_data).tolist()}\")\n",
    "        \n",
    "        # # 3. ì„ë² ë”© ì ìš© (ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„)\n",
    "        # try:\n",
    "        #     if debug_mode:\n",
    "        #         print(f\"ì„ë² ë”© ì ìš© ì¤‘...\")\n",
    "        #         print(f\"  ì„ë² ë”© vocab_size: {self.categorical_embedding.num_embeddings}\")\n",
    "        #         print(f\"  ì„ë² ë”© embed_dim: {self.categorical_embedding.embedding_dim}\")\n",
    "                \n",
    "        #         if max_index >= vocab_size:\n",
    "        #             print(f\"âŒ CRITICAL: ì„ë² ë”© ë²”ìœ„ ì´ˆê³¼!\")\n",
    "        #             print(f\"   ìµœëŒ€ ì¸ë±ìŠ¤: {max_index}, Vocab í¬ê¸°: {vocab_size}\")\n",
    "        #             print(f\"   ë²”ì£¼í˜• ë°ì´í„°: {categorical_data}\")\n",
    "        #             raise ValueError(f\"Embedding index out of range: {max_index} >= {vocab_size}\")\n",
    "                \n",
    "        #         print(f\"  ì„ë² ë”© ê²°ê³¼: {categorical_embedded.shape}\")\n",
    "        #         print(f\"  ì„ë² ë”© NaN: {torch.isnan(categorical_embedded).sum()}\")\n",
    "        #         print(f\"  ì„ë² ë”© Inf: {torch.isinf(categorical_embedded).sum()}\")\n",
    "\n",
    "        #         print(f\"  Flatten í›„: {categorical_flattened.shape}\")\n",
    "        #         print(f\"  Flatten NaN: {torch.isnan(categorical_flattened).sum()}\")\n",
    "            \n",
    "        # except Exception as e:\n",
    "        #     print(f\"âŒ ì„ë² ë”© ì ìš© ì‹¤íŒ¨ (ìƒ˜í”Œ {idx}): {e}\")\n",
    "        #     print(f\"   categorical_data ìƒì„¸:\")\n",
    "        #     print(f\"   í˜•íƒœ: {categorical_data.shape}\")\n",
    "        #     print(f\"   íƒ€ì…: {categorical_data.dtype}\")  \n",
    "        #     print(f\"   ê°’: {categorical_data}\")\n",
    "        #     raise e\n",
    "        \n",
    "        \n",
    "        # if debug_mode:\n",
    "            \n",
    "        #     # 4. ì—°ì†í˜•ê³¼ ë²”ì£¼í˜• ê²°í•©\n",
    "        #     print(f\"ìµœì¢… ê²°í•©: {combined_features.shape}\")\n",
    "        #     print(f\"  ê²°í•© NaN: {torch.isnan(combined_features).sum()}\")\n",
    "        #     print(f\"  ê²°í•© Inf: {torch.isinf(combined_features).sum()}\")\n",
    "        #     if torch.isnan(combined_features).sum() == 0 and torch.isinf(combined_features).sum() == 0:\n",
    "        #         print(f\"  ê²°í•© ë²”ìœ„: [{combined_features.min():.4f}, {combined_features.max():.4f}]\")\n",
    "        \n",
    "        #     # 5. íƒ€ê²Ÿ ë°ì´í„° í™•ì¸\n",
    "        #     print(f\"íƒ€ê²Ÿ ë°ì´í„°: {targets.shape}\")\n",
    "        #     print(f\"  íƒ€ê²Ÿ NaN: {torch.isnan(targets).sum()}\")\n",
    "        #     print(f\"  íƒ€ê²Ÿ ë²”ìœ„: [{targets.min():.4f}, {targets.max():.4f}]\")\n",
    "        \n",
    "        # ìµœì¢… ë°˜í™˜ê°’ì— NaN/Infê°€ ìˆëŠ”ì§€ ì²´í¬\n",
    "        final_nan = (torch.isnan(combined_features).sum() + \n",
    "                    torch.isnan(targets).sum() +\n",
    "                    torch.isinf(combined_features).sum() + \n",
    "                    torch.isinf(targets).sum())\n",
    "        \n",
    "        if final_nan > 0:\n",
    "            print(f\"âŒ CRITICAL: ìƒ˜í”Œ {idx}ì—ì„œ ìµœì¢… NaN/Inf ë°œê²¬!\")\n",
    "            print(f\"   Features NaN: {torch.isnan(combined_features).sum()}\")\n",
    "            print(f\"   Features Inf: {torch.isinf(combined_features).sum()}\")  \n",
    "            print(f\"   Targets NaN: {torch.isnan(targets).sum()}\")\n",
    "            print(f\"   Targets Inf: {torch.isinf(targets).sum()}\")\n",
    "            \n",
    "            # ì–´ë””ì„œ NaNì´ ìƒê²¼ëŠ”ì§€ ì¶”ì \n",
    "            if torch.isnan(continuous_data).sum() > 0:\n",
    "                print(f\"   â†’ ì—°ì†í˜• ë°ì´í„°ì—ì„œ NaN ë°œìƒ\")\n",
    "            if torch.isnan(categorical_flattened).sum() > 0:\n",
    "                print(f\"   â†’ ë²”ì£¼í˜• ì„ë² ë”©ì—ì„œ NaN ë°œìƒ\")\n",
    "        \n",
    "            print(f\"=== ìƒ˜í”Œ {idx} ë””ë²„ê¹… ì™„ë£Œ ===\\n\")\n",
    "        \n",
    "        return {\n",
    "            'features': combined_features,\n",
    "            'targets': targets,\n",
    "            'masks': torch.tensor(sample['mask']),\n",
    "            'actual_length': sample['actual_length'],\n",
    "            'timekey_hr': sample['timekey_hr'],\n",
    "            'oper_ids_list': sample['oper_ids_list'],\n",
    "            'first_oper_id': sample['first_oper_id'],\n",
    "            'last_oper_id': sample['last_oper_id'],\n",
    "            'continuous_data': continuous_data,\n",
    "            'categorical_data': categorical_data\n",
    "        }\n",
    "    \n",
    "    def get_embedding_layer(self) -> nn.Embedding:\n",
    "        \"\"\"ì„ë² ë”© ë ˆì´ì–´ ë°˜í™˜ (ëª¨ë¸ì—ì„œ ì‚¬ìš©)\"\"\"\n",
    "        return self.categorical_embedding\n",
    "    \n",
    "    def get_feature_dim(self) -> int:\n",
    "        \"\"\"ìµœì¢… íŠ¹ì„± ì°¨ì› ë°˜í™˜\"\"\"\n",
    "        return self.feature_dim\n",
    "\n",
    "\n",
    "def split_data_by_days(\n",
    "    df: pd.DataFrame, \n",
    "    train_ratio: float = 0.8, \n",
    "    val_ratio: float = 0.1, \n",
    "    test_ratio: float = 0.1\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„í• \"\"\"\n",
    "    \n",
    "    # timekey_hrì—ì„œ ë‚ ì§œ(day) ì¶”ì¶œ\n",
    "    df['date'] = (df['timekey_hr'].astype(int) // 100).astype(int)\n",
    "    \n",
    "    # ê³ ìœ í•œ ë‚ ì§œë“¤ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬\n",
    "    unique_dates = sorted(df['date'].unique())\n",
    "    total_days = len(unique_dates)\n",
    "    \n",
    "    # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë¶„í•  ì¸ë±ìŠ¤ ê³„ì‚°\n",
    "    train_days = int(total_days * train_ratio)\n",
    "    val_days = int(total_days * val_ratio)\n",
    "    \n",
    "    train_dates = unique_dates[:train_days]\n",
    "    val_dates = unique_dates[train_days:train_days + val_days]\n",
    "    test_dates = unique_dates[train_days + val_days:]\n",
    "    \n",
    "    # ê° ë¶„í• ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ\n",
    "    train_df = df[df['date'].isin(train_dates)].copy()\n",
    "    val_df = df[df['date'].isin(val_dates)].copy()\n",
    "    test_df = df[df['date'].isin(test_dates)].copy()\n",
    "    \n",
    "    logger.info(f\"ë‚ ì§œ ê¸°ì¤€ ë°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "    logger.info(f\"  - ì´ ë‚ ì§œ ìˆ˜: {total_days}ì¼\")\n",
    "    logger.info(f\"  - Train: {len(train_dates)}ì¼ ({len(train_df):,}í–‰)\")\n",
    "    logger.info(f\"  - Validation: {len(val_dates)}ì¼ ({len(val_df):,}í–‰)\")  \n",
    "    logger.info(f\"  - Test: {len(test_dates)}ì¼ ({len(test_df):,}í–‰)\")\n",
    "    logger.info(f\"  - Train ë‚ ì§œ ë²”ìœ„: {min(train_dates)} ~ {max(train_dates)}\")\n",
    "    logger.info(f\"  - Val ë‚ ì§œ ë²”ìœ„: {min(val_dates)} ~ {max(val_dates)}\")\n",
    "    logger.info(f\"  - Test ë‚ ì§œ ë²”ìœ„: {min(test_dates)} ~ {max(test_dates)}\")\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "def sequence_collate_fn(batch):\n",
    "    \"\"\"ì‹œí€€ìŠ¤ ë°°ì¹˜ collate í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ì£¼ìš” í…ì„œë“¤ ì¶”ì¶œ\n",
    "    features = torch.stack([sample['features'] for sample in batch])\n",
    "    targets = torch.stack([sample['targets'] for sample in batch])\n",
    "    masks = torch.stack([sample['masks'] for sample in batch])\n",
    "    \n",
    "    # ë©”íƒ€ ì •ë³´ë“¤\n",
    "    actual_lengths = [sample['actual_length'] for sample in batch]\n",
    "    timekey_hrs = [sample['timekey_hr'] for sample in batch]\n",
    "    oper_ids_lists = [sample['oper_ids_list'] for sample in batch]\n",
    "    first_oper_ids = [sample['first_oper_id'] for sample in batch]\n",
    "    last_oper_ids = [sample['last_oper_id'] for sample in batch]\n",
    "    \n",
    "    # ë””ë²„ê¹…ìš© ì›ë³¸ ë°ì´í„°\n",
    "    continuous_data = torch.stack([sample['continuous_data'] for sample in batch])\n",
    "    categorical_data = torch.stack([sample['categorical_data'] for sample in batch])\n",
    "    \n",
    "    return {\n",
    "        'features': features,  # ìµœì¢… ê²°í•©ëœ íŠ¹ì„± [batch_size, window_size, total_feature_dim]\n",
    "        'targets': targets,\n",
    "        'masks': masks,\n",
    "        'actual_lengths': actual_lengths,\n",
    "        'timekey_hrs': timekey_hrs,\n",
    "        'oper_ids_lists': oper_ids_lists,\n",
    "        'first_oper_ids': first_oper_ids,\n",
    "        'last_oper_ids': last_oper_ids,\n",
    "        # ë””ë²„ê¹…ìš©\n",
    "        'continuous_data': continuous_data,\n",
    "        'categorical_data': categorical_data\n",
    "    }\n",
    "\n",
    "def create_dataloaders(dataset_config: Dict) -> Tuple[DataLoader, DataLoader, DataLoader, CategoricalProcessor, nn.Embedding]:\n",
    "    \"\"\"ë°ì´í„°ë¡œë” ìƒì„± - ë‚ ì§œ ê¸°ì¤€ ë¶„í•  + í†µí•© ì„ë² ë”© + Window sliding\"\"\"\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    data_path = dataset_config[\"file_path\"]\n",
    "    excel = pd.read_excel(data_path, sheet_name=None, header=1)\n",
    "    sheet_names = dataset_config[\"sheet_names\"]\n",
    "    \n",
    "    total_df = pd.concat([excel[sheet_name] for sheet_name in sheet_names])\n",
    "    \n",
    "    # ê¸°ë³¸ ì „ì²˜ë¦¬\n",
    "    if \"Unnamed: 0\" in total_df.columns:\n",
    "        total_df.drop(columns=\"Unnamed: 0\", inplace=True)\n",
    "    \n",
    "    # yê°’ ê²°ì¸¡ì¹˜ ì œê±°\n",
    "    df = total_df[~total_df[dataset_config[\"target_column\"]].isna()].copy()\n",
    "    logger.info(f\"yê°’ ì œê±° í›„: {len(df)}í–‰\")\n",
    "\n",
    "    # âœ… Inf ê°’ í™•ì¸ ë° ì²˜ë¦¬ ì¶”ê°€\n",
    "    logger.info(\"\\nInf ê°’ í™•ì¸:\")\n",
    "    continuous_cols = dataset_config[\"continuous_columns\"]\n",
    "    inf_found = False\n",
    "\n",
    "    for col in continuous_cols:\n",
    "        if col in df.columns:\n",
    "            inf_count = np.isinf(df[col]).sum()\n",
    "            if inf_count > 0:\n",
    "                print(f\"  âŒ {col}: {inf_count}ê°œ Inf ê°’\")\n",
    "                inf_found = True\n",
    "                \n",
    "                # Inf ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜ í›„ ì ì ˆí•œ ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "                df.loc[np.isinf(df[col]), col] = np.nan\n",
    "                df[col] = df[col].fillna(1e+5)  # ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "                logger.info(f\"     â†’ ì•„ì£¼ í° ê°’({1e+5})ìœ¼ë¡œ ëŒ€ì²´\")\n",
    "    \n",
    "    if not inf_found:\n",
    "        print(\"  âœ… ì—°ì†í˜• ë³€ìˆ˜ì— Inf ì—†ìŒ\")\n",
    "\n",
    "\n",
    "    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°\n",
    "    drop_columns = dataset_config.get(\"additional_drop_columns\", [])\n",
    "    if drop_columns:\n",
    "        existing_drops = [col for col in drop_columns if col in df.columns]\n",
    "        if existing_drops:\n",
    "            df = df.drop(columns=existing_drops)\n",
    "\n",
    "    \n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    logger.info(f\"ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\")\n",
    "    logger.info(f\"  - ì´ í–‰ ìˆ˜: {len(df):,}ê°œ\")\n",
    "    logger.info(f\"  - ê³ ìœ  timekey_hr: {df['timekey_hr'].nunique()}ê°œ\")\n",
    "    \n",
    "    # í†µí•© ë²”ì£¼í˜• ì²˜ë¦¬ê¸° ìƒì„± ë° í•™ìŠµ\n",
    "    categorical_processor = CategoricalProcessor(\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),  # ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8)\n",
    "    )\n",
    "    categorical_processor.fit(df)\n",
    "    \n",
    "    # ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° ë¶„í• \n",
    "    train_df, val_df, test_df = split_data_by_days(\n",
    "        df,\n",
    "        train_ratio=dataset_config.get(\"train_ratio\", 0.8),\n",
    "        val_ratio=dataset_config.get(\"val_ratio\", 0.1),\n",
    "        test_ratio=dataset_config.get(\"test_ratio\", 0.1)\n",
    "    )\n",
    "    \n",
    "    # Window sliding íŒŒë¼ë¯¸í„°\n",
    "    window_size = dataset_config.get(\"window_size\", 30)\n",
    "    stride = dataset_config.get(\"stride\", None)\n",
    "    padding_value = dataset_config.get(\"padding_value\", -9999.0)\n",
    "    \n",
    "    logger.info(f\"\\nWindow sliding ì„¤ì •:\")\n",
    "    logger.info(f\"  - Window í¬ê¸°: {window_size}\")\n",
    "    logger.info(f\"  - Stride: {stride if stride is not None else window_size} (ë¹„ê²¹ì¹¨)\")\n",
    "    logger.info(f\"  - Padding ê°’: {padding_value}\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ ìƒì„±\n",
    "    train_dataset = SequenceOperDataset(\n",
    "        df=train_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=padding_value\n",
    "    )\n",
    "    \n",
    "    val_dataset = SequenceOperDataset(\n",
    "        df=val_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=padding_value\n",
    "    )\n",
    "    \n",
    "    test_dataset = SequenceOperDataset(\n",
    "        df=test_df,\n",
    "        categorical_columns=dataset_config[\"categorical_columns\"],\n",
    "        continuous_columns=dataset_config[\"continuous_columns\"],\n",
    "        categories=dataset_config.get(\"categories\", []),\n",
    "        target_column=dataset_config[\"target_column\"],\n",
    "        categorical_processor=categorical_processor,\n",
    "        window_size=window_size,\n",
    "        stride=stride,\n",
    "        embedding_dim=dataset_config.get(\"embedding_dim\", 8),\n",
    "        padding_value=padding_value\n",
    "    )\n",
    "    \n",
    "    # ì„ë² ë”© ë ˆì´ì–´ ì¶”ì¶œ (ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìš©ë„)\n",
    "    embedding_layer = train_dataset.get_embedding_layer()\n",
    "    \n",
    "    # ë°ì´í„°ë¡œë” ìƒì„±\n",
    "    batch_size = dataset_config.get(\"batch_size\", 32)\n",
    "    num_workers = dataset_config.get(\"num_workers\", 4)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=sequence_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=sequence_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=sequence_collate_fn,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"\\në°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ:\")\n",
    "    logger.info(f\"  - ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
    "    logger.info(f\"  - Train ë°°ì¹˜ ìˆ˜: {len(train_loader)}\")\n",
    "    logger.info(f\"  - Val ë°°ì¹˜ ìˆ˜: {len(val_loader)}\")\n",
    "    logger.info(f\"  - Test ë°°ì¹˜ ìˆ˜: {len(test_loader)}\")\n",
    "    \n",
    "    # íŠ¹ì„± ì°¨ì› ì •ë³´ ì¶œë ¥\n",
    "    feature_dim = train_dataset.get_feature_dim()\n",
    "    logger.info(f\"  - ìµœì¢… íŠ¹ì„± ì°¨ì›: {feature_dim}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, categorical_processor, embedding_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ë“¤\n",
    "\n",
    "### RNN ê¸°ë³¸ ëª¨ë¸ (models/rnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"ê¸°ë³¸ RNN/LSTM/GRU ëª¨ë¸ - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš©\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± ì°¨ì› (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "        rnn_type: str = \"LSTM\",\n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: bool = True,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # RNN ë ˆì´ì–´ - ì…ë ¥ ì°¨ì›ì´ ì´ë¯¸ ê²°í•©ëœ feature_dim\n",
    "        if rnn_type.upper() == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type.upper() == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # ì¶œë ¥ ì°¨ì› ê³„ì‚°\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_output_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "            masks: [batch_size, seq_len] (True = íŒ¨ë”©)\n",
    "            **kwargs: í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤ (ë¬´ì‹œë¨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = features.shape[:2]\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ë¥¼ ë§ˆìŠ¤í‚¹\n",
    "        masked_features = features.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # RNN forward\n",
    "        rnn_output, _ = self.rnn(masked_features)\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´\n",
    "        predictions = self.output_layer(rnn_output).squeeze(-1)\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN + Self-Attention ëª¨ë¸ (models/attention.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Self-Attention ë©”ì»¤ë‹ˆì¦˜\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, num_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        \n",
    "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, hidden_dim]\n",
    "            mask: [batch_size, seq_len] (True = íŒ¨ë”©)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Multi-head attention\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask\n",
    "        if mask is not None:\n",
    "            attention_mask = mask.unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, seq_len]\n",
    "            attention_scores = attention_scores.masked_fill(attention_mask, float('-inf'))\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention\n",
    "        attended = torch.matmul(attention_weights, V)\n",
    "        attended = attended.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # Residual connection + Layer norm\n",
    "        output = self.layer_norm(x + attended)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class RNNAttentionModel(nn.Module):\n",
    "    \"\"\"RNN + Self-Attention ëª¨ë¸ - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš©\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± ì°¨ì› (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "        rnn_type: str = \"LSTM\", \n",
    "        hidden_dim: int = 128,\n",
    "        num_layers: int = 2,\n",
    "        num_attention_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        bidirectional: bool = True,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # RNN ë ˆì´ì–´ - ì…ë ¥ ì°¨ì›ì´ ì´ë¯¸ ê²°í•©ëœ feature_dim\n",
    "        if rnn_type.upper() == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        elif rnn_type.upper() == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        else:  # RNN\n",
    "            self.rnn = nn.RNN(\n",
    "                feature_dim, hidden_dim, num_layers,\n",
    "                batch_first=True, dropout=dropout if num_layers > 1 else 0,\n",
    "                bidirectional=bidirectional\n",
    "            )\n",
    "        \n",
    "        # RNN ì¶œë ¥ ì°¨ì›\n",
    "        rnn_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        \n",
    "        # RNN ì¶œë ¥ì„ ì–´í…ì…˜ ì…ë ¥ ì°¨ì›ìœ¼ë¡œ ë³€í™˜\n",
    "        self.rnn_projection = nn.Linear(rnn_output_dim, hidden_dim)\n",
    "        \n",
    "        # Self-Attention\n",
    "        self.self_attention = SelfAttention(\n",
    "            hidden_dim, num_attention_heads, dropout\n",
    "        )\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "            masks: [batch_size, seq_len] (True = íŒ¨ë”©)\n",
    "            **kwargs: í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤ (ë¬´ì‹œë¨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = features.shape[:2]\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ë¥¼ ë§ˆìŠ¤í‚¹\n",
    "        masked_features = features.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # RNN forward\n",
    "        rnn_output, _ = self.rnn(masked_features)\n",
    "        \n",
    "        # RNN ì¶œë ¥ ì°¨ì› ë³€í™˜\n",
    "        projected_output = self.rnn_projection(rnn_output)\n",
    "        \n",
    "        # Self-Attention ì ìš©\n",
    "        attended_output = self.self_attention(projected_output, masks)\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´\n",
    "        predictions = self.output_layer(attended_output).squeeze(-1)\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 1D ëª¨ë¸ (models/cnn.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1DModel(nn.Module):\n",
    "    \"\"\"1D CNN ëª¨ë¸ (ë‹¤ì¤‘ ì»¤ë„) - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš©\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± ì°¨ì› (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "        kernel_sizes: List[int] = [3, 5, 7],\n",
    "        num_filters: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.padding_value = padding_value\n",
    "        \n",
    "        # ë‹¤ì¤‘ ì»¤ë„ 1D Conv ë ˆì´ì–´ë“¤ - ì…ë ¥ ì°¨ì›ì´ ì´ë¯¸ ê²°í•©ëœ feature_dim\n",
    "        self.conv_layers = nn.ModuleList([\n",
    "            nn.Conv1d(feature_dim, num_filters, kernel_size, padding=kernel_size//2)\n",
    "            for kernel_size in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(num_filters) for _ in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´\n",
    "        total_filters = len(kernel_sizes) * num_filters\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters, total_filters // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(total_filters // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "            masks: [batch_size, seq_len] (True = íŒ¨ë”©)\n",
    "            **kwargs: í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤ (ë¬´ì‹œë¨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = features.shape[:2]\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ë¥¼ ë§ˆìŠ¤í‚¹\n",
    "        masked_features = features.masked_fill(\n",
    "            masks.unsqueeze(-1), self.padding_value\n",
    "        )\n",
    "        \n",
    "        # Conv1dë¥¼ ìœ„í•´ ì°¨ì› ë³€í™˜: [batch, seq_len, features] -> [batch, features, seq_len]\n",
    "        conv_input = masked_features.transpose(1, 2)\n",
    "        \n",
    "        # ë‹¤ì¤‘ ì»¤ë„ Conv1D ì ìš©\n",
    "        conv_outputs = []\n",
    "        for conv, bn in zip(self.conv_layers, self.batch_norms):\n",
    "            conv_out = F.relu(bn(conv(conv_input)))  # [batch, filters, seq_len]\n",
    "            conv_outputs.append(conv_out)\n",
    "        \n",
    "        # ëª¨ë“  ì»¤ë„ ì¶œë ¥ ê²°í•©\n",
    "        combined_conv = torch.cat(conv_outputs, dim=1)  # [batch, total_filters, seq_len]\n",
    "        \n",
    "        # ë‹¤ì‹œ ì›ë˜ ì°¨ì›ìœ¼ë¡œ: [batch, total_filters, seq_len] -> [batch, seq_len, total_filters]\n",
    "        combined_conv = combined_conv.transpose(1, 2)\n",
    "        \n",
    "        # ì¶œë ¥ ë ˆì´ì–´\n",
    "        predictions = self.output_layer(combined_conv).squeeze(-1)\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer(Encoder) ëª¨ë¸(models/transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"ì‚¬ì¸/ì½”ì‚¬ì¸ ìœ„ì¹˜ ì¸ì½”ë”©\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # ì§ìˆ˜ ì¸ë±ìŠ¤\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # í™€ìˆ˜ ì¸ë±ìŠ¤\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:seq_len, :, :].transpose(0, 1)  # [batch_size, seq_len, d_model]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Point-wise Transformer ëª¨ë¸ (ì¸ì½”ë”ë§Œ ì‚¬ìš©) - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš©\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,  # ì…ë ¥ íŠ¹ì„± ì°¨ì› (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”© flatten ê²°ê³¼)\n",
    "        d_model: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 6,\n",
    "        dim_feedforward: int = 1024,\n",
    "        dropout: float = 0.1,\n",
    "        activation: str = \"relu\",\n",
    "        use_positional_encoding: bool = True,\n",
    "        padding_value: float = -9999.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.padding_value = padding_value\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        \n",
    "        # ì…ë ¥ íŠ¹ì„±ì„ d_model ì°¨ì›ìœ¼ë¡œ projection\n",
    "        self.input_projection = nn.Linear(feature_dim, d_model)\n",
    "        \n",
    "        # ìœ„ì¹˜ ì¸ì½”ë”© (ì„ íƒì )\n",
    "        if use_positional_encoding:\n",
    "            self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        # Transformer ì¸ì½”ë” ë ˆì´ì–´\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            batch_first=True,  # [batch_size, seq_len, d_model]\n",
    "            norm_first=False   # Post-norm (í‘œì¤€)\n",
    "        )\n",
    "        \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Point-wise ì¶œë ¥ ë ˆì´ì–´ (ê° ìœ„ì¹˜ë³„ë¡œ ë…ë¦½ì  ì˜ˆì¸¡)\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
    "        self._init_parameters()\n",
    "    \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Xavier uniform ì´ˆê¸°í™”\"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, features, masks, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features: [batch_size, seq_len, feature_dim] - ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„± (ì—°ì†í˜• + ë²”ì£¼í˜• ì„ë² ë”©)\n",
    "            masks: [batch_size, seq_len] (True = íŒ¨ë”© ìœ„ì¹˜)\n",
    "            **kwargs: í˜¸í™˜ì„±ì„ ìœ„í•œ ì¶”ê°€ ì¸ìë“¤ (ë¬´ì‹œë¨)\n",
    "        \n",
    "        Returns:\n",
    "            predictions: [batch_size, seq_len] - ê° ìœ„ì¹˜ë³„ ì˜ˆì¸¡ê°’\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = features.shape\n",
    "        \n",
    "        # ì…ë ¥ íŠ¹ì„±ì„ Transformer ì°¨ì›ìœ¼ë¡œ projection\n",
    "        x = self.input_projection(features)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€ (ì„ íƒì )\n",
    "        if self.use_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        \n",
    "        # Transformer ì¸ì½”ë” ì ìš©\n",
    "        # src_key_padding_mask: Trueì¸ ìœ„ì¹˜ëŠ” attentionì—ì„œ ë¬´ì‹œ\n",
    "        transformer_output = self.transformer_encoder(\n",
    "            x, \n",
    "            src_key_padding_mask=masks\n",
    "        )  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # Point-wise ì˜ˆì¸¡ (ê° ìœ„ì¹˜ë³„ë¡œ ë…ë¦½ì )\n",
    "        predictions = self.output_projection(transformer_output).squeeze(-1)  # [batch_size, seq_len]\n",
    "        \n",
    "        # íŒ¨ë”©ëœ ìœ„ì¹˜ëŠ” 0ìœ¼ë¡œ ë§ˆìŠ¤í‚¹\n",
    "        predictions = predictions.masked_fill(masks, 0.0)\n",
    "        \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ íŒ©í† ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_config: Dict, feature_dim: int):\n",
    "    \"\"\"ì„¤ì •ì— ë”°ë¥¸ ëª¨ë¸ ìƒì„± - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš© (ëª¨ë“  ëª¨ë¸ íƒ€ì… ì§€ì›)\"\"\"\n",
    "    \n",
    "    model_type = model_config.get(\"model_type\", \"lstm\").lower()\n",
    "    dropout = model_config.get(\"dropout\", 0.1)\n",
    "    padding_value = model_config.get(\"padding_value\", -9999.0)\n",
    "    hidden_dim = model_config.get(\"hidden_dim\", 128)\n",
    "    num_layers = model_config.get(\"num_layers\", 2)\n",
    "    bidirectional = model_config.get(\"bidirectional\", True)\n",
    "    \n",
    "    logger.info(f\"ëª¨ë¸ ìƒì„± ì¤‘:\")\n",
    "    logger.info(f\"  - ëª¨ë¸ íƒ€ì…: {model_type}\")\n",
    "    logger.info(f\"  - ì…ë ¥ íŠ¹ì„± ì°¨ì›: {feature_dim}\")\n",
    "    logger.info(f\"  - íˆë“  ì°¨ì›: {hidden_dim}\")\n",
    "    logger.info(f\"  - ë ˆì´ì–´ ìˆ˜: {num_layers}\")\n",
    "    logger.info(f\"  - ë“œë¡­ì•„ì›ƒ: {dropout}\")\n",
    "    \n",
    "    if model_type == \"transformer\":\n",
    "        # Transformer ëª¨ë¸\n",
    "        d_model = model_config.get(\"d_model\", 256)\n",
    "        num_heads = model_config.get(\"num_heads\", 8)\n",
    "        dim_feedforward = model_config.get(\"dim_feedforward\", 1024)\n",
    "        activation = model_config.get(\"activation\", \"relu\")\n",
    "        use_positional_encoding = model_config.get(\"use_positional_encoding\", True)\n",
    "        \n",
    "        model = TransformerModel(\n",
    "            feature_dim=feature_dim,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation=activation,\n",
    "            use_positional_encoding=use_positional_encoding,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - d_model: {d_model}\")\n",
    "        logger.info(f\"  - num_heads: {num_heads}\")\n",
    "        logger.info(f\"  - dim_feedforward: {dim_feedforward}\")\n",
    "        logger.info(f\"  - ìœ„ì¹˜ ì¸ì½”ë”©: {'ì‚¬ìš©' if use_positional_encoding else 'ë¯¸ì‚¬ìš©'}\")\n",
    "        \n",
    "    elif model_type in [\"rnn\", \"lstm\", \"gru\"]:\n",
    "        # ê¸°ë³¸ RNN/LSTM/GRU ëª¨ë¸\n",
    "        model = RNNModel(\n",
    "            feature_dim=feature_dim,\n",
    "            rnn_type=model_type.upper(),\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - RNN íƒ€ì…: {model_type.upper()}\")\n",
    "        logger.info(f\"  - ì–‘ë°©í–¥: {'Yes' if bidirectional else 'No'}\")\n",
    "        \n",
    "    elif model_type in [\"rnn_attention\", \"lstm_attention\", \"gru_attention\"]:\n",
    "        # RNN + Self-Attention ëª¨ë¸\n",
    "        rnn_type = model_type.replace(\"_attention\", \"\").upper()\n",
    "        num_attention_heads = model_config.get(\"num_attention_heads\", 8)\n",
    "        \n",
    "        model = RNNAttentionModel(\n",
    "            feature_dim=feature_dim,\n",
    "            rnn_type=rnn_type,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            num_attention_heads=num_attention_heads,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - RNN íƒ€ì…: {rnn_type}\")\n",
    "        logger.info(f\"  - ì–‘ë°©í–¥: {'Yes' if bidirectional else 'No'}\")\n",
    "        logger.info(f\"  - Attention heads: {num_attention_heads}\")\n",
    "        \n",
    "    elif model_type == \"cnn1d\":\n",
    "        # CNN 1D ëª¨ë¸\n",
    "        kernel_sizes = model_config.get(\"kernel_sizes\", [3, 5, 7])\n",
    "        num_filters = model_config.get(\"num_filters\", 64)\n",
    "        \n",
    "        model = CNN1DModel(\n",
    "            feature_dim=feature_dim,\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            num_filters=num_filters,\n",
    "            dropout=dropout,\n",
    "            padding_value=padding_value\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"  - ì»¤ë„ í¬ê¸°ë“¤: {kernel_sizes}\")\n",
    "        logger.info(f\"  - í•„í„° ìˆ˜: {num_filters}\")\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_type}\")\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚° ë° ì¶œë ¥\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    logger.info(f\"  - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}\")\n",
    "    logger.info(f\"  - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}\")\n",
    "    logger.info(f\"ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model_from_config_and_dataloader(model_config: Dict, train_loader: DataLoader) -> nn.Module:\n",
    "    \"\"\"ë°ì´í„°ë¡œë”ì—ì„œ íŠ¹ì„± ì°¨ì›ì„ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ì²« ë²ˆì§¸ ë°°ì¹˜ì—ì„œ íŠ¹ì„± ì°¨ì› ì¶”ì¶œ\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    feature_dim = sample_batch['features'].shape[-1]  # [batch_size, seq_len, feature_dim]\n",
    "    \n",
    "    logger.info(f\"ë°ì´í„°ë¡œë”ì—ì„œ ì¶”ì¶œí•œ íŠ¹ì„± ì°¨ì›: {feature_dim}\")\n",
    "    \n",
    "    return create_model(model_config, feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš‚ í›ˆë ¨ ë° í‰ê°€ í•¨ìˆ˜ë“¤\n",
    "\n",
    "### ë§ˆìŠ¤í¬ ê¸°ë°˜ ì†ì‹¤ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    \"\"\"íŒ¨ë”©ì„ ê³ ë ¤í•œ ì•ˆì „í•œ MSE Loss - NaN ë°©ì§€\"\"\"\n",
    "    \n",
    "    def __init__(self, padding_value: float = -9999.0):\n",
    "        super().__init__()\n",
    "        self.padding_value = padding_value\n",
    "    \n",
    "    def forward(self, predictions, targets, masks):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: [batch_size, seq_len]\n",
    "            targets: [batch_size, seq_len]  \n",
    "            masks: [batch_size, seq_len] (True = íŒ¨ë”©)\n",
    "        \"\"\"\n",
    "        # íŒ¨ë”©ë˜ì§€ ì•Šì€ ìœ„ì¹˜ë§Œ ì„ íƒ\n",
    "        valid_mask = ~masks\n",
    "        \n",
    "        if valid_mask.sum() == 0:\n",
    "            print(\"âš ï¸ ê²½ê³ : ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "            return torch.tensor(0.0, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        valid_predictions = predictions[valid_mask]\n",
    "        valid_targets = targets[valid_mask]\n",
    "        \n",
    "        # NaN/Inf ì²´í¬ ë° ì œê±°\n",
    "        finite_mask = torch.isfinite(valid_predictions) & torch.isfinite(valid_targets)\n",
    "        \n",
    "        if finite_mask.sum() == 0:\n",
    "            print(\"âš ï¸ ê²½ê³ : finiteí•œ ê°’ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "            return torch.tensor(1e6, device=predictions.device, requires_grad=True)  # í° ì†ì‹¤ê°’ ë°˜í™˜\n",
    "        \n",
    "        valid_predictions = valid_predictions[finite_mask]\n",
    "        valid_targets = valid_targets[finite_mask]\n",
    "        \n",
    "        # MSE ê³„ì‚°\n",
    "        mse = F.mse_loss(valid_predictions, valid_targets)\n",
    "        \n",
    "        # NaN ì²´í¬\n",
    "        if torch.isnan(mse) or torch.isinf(mse):\n",
    "            print(f\"âš ï¸ ê²½ê³ : MSEê°€ {mse.item()}ì…ë‹ˆë‹¤!\")\n",
    "            return torch.tensor(1e6, device=predictions.device, requires_grad=True)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets, masks, padding_value: float = -9999.0):\n",
    "    \"\"\"íŒ¨ë”©ì„ ê³ ë ¤í•œ ë©”íŠ¸ë¦­ ê³„ì‚°\"\"\"\n",
    "    valid_mask = ~masks\n",
    "    \n",
    "    if valid_mask.sum() == 0:\n",
    "        return {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    valid_predictions = predictions[valid_mask]\n",
    "    valid_targets = targets[valid_mask]\n",
    "    \n",
    "    # CPUë¡œ ë³€í™˜\n",
    "    valid_predictions = valid_predictions.detach().cpu().numpy()\n",
    "    valid_targets = valid_targets.detach().cpu().numpy()\n",
    "    \n",
    "    mse = np.mean((valid_predictions - valid_targets) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(valid_predictions - valid_targets))\n",
    "    \n",
    "    # MAPE ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)\n",
    "    epsilon = 1e-8\n",
    "    abs_targets = np.abs(valid_targets)\n",
    "    abs_errors = np.abs(valid_predictions - valid_targets)\n",
    "    safe_targets = np.maximum(abs_targets, epsilon)\n",
    "    mape = np.mean(abs_errors / safe_targets * 100)\n",
    "    \n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse, \n",
    "        \"mae\": mae,\n",
    "        \"mape\": mape,\n",
    "        \"valid_count\": len(valid_predictions)\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í›ˆë ¨ ì—í­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, epoch):\n",
    "    \"\"\"í•œ ì—í­ í›ˆë ¨ - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš©\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_metrics = {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    \n",
    "    pbar = tqdm(\n",
    "        enumerate(dataloader), \n",
    "        total=len(dataloader),\n",
    "        desc=f\"Epoch {epoch} [Train]\",\n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    for batch_idx, batch in pbar:\n",
    "        # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë§ëŠ” í‚¤ ì‚¬ìš©\n",
    "        features = batch[\"features\"].to(device)  # ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„±\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "        masks = batch[\"masks\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©\n",
    "        predictions = model(features, masks)\n",
    "        loss = criterion(predictions, targets, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "        with torch.no_grad():\n",
    "            batch_metrics = compute_metrics(predictions, targets, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "            total_metrics[key] += batch_metrics[key]\n",
    "        total_metrics[\"valid_count\"] += batch_metrics[\"valid_count\"]\n",
    "        \n",
    "        # ì§„í–‰ë°” ì—…ë°ì´íŠ¸\n",
    "        pbar.set_postfix({\n",
    "            \"Loss\": f\"{loss.item():.4f}\",\n",
    "            \"MAPE\": f\"{batch_metrics['mape']:.2f}%\"\n",
    "        })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # í‰ê·  ê³„ì‚°\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "        total_metrics[key] = total_metrics[key] / len(dataloader)\n",
    "    \n",
    "    return avg_loss, total_metrics\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device, epoch=None):\n",
    "    \"\"\"ê²€ì¦ ì—í­ - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš©\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_metrics = {\"mse\": 0, \"rmse\": 0, \"mae\": 0, \"mape\": 0, \"valid_count\": 0}\n",
    "    \n",
    "    desc = f\"Epoch {epoch} [Val]\" if epoch is not None else \"Validation\"\n",
    "    pbar = tqdm(dataloader, desc=desc, leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            # ìƒˆë¡œìš´ ë°ì´í„°ì…‹ êµ¬ì¡°ì— ë§ëŠ” í‚¤ ì‚¬ìš©\n",
    "            features = batch[\"features\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            masks = batch[\"masks\"].to(device)\n",
    "            \n",
    "            # ëª¨ë“  ëª¨ë¸ì´ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©\n",
    "            predictions = model(features, masks)\n",
    "            loss = criterion(predictions, targets, masks)\n",
    "            \n",
    "            batch_metrics = compute_metrics(predictions, targets, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "                total_metrics[key] += batch_metrics[key]\n",
    "            total_metrics[\"valid_count\"] += batch_metrics[\"valid_count\"]\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                \"Loss\": f\"{loss.item():.4f}\",\n",
    "                \"MAPE\": f\"{batch_metrics['mape']:.2f}%\"\n",
    "            })\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    for key in [\"mse\", \"rmse\", \"mae\", \"mape\"]:\n",
    "        total_metrics[key] = total_metrics[key] / len(dataloader)\n",
    "    \n",
    "    return avg_loss, total_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©”ì¸ í›ˆë ¨ ë£¨í”„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, training_config, device, save_path):\n",
    "    \"\"\"ë©”ì¸ í›ˆë ¨ ë£¨í”„ - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš© (ê²€ì¦ ê°„ê²© ì„¤ì • ê°€ëŠ¥)\"\"\"\n",
    "    \n",
    "    num_epochs = training_config.get(\"num_epochs\", 100)\n",
    "    learning_rate = training_config.get(\"learning_rate\", 1e-3)\n",
    "    patience = training_config.get(\"patience\", 20)\n",
    "    padding_value = training_config.get(\"padding_value\", -9999.0)\n",
    "    \n",
    "    # ê²€ì¦ ê°„ê²© ì„¤ì • (ìƒˆë¡œ ì¶”ê°€)\n",
    "    val_interval = training_config.get(\"val_interval\", 1)  # ê¸°ë³¸ê°’: ë§¤ ì—í­ë§ˆë‹¤ ê²€ì¦\n",
    "    log_interval = training_config.get(\"log_interval\", 1)  # ê¸°ë³¸ê°’: ë§¤ ì—í­ë§ˆë‹¤ ë¡œê¹…\n",
    "    \n",
    "    # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì €\n",
    "    criterion = MaskedMSELoss(padding_value)\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=patience//2, verbose=True)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    last_val_loss = None\n",
    "    last_val_metrics = None\n",
    "    \n",
    "    logger.info(f\"í›ˆë ¨ ì‹œì‘: {num_epochs} ì—í­, í•™ìŠµë¥  {learning_rate}\")\n",
    "    logger.info(f\"ëª¨ë¸ íƒ€ì…: {training_config.get('model_type', 'unknown')}\")\n",
    "    logger.info(f\"ê²€ì¦ ê°„ê²©: {val_interval} ì—í­ë§ˆë‹¤\")\n",
    "    logger.info(f\"ë¡œê¹… ê°„ê²©: {log_interval} ì—í­ë§ˆë‹¤\")\n",
    "    \n",
    "    # ì—í­ ì§„í–‰ë°”\n",
    "    epoch_pbar = tqdm(range(1, num_epochs + 1), desc=\"Training Progress\")\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        # í›ˆë ¨ì€ ë§¤ ì—í­ë§ˆë‹¤ ì‹¤ì‹œ\n",
    "        train_loss, train_metrics = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, epoch\n",
    "        )\n",
    "        \n",
    "        # ê²€ì¦ì€ ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ ë˜ëŠ” ë§ˆì§€ë§‰ ì—í­ì—ì„œ ì‹¤ì‹œ\n",
    "        should_validate = (epoch % val_interval == 0) or (epoch == num_epochs)\n",
    "        \n",
    "        if should_validate:\n",
    "            val_loss, val_metrics = validate_epoch(\n",
    "                model, val_loader, criterion, device, epoch\n",
    "            )\n",
    "            last_val_loss = val_loss\n",
    "            last_val_metrics = val_metrics\n",
    "            \n",
    "            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (ê²€ì¦ì´ ì‹¤ì‹œëœ ê²½ìš°ë§Œ)\n",
    "            scheduler.step(val_loss)\n",
    "        else:\n",
    "            # ê²€ì¦í•˜ì§€ ì•ŠëŠ” ì—í­ì—ì„œëŠ” ì´ì „ ê²€ì¦ ê²°ê³¼ ì‚¬ìš©\n",
    "            val_loss = last_val_loss if last_val_loss is not None else float('inf')\n",
    "            val_metrics = last_val_metrics if last_val_metrics is not None else {\n",
    "                \"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0\n",
    "            }\n",
    "        \n",
    "        # ë¡œê¹… (ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ ë˜ëŠ” ê²€ì¦ì´ ì‹¤ì‹œëœ ê²½ìš°)\n",
    "        should_log = (epoch % log_interval == 0) or should_validate or (epoch == num_epochs)\n",
    "        \n",
    "        if should_log:\n",
    "            if should_validate:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, \"\n",
    "                    f'Train MAPE={train_metrics[\"mape\"]:.2f}%, Val MAPE={val_metrics[\"mape\"]:.2f}%'\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    f\"Epoch {epoch:3d}: Train Loss={train_loss:.4f}, \"\n",
    "                    f'Train MAPE={train_metrics[\"mape\"]:.2f}% (ê²€ì¦ ìƒëµ)'\n",
    "                )\n",
    "        \n",
    "        # ì§„í–‰ë°” ì—…ë°ì´íŠ¸\n",
    "        val_status = \"ê²€ì¦ë¨\" if should_validate else \"ì´ì „ê°’\"\n",
    "        epoch_pbar.set_postfix({\n",
    "            \"T_Loss\": f\"{train_loss:.4f}\",\n",
    "            \"V_Loss\": f\"{val_loss:.4f}({val_status})\",\n",
    "            \"V_MAPE\": f'{val_metrics[\"mape\"]:.2f}%',\n",
    "            \"Best\": f\"{best_val_loss:.4f}\",\n",
    "            \"Patience\": f\"{patience_counter}/{patience}\"\n",
    "        })\n",
    "        \n",
    "        # ìµœê³  ëª¨ë¸ ì €ì¥ (ê²€ì¦ì´ ì‹¤ì‹œëœ ê²½ìš°ë§Œ)\n",
    "        if should_validate and val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_metrics': val_metrics,\n",
    "                'train_metrics': train_metrics,\n",
    "                'model_type': training_config.get('model_type', 'unknown'),\n",
    "                'feature_dim': training_config.get('feature_dim', None),\n",
    "                'config': training_config\n",
    "            }, save_path)\n",
    "            \n",
    "            logger.info(f\"  â†’ Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        elif should_validate:\n",
    "            # ê²€ì¦ì€ í–ˆì§€ë§Œ ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šì€ ê²½ìš°\n",
    "            patience_counter += 1\n",
    "        # ê²€ì¦í•˜ì§€ ì•Šì€ ê²½ìš° patience_counterëŠ” ì¦ê°€ì‹œí‚¤ì§€ ì•ŠìŒ\n",
    "        \n",
    "        # ì¡°ê¸° ì¢…ë£Œ (ê²€ì¦ì´ ì‹¤ì‹œëœ ê²½ìš°ë§Œ í™•ì¸)\n",
    "        if should_validate and patience_counter >= patience:\n",
    "            logger.info(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "    \n",
    "    epoch_pbar.close()\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ì— í•œë²ˆ ë” ê²€ì¦ (ë§ˆì§€ë§‰ ì—í­ì—ì„œ ê²€ì¦í•˜ì§€ ì•Šì•˜ë‹¤ë©´)\n",
    "    if not should_validate:\n",
    "        logger.info(\"ìµœì¢… ê²€ì¦ ì‹¤ì‹œ ì¤‘...\")\n",
    "        final_val_loss, final_val_metrics = validate_epoch(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "        logger.info(f\"ìµœì¢… ê²€ì¦ ê²°ê³¼: Val Loss={final_val_loss:.4f}, Val MAPE={final_val_metrics['mape']:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'total_epochs': epoch,\n",
    "        'early_stopped': patience_counter >= patience,\n",
    "        'validation_count': len([e for e in range(1, epoch + 1) if e % val_interval == 0 or e == epoch])\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, test_loader, device, model_path, config):\n",
    "    \"\"\"ëª¨ë¸ í‰ê°€ - í†µí•© ì„ë² ë”© ë°ì´í„°ì…‹ìš© (êµ¬ì¡° ì •ë³´ í¬í•¨)\"\"\"\n",
    "    logger.info(f\"ëª¨ë¸ ë¡œë“œ: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    padding_value = config.get(\"padding_value\", -9999.0)\n",
    "    criterion = MaskedMSELoss(padding_value)\n",
    "    \n",
    "    # êµ¬ì¡°í™”ëœ ê²°ê³¼ë¥¼ ìœ„í•œ ë¦¬ìŠ¤íŠ¸ë“¤\n",
    "    structured_predictions = []\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    logger.info(\"í…ŒìŠ¤íŠ¸ ì‹œì‘\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            # collate_fnì—ì„œ ë°˜í™˜í•˜ëŠ” í‚¤ ì´ë¦„ë“¤ ì‚¬ìš©\n",
    "            features = batch[\"features\"].to(device)  # ì´ë¯¸ ê²°í•©ëœ íŠ¹ì„±\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "            masks = batch[\"masks\"].to(device)\n",
    "            \n",
    "            # êµ¬ì¡° ì •ë³´ ì¶”ì¶œ (collate_fnì˜ í‚¤ ì´ë¦„ë“¤ê³¼ ì¼ì¹˜)\n",
    "            timekey_hrs = batch[\"timekey_hrs\"]\n",
    "            oper_ids_lists = batch[\"oper_ids_lists\"]\n",
    "            actual_lengths = batch[\"actual_lengths\"]\n",
    "            first_oper_ids = batch[\"first_oper_ids\"]\n",
    "            last_oper_ids = batch[\"last_oper_ids\"]\n",
    "            \n",
    "            # ëª¨ë¸ ì˜ˆì¸¡\n",
    "            predictions = model(features, masks)\n",
    "            loss = criterion(predictions, targets, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # CPUë¡œ ë³€í™˜\n",
    "            predictions_cpu = predictions.cpu()\n",
    "            targets_cpu = targets.cpu()\n",
    "            masks_cpu = masks.cpu()\n",
    "            \n",
    "            # ë°°ì¹˜ ë‚´ ê° ìƒ˜í”Œì— ëŒ€í•´ êµ¬ì¡°í™”ëœ ê²°ê³¼ ìƒì„±\n",
    "            batch_size = predictions_cpu.shape[0]\n",
    "            for sample_idx in range(batch_size):\n",
    "                timekey_hr = timekey_hrs[sample_idx]\n",
    "                oper_ids = oper_ids_lists[sample_idx]\n",
    "                actual_length = actual_lengths[sample_idx]\n",
    "                first_oper_id = first_oper_ids[sample_idx]\n",
    "                last_oper_id = last_oper_ids[sample_idx]\n",
    "                \n",
    "                sample_predictions = predictions_cpu[sample_idx]\n",
    "                sample_targets = targets_cpu[sample_idx]\n",
    "                sample_masks = masks_cpu[sample_idx]\n",
    "                \n",
    "                # ê° ì‹œí€€ìŠ¤ ìœ„ì¹˜ì— ëŒ€í•´ (íŒ¨ë”©ë˜ì§€ ì•Šì€ ìœ„ì¹˜ë§Œ)\n",
    "                for seq_idx in range(actual_length):\n",
    "                    if seq_idx < len(sample_predictions) and not sample_masks[seq_idx]:\n",
    "                        pred_val = sample_predictions[seq_idx].item()\n",
    "                        target_val = sample_targets[seq_idx].item()\n",
    "                        oper_id = oper_ids[seq_idx] if seq_idx < len(oper_ids) else None\n",
    "                        \n",
    "                        # ê°œë³„ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "                        structured_predictions.append({\n",
    "                            'timekey_hr': timekey_hr,\n",
    "                            'oper_id': oper_id,\n",
    "                            'seq_position': seq_idx,\n",
    "                            'prediction': pred_val,\n",
    "                            'actual': target_val,\n",
    "                            'first_oper_id': first_oper_id,\n",
    "                            'last_oper_id': last_oper_id,\n",
    "                            'window_length': actual_length\n",
    "                        })\n",
    "                        \n",
    "                        all_predictions.append(pred_val)\n",
    "                        all_targets.append(target_val)\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    if len(all_predictions) == 0:\n",
    "        logger.warning(\"ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "        metrics = {\"mse\": 0.0, \"rmse\": 0.0, \"mae\": 0.0, \"mape\": 0.0, \"valid_count\": 0}\n",
    "    else:\n",
    "        mse = np.mean((all_predictions - all_targets) ** 2)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = np.mean(np.abs(all_predictions - all_targets))\n",
    "        \n",
    "        # MAPE ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)\n",
    "        epsilon = 1e-8\n",
    "        abs_targets = np.abs(all_targets)\n",
    "        abs_errors = np.abs(all_predictions - all_targets)\n",
    "        safe_targets = np.maximum(abs_targets, epsilon)\n",
    "        mape = np.mean(abs_errors / safe_targets * 100)\n",
    "        \n",
    "        metrics = {\n",
    "            \"mse\": mse,\n",
    "            \"rmse\": rmse,\n",
    "            \"mae\": mae,\n",
    "            \"mape\": mape,\n",
    "            \"valid_count\": len(all_predictions)\n",
    "        }\n",
    "    \n",
    "    logger.info(f\"í…ŒìŠ¤íŠ¸ ê²°ê³¼: RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}, MAPE={metrics['mape']:.2f}%\")\n",
    "    logger.info(f\"êµ¬ì¡°í™”ëœ ì˜ˆì¸¡ ê²°ê³¼: {len(structured_predictions):,}ê°œ\")\n",
    "    \n",
    "    # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘\n",
    "    model_info = {\n",
    "        \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "        \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        \"model_type\": config.get(\"model_type\", \"unknown\")\n",
    "    }\n",
    "    \n",
    "    # checkpointì—ì„œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ (ìˆëŠ” ê²½ìš°)\n",
    "    if 'feature_dim' in checkpoint:\n",
    "        model_info['feature_dim'] = checkpoint['feature_dim']\n",
    "    if 'epoch' in checkpoint:\n",
    "        model_info['best_epoch'] = checkpoint['epoch']\n",
    "    if 'val_loss' in checkpoint:\n",
    "        model_info['best_val_loss'] = checkpoint['val_loss']\n",
    "    \n",
    "    return {\n",
    "        \"test_loss\": avg_loss,\n",
    "        \"metrics\": metrics,\n",
    "        \"predictions\": all_predictions,\n",
    "        \"targets\": all_targets,\n",
    "        \"structured_predictions\": structured_predictions,\n",
    "        \"model_info\": model_info\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 13:52:30,815 - INFO - Using device: cuda:0\n",
      "2025-09-04 13:52:30,815 - INFO - ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "2025-09-04 14:07:57,271 - INFO - yê°’ ì œê±° í›„: 1671457í–‰\n",
      "2025-09-04 14:07:57,272 - INFO - \n",
      "Inf ê°’ í™•ì¸:\n",
      "2025-09-04 14:07:57,285 - INFO -      â†’ ì•„ì£¼ í° ê°’(100000.0)ìœ¼ë¡œ ëŒ€ì²´\n",
      "2025-09-04 14:07:57,478 - INFO - ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  âŒ x5: 144ê°œ Inf ê°’\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:07:57,479 - INFO -   - ì´ í–‰ ìˆ˜: 1,671,457ê°œ\n",
      "2025-09-04 14:07:57,488 - INFO -   - ê³ ìœ  timekey_hr: 2136ê°œ\n",
      "2025-09-04 14:07:57,488 - INFO - í†µí•©ëœ ë²”ì£¼í˜• ë³€ìˆ˜ ì„¤ì •:\n",
      "2025-09-04 14:07:57,488 - INFO -   - ë³€ìˆ˜ë³„ ì¹´í…Œê³ ë¦¬ ìˆ˜: {'oper_group': 277, 'days': 7, 'shift': 3, 'x1': 20}\n",
      "2025-09-04 14:07:57,489 - INFO -   - ì´ vocabulary í¬ê¸°: 307\n",
      "2025-09-04 14:07:57,600 - INFO -   - oper_group: ì¸ë±ìŠ¤ 0~276 (277ê°œ)\n",
      "2025-09-04 14:07:57,693 - INFO -   - days: ì¸ë±ìŠ¤ 277~283 (7ê°œ)\n",
      "2025-09-04 14:07:57,783 - INFO -   - shift: ì¸ë±ìŠ¤ 284~286 (3ê°œ)\n",
      "2025-09-04 14:07:57,886 - INFO -   - x1: ì¸ë±ìŠ¤ 287~306 (20ê°œ)\n",
      "2025-09-04 14:07:57,886 - INFO -   - ìµœì¢… í†µí•© vocabulary í¬ê¸°: 307\n",
      "2025-09-04 14:07:58,507 - INFO - ë‚ ì§œ ê¸°ì¤€ ë°ì´í„° ë¶„í•  ì™„ë£Œ:\n",
      "2025-09-04 14:07:58,508 - INFO -   - ì´ ë‚ ì§œ ìˆ˜: 90ì¼\n",
      "2025-09-04 14:07:58,508 - INFO -   - Train: 72ì¼ (1,342,808í–‰)\n",
      "2025-09-04 14:07:58,509 - INFO -   - Validation: 9ì¼ (170,705í–‰)\n",
      "2025-09-04 14:07:58,509 - INFO -   - Test: 9ì¼ (157,944í–‰)\n",
      "2025-09-04 14:07:58,510 - INFO -   - Train ë‚ ì§œ ë²”ìœ„: 20250503 ~ 20250713\n",
      "2025-09-04 14:07:58,511 - INFO -   - Val ë‚ ì§œ ë²”ìœ„: 20250714 ~ 20250722\n",
      "2025-09-04 14:07:58,511 - INFO -   - Test ë‚ ì§œ ë²”ìœ„: 20250723 ~ 20250731\n",
      "2025-09-04 14:07:58,512 - INFO - \n",
      "Window sliding ì„¤ì •:\n",
      "2025-09-04 14:07:58,513 - INFO -   - Window í¬ê¸°: 50\n",
      "2025-09-04 14:07:58,513 - INFO -   - Stride: 50 (ë¹„ê²¹ì¹¨)\n",
      "2025-09-04 14:07:58,514 - INFO -   - Padding ê°’: -9999.0\n",
      "2025-09-04 14:08:31,958 - INFO - Window sliding ê²°ê³¼:\n",
      "2025-09-04 14:08:31,958 - INFO -   - ì´ ìƒ˜í”Œ ìˆ˜: 27424\n",
      "2025-09-04 14:08:31,962 - INFO -   - í‰ê·  ì‹¤ì œ ê¸¸ì´: 49.0\n",
      "2025-09-04 14:08:31,965 - INFO -   - ìµœëŒ€ ì‹¤ì œ ê¸¸ì´: 50\n",
      "2025-09-04 14:08:31,966 - INFO -   - ìµœì†Œ ì‹¤ì œ ê¸¸ì´: 1\n",
      "2025-09-04 14:08:31,970 - INFO -   - ê³ ìœ í•œ timekey_hr: 1721ê°œ\n",
      "2025-09-04 14:08:31,971 - INFO -   - timekey_hrë‹¹ í‰ê·  ìƒ˜í”Œ ìˆ˜: 15.9ê°œ\n",
      "2025-09-04 14:08:31,975 - INFO - ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ:\n",
      "2025-09-04 14:08:31,976 - INFO -   - ì´ ìƒ˜í”Œ ìˆ˜: 27424\n",
      "2025-09-04 14:08:31,976 - INFO -   - Window í¬ê¸°: 50\n",
      "2025-09-04 14:08:31,977 - INFO -   - Stride: 50\n",
      "2025-09-04 14:08:31,977 - INFO -   - ì—°ì†í˜• ì°¨ì›: 20\n",
      "2025-09-04 14:08:31,978 - INFO -   - ë²”ì£¼í˜• ë³€ìˆ˜ ìˆ˜: 4\n",
      "2025-09-04 14:08:31,979 - INFO -   - ì„ë² ë”© ì°¨ì›: 8\n",
      "2025-09-04 14:08:31,979 - INFO -   - ìµœì¢… íŠ¹ì„± ì°¨ì›: 52\n",
      "2025-09-04 14:08:31,980 - INFO -   - íŒ¨ë”©ê°’: -9999.0\n",
      "2025-09-04 14:08:36,229 - INFO - Window sliding ê²°ê³¼:\n",
      "2025-09-04 14:08:36,229 - INFO -   - ì´ ìƒ˜í”Œ ìˆ˜: 3458\n",
      "2025-09-04 14:08:36,231 - INFO -   - í‰ê·  ì‹¤ì œ ê¸¸ì´: 49.4\n",
      "2025-09-04 14:08:36,231 - INFO -   - ìµœëŒ€ ì‹¤ì œ ê¸¸ì´: 50\n",
      "2025-09-04 14:08:36,232 - INFO -   - ìµœì†Œ ì‹¤ì œ ê¸¸ì´: 1\n",
      "2025-09-04 14:08:36,233 - INFO -   - ê³ ìœ í•œ timekey_hr: 216ê°œ\n",
      "2025-09-04 14:08:36,234 - INFO -   - timekey_hrë‹¹ í‰ê·  ìƒ˜í”Œ ìˆ˜: 16.0ê°œ\n",
      "2025-09-04 14:08:36,235 - INFO - ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ:\n",
      "2025-09-04 14:08:36,236 - INFO -   - ì´ ìƒ˜í”Œ ìˆ˜: 3458\n",
      "2025-09-04 14:08:36,236 - INFO -   - Window í¬ê¸°: 50\n",
      "2025-09-04 14:08:36,237 - INFO -   - Stride: 50\n",
      "2025-09-04 14:08:36,237 - INFO -   - ì—°ì†í˜• ì°¨ì›: 20\n",
      "2025-09-04 14:08:36,238 - INFO -   - ë²”ì£¼í˜• ë³€ìˆ˜ ìˆ˜: 4\n",
      "2025-09-04 14:08:36,238 - INFO -   - ì„ë² ë”© ì°¨ì›: 8\n",
      "2025-09-04 14:08:36,239 - INFO -   - ìµœì¢… íŠ¹ì„± ì°¨ì›: 52\n",
      "2025-09-04 14:08:36,239 - INFO -   - íŒ¨ë”©ê°’: -9999.0\n",
      "2025-09-04 14:08:40,041 - INFO - Window sliding ê²°ê³¼:\n",
      "2025-09-04 14:08:40,041 - INFO -   - ì´ ìƒ˜í”Œ ìˆ˜: 3195\n",
      "2025-09-04 14:08:40,042 - INFO -   - í‰ê·  ì‹¤ì œ ê¸¸ì´: 49.4\n",
      "2025-09-04 14:08:40,043 - INFO -   - ìµœëŒ€ ì‹¤ì œ ê¸¸ì´: 50\n",
      "2025-09-04 14:08:40,044 - INFO -   - ìµœì†Œ ì‹¤ì œ ê¸¸ì´: 1\n",
      "2025-09-04 14:08:40,045 - INFO -   - ê³ ìœ í•œ timekey_hr: 199ê°œ\n",
      "2025-09-04 14:08:40,046 - INFO -   - timekey_hrë‹¹ í‰ê·  ìƒ˜í”Œ ìˆ˜: 16.1ê°œ\n",
      "2025-09-04 14:08:40,047 - INFO - ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ:\n",
      "2025-09-04 14:08:40,047 - INFO -   - ì´ ìƒ˜í”Œ ìˆ˜: 3195\n",
      "2025-09-04 14:08:40,048 - INFO -   - Window í¬ê¸°: 50\n",
      "2025-09-04 14:08:40,048 - INFO -   - Stride: 50\n",
      "2025-09-04 14:08:40,049 - INFO -   - ì—°ì†í˜• ì°¨ì›: 20\n",
      "2025-09-04 14:08:40,049 - INFO -   - ë²”ì£¼í˜• ë³€ìˆ˜ ìˆ˜: 4\n",
      "2025-09-04 14:08:40,050 - INFO -   - ì„ë² ë”© ì°¨ì›: 8\n",
      "2025-09-04 14:08:40,050 - INFO -   - ìµœì¢… íŠ¹ì„± ì°¨ì›: 52\n",
      "2025-09-04 14:08:40,051 - INFO -   - íŒ¨ë”©ê°’: -9999.0\n",
      "2025-09-04 14:08:40,051 - INFO - \n",
      "ë°ì´í„°ë¡œë” ìƒì„± ì™„ë£Œ:\n",
      "2025-09-04 14:08:40,052 - INFO -   - ë°°ì¹˜ í¬ê¸°: 16\n",
      "2025-09-04 14:08:40,052 - INFO -   - Train ë°°ì¹˜ ìˆ˜: 1714\n",
      "2025-09-04 14:08:40,053 - INFO -   - Val ë°°ì¹˜ ìˆ˜: 217\n",
      "2025-09-04 14:08:40,053 - INFO -   - Test ë°°ì¹˜ ìˆ˜: 200\n",
      "2025-09-04 14:08:40,054 - INFO -   - ìµœì¢… íŠ¹ì„± ì°¨ì›: 52\n",
      "2025-09-04 14:08:40,153 - INFO - ëª¨ë¸ ìƒì„± ì¤‘...\n",
      "2025-09-04 14:08:41,721 - INFO - ë°ì´í„°ë¡œë”ì—ì„œ ì¶”ì¶œí•œ íŠ¹ì„± ì°¨ì›: 52\n",
      "2025-09-04 14:08:41,722 - INFO - ëª¨ë¸ ìƒì„± ì¤‘:\n",
      "2025-09-04 14:08:41,722 - INFO -   - ëª¨ë¸ íƒ€ì…: lstm\n",
      "2025-09-04 14:08:41,723 - INFO -   - ì…ë ¥ íŠ¹ì„± ì°¨ì›: 52\n",
      "2025-09-04 14:08:41,723 - INFO -   - íˆë“  ì°¨ì›: 128\n",
      "2025-09-04 14:08:41,724 - INFO -   - ë ˆì´ì–´ ìˆ˜: 2\n",
      "2025-09-04 14:08:41,724 - INFO -   - ë“œë¡­ì•„ì›ƒ: 0.1\n",
      "2025-09-04 14:08:41,730 - INFO -   - RNN íƒ€ì…: LSTM\n",
      "2025-09-04 14:08:41,730 - INFO -   - ì–‘ë°©í–¥: Yes\n",
      "2025-09-04 14:08:41,731 - INFO -   - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: 614,657\n",
      "2025-09-04 14:08:41,731 - INFO -   - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: 614,657\n",
      "2025-09-04 14:08:41,732 - INFO - ëª¨ë¸ ìƒì„± ì™„ë£Œ!\n",
      "2025-09-04 14:08:41,732 - INFO - í›ˆë ¨ ì‹œì‘...\n",
      "/usr/local/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "2025-09-04 14:08:43,014 - INFO - í›ˆë ¨ ì‹œì‘: 10 ì—í­, í•™ìŠµë¥  0.0001\n",
      "2025-09-04 14:08:43,015 - INFO - ëª¨ë¸ íƒ€ì…: lstm\n",
      "2025-09-04 14:08:43,015 - INFO - ê²€ì¦ ê°„ê²©: 2 ì—í­ë§ˆë‹¤\n",
      "2025-09-04 14:08:43,016 - INFO - ë¡œê¹… ê°„ê²©: 1 ì—í­ë§ˆë‹¤\n",
      "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  27%|â–ˆâ–ˆâ–‹       | 468/1714 [00:05<00:11, 105.64it/s, Loss=0.0377, MAPE=62.83%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  27%|â–ˆâ–ˆâ–‹       | 468/1714 [00:05<00:11, 105.64it/s, Loss=0.0256, MAPE=62.13%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  27%|â–ˆâ–ˆâ–‹       | 468/1714 [00:05<00:11, 105.64it/s, Loss=0.0191, MAPE=71.60%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1282/1714 [00:13<00:04, 103.81it/s, Loss=0.0315, MAPE=60.68%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1282/1714 [00:13<00:04, 103.81it/s, Loss=0.0183, MAPE=59.88%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1293/1714 [00:13<00:04, 100.78it/s, Loss=0.0183, MAPE=59.88%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1458/1714 [00:15<00:02, 104.94it/s, Loss=0.0145, MAPE=63.11%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0  ì„ë² ë”© Inf: 0\n",
      "\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 [Train]:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1458/1714 [00:15<00:02, 104.94it/s, Loss=0.0156, MAPE=65.41%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1458/1714 [00:15<00:02, 104.94it/s, Loss=0.0144, MAPE=54.38%]\u001b[A2025-09-04 14:09:01,401 - INFO - Epoch   1: Train Loss=0.0267, Train MAPE=60036.54% (ê²€ì¦ ìƒëµ)\n",
      "Training Progress:  10%|â–ˆ         | 1/10 [00:18<02:45, 18.38s/it, T_Loss=0.0267, V_Loss=inf(ì´ì „ê°’), V_MAPE=0.00%, Best=inf, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  21%|â–ˆâ–ˆ        | 364/1714 [00:03<00:13, 101.13it/s, Loss=0.0321, MAPE=131.03%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  21%|â–ˆâ–ˆ        | 364/1714 [00:03<00:13, 101.13it/s, Loss=0.0110, MAPE=81.75%] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten í›„: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1344/1714 [00:13<00:03, 103.81it/s, Loss=0.0235, MAPE=54.70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "\n",
      "  ê²°í•© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 1344/1714 [00:13<00:03, 103.81it/s, Loss=0.0362, MAPE=52.00%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1661/1714 [00:16<00:00, 104.06it/s, Loss=0.0158, MAPE=50.23%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "\n",
      "  ì„ë² ë”© NaN: 0  ì„ë² ë”© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten í›„: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1661/1714 [00:16<00:00, 104.06it/s, Loss=0.0091, MAPE=49.57%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 [Train]:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1661/1714 [00:16<00:00, 104.06it/s, Loss=0.0412, MAPE=55.35%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 5749.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [0, 289]\n",
      "  ê³ ìœ ê°’: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 5749.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0150, 1.0080]\n",
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4450.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 291]\n",
      "  ê³ ìœ ê°’: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 4450.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.3440]\n",
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 7433.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [12, 292]\n",
      "  ê³ ìœ ê°’: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7486, 7433.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:09:20,625 - INFO - Epoch   2: Train Loss=0.0248, Val Loss=0.0205, Train MAPE=57499.59%, Val MAPE=16735.68%\n",
      "Training Progress:  10%|â–ˆ         | 1/10 [00:37<02:45, 18.38s/it, T_Loss=0.0248, V_Loss=0.0205(ê²€ì¦ë¨), V_MAPE=16735.68%, Best=inf, Patience=0/20]2025-09-04 14:09:20,640 - INFO -   â†’ Best model saved! (Val Loss: 0.0205)\n",
      "Training Progress:  20%|â–ˆâ–ˆ        | 2/10 [00:37<02:31, 18.89s/it, T_Loss=0.0248, V_Loss=0.0205(ê²€ì¦ë¨), V_MAPE=16735.68%, Best=inf, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 867/1714 [00:08<00:08, 101.54it/s, Loss=0.0290, MAPE=58.23%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 867/1714 [00:08<00:08, 101.54it/s, Loss=0.0189, MAPE=58.40%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten í›„: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 [Train]:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 966/1714 [00:09<00:07, 100.86it/s, Loss=0.0213, MAPE=64.06%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© embed_dim: 8  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 966/1714 [00:09<00:07, 100.86it/s, Loss=0.0193, MAPE=71.04%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 [Train]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1372/1714 [00:13<00:03, 103.02it/s, Loss=0.0104, MAPE=55.11%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 [Train]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1372/1714 [00:13<00:03, 103.02it/s, Loss=0.0099, MAPE=61.52%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  íƒ€ê²Ÿ NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1372/1714 [00:13<00:03, 103.02it/s, Loss=0.0395, MAPE=52.20%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:09:37,794 - INFO - Epoch   3: Train Loss=0.0247, Train MAPE=56787.93% (ê²€ì¦ ìƒëµ)\n",
      "Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [00:54<02:06, 18.10s/it, T_Loss=0.0247, V_Loss=0.0205(ì´ì „ê°’), V_MAPE=16735.68%, Best=0.0205, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:   6%|â–Œ         | 95/1714 [00:00<00:16, 100.47it/s, Loss=0.0208, MAPE=59.53%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:   6%|â–Œ         | 95/1714 [00:01<00:16, 100.47it/s, Loss=0.0197, MAPE=64.22%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1230/1714 [00:12<00:04, 103.92it/s, Loss=0.0120, MAPE=63.70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1230/1714 [00:12<00:04, 103.92it/s, Loss=0.0165, MAPE=52.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 [Train]:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1494/1714 [00:14<00:02, 102.92it/s, Loss=0.0087, MAPE=55.82%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1494/1714 [00:14<00:02, 102.92it/s, Loss=0.1138, MAPE=53.33%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 5749.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [0, 289]\n",
      "  ê³ ìœ ê°’: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 5749.0000]íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0150, 1.0080]\n",
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4450.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 291]\n",
      "  ê³ ìœ ê°’: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 4450.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.3440]\n",
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 7433.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [12, 292]\n",
      "  ê³ ìœ ê°’: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7486, 7433.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:09:56,790 - INFO - Epoch   4: Train Loss=0.0244, Val Loss=0.0202, Train MAPE=61718.76%, Val MAPE=20951.88%\n",
      "Training Progress:  30%|â–ˆâ–ˆâ–ˆ       | 3/10 [01:13<02:06, 18.10s/it, T_Loss=0.0244, V_Loss=0.0202(ê²€ì¦ë¨), V_MAPE=20951.88%, Best=0.0205, Patience=0/20]2025-09-04 14:09:56,816 - INFO -   â†’ Best model saved! (Val Loss: 0.0202)\n",
      "Training Progress:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:13<01:50, 18.46s/it, T_Loss=0.0244, V_Loss=0.0202(ê²€ì¦ë¨), V_MAPE=20951.88%, Best=0.0205, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  26%|â–ˆâ–ˆâ–‹       | 450/1714 [00:04<00:12, 103.57it/s, Loss=0.1339, MAPE=75.13%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])  ì„ë² ë”© NaN: 0  ì„ë² ë”© Inf: 0\n",
      "\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  26%|â–ˆâ–ˆâ–‹       | 450/1714 [00:04<00:12, 103.57it/s, Loss=0.0182, MAPE=77.79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  27%|â–ˆâ–ˆâ–‹       | 461/1714 [00:04<00:12, 97.85it/s, Loss=0.0182, MAPE=77.79%] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 731/1714 [00:07<00:09, 102.44it/s, Loss=0.0164, MAPE=55.96%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 742/1714 [00:07<00:09, 102.68it/s, Loss=0.0164, MAPE=55.96%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 742/1714 [00:07<00:09, 102.68it/s, Loss=0.0183, MAPE=61.68%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 818/1714 [00:08<00:08, 102.54it/s, Loss=0.0137, MAPE=62.32%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 818/1714 [00:08<00:08, 102.54it/s, Loss=0.0151, MAPE=55.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:10:14,087 - INFO - Epoch   5: Train Loss=0.0243, Train MAPE=66567.06% (ê²€ì¦ ìƒëµ)\n",
      "Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:31<01:30, 18.03s/it, T_Loss=0.0243, V_Loss=0.0202(ì´ì „ê°’), V_MAPE=20951.88%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 [Train]:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 831/1714 [00:08<00:08, 101.70it/s, Loss=0.0503, MAPE=61.21%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 831/1714 [00:08<00:08, 101.70it/s, Loss=0.0126, MAPE=69.67%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1205/1714 [00:11<00:04, 102.05it/s, Loss=0.0154, MAPE=67.85%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307  ì„ë² ë”© embed_dim: 8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1205/1714 [00:11<00:04, 102.05it/s, Loss=0.0130, MAPE=64.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1656/1714 [00:16<00:00, 101.68it/s, Loss=0.0077, MAPE=53.02%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê°’ ë²”ìœ„: [3, 296]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 [Train]:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1667/1714 [00:16<00:00, 100.70it/s, Loss=0.0077, MAPE=53.02%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  íƒ€ê²Ÿ NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 1667/1714 [00:16<00:00, 100.70it/s, Loss=0.0335, MAPE=52.16%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 5749.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [0, 289]\n",
      "  ê³ ìœ ê°’: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 5749.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0150, 1.0080]\n",
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4450.0000]\n",
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])  ê°’ ë²”ìœ„: [1, 291]\n",
      "  ê³ ìœ ê°’: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 4450.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.3440]\n",
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 7433.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [12, 292]\n",
      "  ê³ ìœ ê°’: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7486, 7433.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:10:33,191 - INFO - Epoch   6: Train Loss=0.0242, Val Loss=0.0202, Train MAPE=66663.10%, Val MAPE=17262.90%\n",
      "Training Progress:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5/10 [01:50<01:30, 18.03s/it, T_Loss=0.0242, V_Loss=0.0202(ê²€ì¦ë¨), V_MAPE=17262.90%, Best=0.0202, Patience=0/20]2025-09-04 14:10:33,219 - INFO -   â†’ Best model saved! (Val Loss: 0.0202)\n",
      "Training Progress:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [01:50<01:13, 18.41s/it, T_Loss=0.0242, V_Loss=0.0202(ê²€ì¦ë¨), V_MAPE=17262.90%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 611/1714 [00:06<00:10, 101.82it/s, Loss=0.0169, MAPE=58.79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 611/1714 [00:06<00:10, 101.82it/s, Loss=0.0460, MAPE=114.58%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 [Train]:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1356/1714 [00:13<00:03, 101.52it/s, Loss=0.0218, MAPE=66.65%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê°’ ë²”ìœ„: [1, 293]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1356/1714 [00:13<00:03, 101.52it/s, Loss=0.0767, MAPE=59.25%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© Inf: 0ìµœì¢… ê²°í•©: torch.Size([50, 52])  ê²°í•© NaN: 0\n",
      "\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 [Train]:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1486/1714 [00:14<00:02, 100.94it/s, Loss=0.0218, MAPE=74.92%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1486/1714 [00:14<00:02, 100.94it/s, Loss=0.0152, MAPE=60.60%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:10:50,461 - INFO - Epoch   7: Train Loss=0.0239, Train MAPE=68148.45% (ê²€ì¦ ìƒëµ)\n",
      "Training Progress:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:07<00:54, 18.03s/it, T_Loss=0.0239, V_Loss=0.0202(ì´ì „ê°’), V_MAPE=17262.90%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]:  14%|â–ˆâ–        | 239/1714 [00:02<00:14, 101.50it/s, Loss=0.0319, MAPE=55.30%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  14%|â–ˆâ–        | 239/1714 [00:02<00:14, 101.50it/s, Loss=0.0183, MAPE=59.58%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 820/1714 [00:08<00:08, 102.20it/s, Loss=0.0193, MAPE=58.45%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 820/1714 [00:08<00:08, 102.20it/s, Loss=0.0079, MAPE=59.60%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1409/1714 [00:13<00:03, 101.37it/s, Loss=0.0191, MAPE=78.16%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 [Train]:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1409/1714 [00:13<00:03, 101.37it/s, Loss=0.0183, MAPE=82.70%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© vocab_size: 307  ì„ë² ë”© Inf: 0\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 5749.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [0, 289]\n",
      "  ê³ ìœ ê°’: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 5749.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0150, 1.0080]\n",
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4450.0000]ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "\n",
      "  ê°’ ë²”ìœ„: [1, 291]\n",
      "  ê³ ìœ ê°’: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])  ì„ë² ë”© NaN: 0\n",
      "\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 4450.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.3440]\n",
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 7433.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [12, 292]\n",
      "  ê³ ìœ ê°’: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7486, 7433.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:11:09,772 - INFO - Epoch   8: Train Loss=0.0238, Val Loss=0.0206, Train MAPE=64260.82%, Val MAPE=22810.22%\n",
      "Training Progress:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 8/10 [02:26<00:36, 18.43s/it, T_Loss=0.0238, V_Loss=0.0206(ê²€ì¦ë¨), V_MAPE=22810.22%, Best=0.0202, Patience=0/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 [Train]:  18%|â–ˆâ–Š        | 304/1714 [00:03<00:13, 101.85it/s, Loss=0.0203, MAPE=59.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  18%|â–ˆâ–Š        | 315/1714 [00:03<00:13, 100.58it/s, Loss=0.0203, MAPE=59.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  18%|â–ˆâ–Š        | 315/1714 [00:03<00:13, 100.58it/s, Loss=0.0149, MAPE=67.48%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  20%|â–ˆâ–‰        | 336/1714 [00:03<00:14, 95.39it/s, Loss=0.0330, MAPE=55.84%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  20%|â–ˆâ–‰        | 336/1714 [00:03<00:14, 95.39it/s, Loss=0.0076, MAPE=63.79%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 [Train]:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 993/1714 [00:09<00:07, 102.47it/s, Loss=0.1532, MAPE=54.32%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 993/1714 [00:09<00:07, 102.47it/s, Loss=0.0346, MAPE=58.47%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:11:27,036 - INFO - Epoch   9: Train Loss=0.0237, Train MAPE=68876.10% (ê²€ì¦ ìƒëµ)\n",
      "Training Progress:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [02:44<00:18, 18.07s/it, T_Loss=0.0237, V_Loss=0.0206(ì´ì „ê°’), V_MAPE=22810.22%, Best=0.0202, Patience=1/20]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:   4%|â–         | 61/1714 [00:00<00:16, 100.33it/s, Loss=0.0100, MAPE=57.07%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [3, 296]\n",
      "  ê³ ìœ ê°’: [3, 4, 5, 9, 11, 12, 13, 19, 24, 25, 56, 67, 78, 89, 100, 145, 200, 221, 222, 233, 238, 241, 244, 256, 262, 268, 271, 276, 279, 284, 293, 294, 295, 296]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ê²°í•© Inf: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:   4%|â–         | 61/1714 [00:00<00:16, 100.33it/s, Loss=0.0223, MAPE=67.05%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ê²°í•© ë²”ìœ„: [-2.5095, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 293]\n",
      "  ê³ ìœ ê°’: [1, 34, 56, 67, 78, 89, 100, 112, 200, 204, 214, 218, 219, 221, 222, 223, 224, 229, 241, 244, 245, 249, 255, 256, 279, 284, 287, 288, 289, 290, 291, 292, 293]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 753/1714 [00:07<00:09, 102.07it/s, Loss=0.0141, MAPE=52.98%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© NaN: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.9151, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0030, 0.0400]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 753/1714 [00:07<00:09, 102.07it/s, Loss=0.0520, MAPE=523899.81%]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025050307\n",
      "actual_length: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1456/1714 [00:14<00:02, 102.40it/s, Loss=0.0216, MAPE=54.44%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4175.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [9, 301]\n",
      "  ê³ ìœ ê°’: [9, 46, 56, 58, 60, 65, 66, 68, 78, 100, 145, 200, 218, 219, 221, 222, 232, 233, 234, 241, 242, 244, 255, 256, 276, 279, 284, 296, 298, 299, 300, 301]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 [Train]:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1456/1714 [00:14<00:02, 102.40it/s, Loss=0.0239, MAPE=58.39%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-3.8325, 4175.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0060, 0.0380]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 5749.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [0, 289]\n",
      "  ê³ ìœ ê°’: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 280, 286, 287, 288, 289]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307  ì„ë² ë”© embed_dim: 8\n",
      "\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 5749.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0150, 1.0080]\n",
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4450.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [1, 291]\n",
      "  ê³ ìœ ê°’: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 280, 286, 289, 290, 291]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7773, 4450.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.3440]\n",
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025071400\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 7433.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [12, 292]\n",
      "  ê³ ìœ ê°’: [12, 34, 56, 78, 89, 100, 112, 145, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 255, 266, 280, 286, 291, 292]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.7486, 7433.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 5.3140]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 14:11:46,228 - INFO - Epoch  10: Train Loss=0.0234, Val Loss=0.0204, Train MAPE=64784.27%, Val MAPE=20515.76%\n",
      "Training Progress: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:03<00:00, 18.32s/it, T_Loss=0.0234, V_Loss=0.0204(ê²€ì¦ë¨), V_MAPE=20515.76%, Best=0.0202, Patience=1/20]\n",
      "2025-09-04 14:11:46,231 - INFO - í›ˆë ¨ ì™„ë£Œ, í…ŒìŠ¤íŠ¸ ì‹œì‘...\n",
      "2025-09-04 14:11:46,231 - INFO - ëª¨ë¸ ë¡œë“œ: 250904_test/lstm_20250904_135229.pth\n",
      "2025-09-04 14:11:46,246 - INFO - í…ŒìŠ¤íŠ¸ ì‹œì‘\n",
      "Testing:   0%|          | 0/200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ìƒ˜í”Œ 0 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025072300\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 6911.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [0, 289]\n",
      "  ê³ ìœ ê°’: [0, 1, 12, 23, 34, 45, 56, 67, 78, 89, 100, 111, 112, 134, 156, 167, 178, 189, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 222, 233, 244, 255, 266, 282, 286, 287, 288, 289]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0  ê²°í•© ë²”ìœ„: [-2.2500, 6911.0000]\n",
      "\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.2450]\n",
      "\n",
      "=== ìƒ˜í”Œ 1 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025072300\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4500.0000]\n",
      "\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])  ê°’ ë²”ìœ„: [1, 291]\n",
      "  ê³ ìœ ê°’: [1, 12, 23, 34, 45, 56, 67, 78, 89, 145, 200, 208, 212, 213, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 234, 235, 236, 238, 244, 255, 266, 282, 286, 289, 290, 291]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.4695, 4500.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0110, 0.8490]\n",
      "\n",
      "=== ìƒ˜í”Œ 2 ë””ë²„ê¹… ===\n",
      "timekey_hr: 2025072300\n",
      "actual_length: 50\n",
      "ì—°ì†í˜• ë°ì´í„°: torch.Size([50, 20])\n",
      "  NaN: 0\n",
      "  Inf: 0\n",
      "  ë²”ìœ„: [0.0000, 4721.0000]\n",
      "ë²”ì£¼í˜• ë°ì´í„°: torch.Size([50, 4])\n",
      "  ê°’ ë²”ìœ„: [12, 292]\n",
      "  ê³ ìœ ê°’: [12, 34, 56, 78, 89, 100, 112, 145, 200, 203, 204, 208, 221, 222, 223, 235, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 266, 282, 286, 291, 292]\n",
      "ì„ë² ë”© ì ìš© ì¤‘...\n",
      "  ì„ë² ë”© vocab_size: 307\n",
      "  ì„ë² ë”© embed_dim: 8\n",
      "  ì„ë² ë”© ê²°ê³¼: torch.Size([50, 4, 8])\n",
      "  ì„ë² ë”© NaN: 0\n",
      "  ì„ë² ë”© Inf: 0\n",
      "  Flatten í›„: torch.Size([50, 32])\n",
      "  Flatten NaN: 0\n",
      "ìµœì¢… ê²°í•©: torch.Size([50, 52])\n",
      "  ê²°í•© NaN: 0\n",
      "  ê²°í•© Inf: 0\n",
      "  ê²°í•© ë²”ìœ„: [-2.3043, 4721.0000]\n",
      "íƒ€ê²Ÿ ë°ì´í„°: torch.Size([50])\n",
      "  íƒ€ê²Ÿ NaN: 0\n",
      "  íƒ€ê²Ÿ ë²”ìœ„: [0.0130, 1.8050]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:03<00:00, 59.86it/s]\n",
      "2025-09-04 14:11:49,611 - INFO - í…ŒìŠ¤íŠ¸ ê²°ê³¼: RMSE=0.1503, MAE=0.0523, MAPE=8027.00%\n",
      "2025-09-04 14:11:49,611 - INFO - êµ¬ì¡°í™”ëœ ì˜ˆì¸¡ ê²°ê³¼: 157,944ê°œ\n",
      "2025-09-04 14:11:49,613 - INFO - ê²°ê³¼ ì €ì¥ ì™„ë£Œ: 250904_test/lstm_20250904_135229_results.json\n",
      "2025-09-04 14:11:51,202 - INFO - êµ¬ì¡°í™”ëœ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥:\n",
      "2025-09-04 14:11:51,203 - INFO -   - íŒŒì¼ ê²½ë¡œ: 250904_test/lstm_20250904_135229_predictions.csv\n",
      "2025-09-04 14:11:51,203 - INFO -   - ì €ì¥ëœ ì˜ˆì¸¡ ê°œìˆ˜: 157,944ê°œ\n",
      "2025-09-04 14:11:51,205 - INFO -   - ê³ ìœ í•œ timekey_hr: 199ê°œ\n",
      "2025-09-04 14:11:51,215 - INFO -   - ê³ ìœ í•œ oper_id: 813ê°œ\n",
      "2025-09-04 14:11:51,228 - INFO -   - ê³ ìœ í•œ first_oper_id: 255ê°œ\n",
      "2025-09-04 14:11:51,236 - INFO -   - ê³ ìœ í•œ last_oper_id: 249ê°œ\n",
      "2025-09-04 14:11:51,237 - INFO -   - í‰ê·  ìœˆë„ìš° ê¸¸ì´: 49.7\n",
      "2025-09-04 14:11:51,237 - INFO - ==================================================\n",
      "2025-09-04 14:11:51,238 - INFO - ì‹¤í—˜ ì™„ë£Œ: lstm_20250904_135229\n",
      "2025-09-04 14:11:51,239 - INFO - ëª¨ë¸ íƒ€ì…: lstm\n",
      "2025-09-04 14:11:51,239 - INFO - í…ŒìŠ¤íŠ¸ RMSE: 0.1503\n",
      "2025-09-04 14:11:51,240 - INFO - í…ŒìŠ¤íŠ¸ MAE: 0.0523\n",
      "2025-09-04 14:11:51,240 - INFO - í…ŒìŠ¤íŠ¸ MAPE: 8027.00%\n",
      "2025-09-04 14:11:51,241 - INFO - ì´ íŒŒë¼ë¯¸í„° ìˆ˜: 614,657\n",
      "2025-09-04 14:11:51,241 - INFO - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: 614,657\n",
      "2025-09-04 14:11:51,242 - INFO - ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: 250904_test/lstm_20250904_135229.pth\n",
      "2025-09-04 14:11:51,242 - INFO - ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: 250904_test/lstm_20250904_135229_results.json\n",
      "2025-09-04 14:11:51,243 - INFO - ==================================================\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ëª¨ë¸ë§\")\n",
    "    parser.add_argument(\"--config-dir\", default=\"configs\", help=\"ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬\")\n",
    "    parser.add_argument(\"--mode\", choices=[\"train\", \"eval\"], default=\"train\", help=\"ì‹¤í–‰ ëª¨ë“œ\")\n",
    "    parser.add_argument(\"--model-path\", default=None, help=\"í‰ê°€ìš© ëª¨ë¸ ê²½ë¡œ\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"GPU ë²ˆí˜¸\")\n",
    "    parser.add_argument(\"--exp-name\", default=None, help=\"ì‹¤í—˜ëª…\")\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "\n",
    "    # ì„¤ì • ë¡œë“œ\n",
    "    config = load_config(args.config_dir)\n",
    "    set_random_seeds(42)\n",
    "\n",
    "    # ì‹¤í—˜ëª… ì„¤ì •\n",
    "    if args.exp_name:\n",
    "        exp_name = args.exp_name\n",
    "    else:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_type = config.get(\"model_type\", \"lstm\")\n",
    "        exp_name = f\"{model_type}_{timestamp}\"\n",
    "\n",
    "    # ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    save_dir = config.get(\"save_dir\", \"models\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model_save_path = os.path.join(save_dir, f\"{exp_name}.pth\")\n",
    "\n",
    "    # ë¡œê¹… ì„¤ì •\n",
    "    logger = setup_logging(os.path.join(save_dir, config.get(\"log_file\", f\"{exp_name}.log\")))\n",
    "\n",
    "    # ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "    device = torch.device(f\"cuda:{args.gpu}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    # ë°ì´í„°ë¡œë” ìƒì„± (í†µí•© ì„ë² ë”© ë°©ì‹)\n",
    "    logger.info(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "    train_loader, val_loader, test_loader, categorical_processor, embedding_layer = create_dataloaders(config)\n",
    "\n",
    "    # ëª¨ë¸ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹)\n",
    "    logger.info(\"ëª¨ë¸ ìƒì„± ì¤‘...\")\n",
    "    model = create_model_from_config_and_dataloader(config, train_loader)\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        # í›ˆë ¨\n",
    "        logger.info(\"í›ˆë ¨ ì‹œì‘...\")\n",
    "        train_results = train_model(\n",
    "            model, train_loader, val_loader, config, device, model_save_path\n",
    "        )\n",
    "        \n",
    "        logger.info(\"í›ˆë ¨ ì™„ë£Œ, í…ŒìŠ¤íŠ¸ ì‹œì‘...\")\n",
    "        test_results = evaluate_model(\n",
    "            model, test_loader, device, model_save_path, config\n",
    "        )\n",
    "        \n",
    "    else:\n",
    "        # í‰ê°€\n",
    "        if not args.model_path:\n",
    "            raise ValueError(\"--model-path must be provided in eval mode\")\n",
    "        test_results = evaluate_model(\n",
    "            model, test_loader, device, args.model_path, config\n",
    "        )\n",
    "\n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    results = {\n",
    "        \"exp_name\": exp_name,\n",
    "        \"config\": config,\n",
    "        \"test_metrics\": test_results[\"metrics\"],\n",
    "        \"model_info\": test_results.get(\"model_info\", {\n",
    "            \"total_parameters\": sum(p.numel() for p in model.parameters()),\n",
    "            \"trainable_parameters\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "            \"model_type\": config.get(\"model_type\", \"unknown\")\n",
    "        })\n",
    "    }\n",
    "\n",
    "    results_path = os.path.join(save_dir, f\"{exp_name}_results.json\")\n",
    "    with open(results_path, \"w\") as f:\n",
    "        json.dump(results, f, indent=2, default=str)\n",
    "\n",
    "    logger.info(f\"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}\")\n",
    "\n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥\n",
    "    if \"structured_predictions\" in test_results and test_results[\"structured_predictions\"]:\n",
    "        # êµ¬ì¡°í™”ëœ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ (timekey_hr, oper_id í¬í•¨)\n",
    "        structured_df = pd.DataFrame(test_results[\"structured_predictions\"])\n",
    "        \n",
    "        # ì»¬ëŸ¼ëª… í™•ì¸ í›„ ì—ëŸ¬ ê³„ì‚° (prediction vs predicted í†µì¼)\n",
    "        pred_col = \"prediction\" if \"prediction\" in structured_df.columns else \"predicted\"\n",
    "        \n",
    "        structured_df[\"error\"] = structured_df[pred_col] - structured_df[\"actual\"]\n",
    "        structured_df[\"abs_error\"] = structured_df[\"error\"].abs()\n",
    "        structured_df[\"abs_percent_error\"] = (\n",
    "            structured_df[\"abs_error\"] / structured_df[\"actual\"].abs().clip(lower=1e-8) * 100\n",
    "        )\n",
    "        \n",
    "        # êµ¬ì¡°í™”ëœ ê²°ê³¼ë¥¼ ë©”ì¸ ì˜ˆì¸¡ íŒŒì¼ë¡œ ì €ì¥\n",
    "        predictions_path = os.path.join(save_dir, f\"{exp_name}_predictions.csv\")\n",
    "        structured_df.to_csv(predictions_path, index=False)\n",
    "        \n",
    "        logger.info(f\"êµ¬ì¡°í™”ëœ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥:\")\n",
    "        logger.info(f\"  - íŒŒì¼ ê²½ë¡œ: {predictions_path}\")\n",
    "        logger.info(f\"  - ì €ì¥ëœ ì˜ˆì¸¡ ê°œìˆ˜: {len(structured_df):,}ê°œ\")\n",
    "        logger.info(f\"  - ê³ ìœ í•œ timekey_hr: {structured_df['timekey_hr'].nunique()}ê°œ\")\n",
    "        \n",
    "        # oper_id ì •ë³´ ì¶œë ¥ (ìˆëŠ” ê²½ìš°)\n",
    "        if 'oper_id' in structured_df.columns:\n",
    "            logger.info(f\"  - ê³ ìœ í•œ oper_id: {structured_df['oper_id'].nunique()}ê°œ\")\n",
    "        \n",
    "        # ì¶”ê°€ êµ¬ì¡° ì •ë³´ ì¶œë ¥ (ìˆëŠ” ê²½ìš°)\n",
    "        if 'first_oper_id' in structured_df.columns:\n",
    "            logger.info(f\"  - ê³ ìœ í•œ first_oper_id: {structured_df['first_oper_id'].nunique()}ê°œ\")\n",
    "        if 'last_oper_id' in structured_df.columns:\n",
    "            logger.info(f\"  - ê³ ìœ í•œ last_oper_id: {structured_df['last_oper_id'].nunique()}ê°œ\")\n",
    "        if 'window_length' in structured_df.columns:\n",
    "            avg_window = structured_df['window_length'].mean()\n",
    "            logger.info(f\"  - í‰ê·  ìœˆë„ìš° ê¸¸ì´: {avg_window:.1f}\")\n",
    "        \n",
    "    else:\n",
    "        # êµ¬ì¡°í™”ëœ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ë°©ì‹ìœ¼ë¡œ ì €ì¥ (í˜¸í™˜ì„± ìœ ì§€)\n",
    "        if \"predictions\" in test_results and \"targets\" in test_results:\n",
    "            predictions_df = pd.DataFrame({\n",
    "                \"actual\": test_results[\"targets\"],\n",
    "                \"predicted\": test_results[\"predictions\"],\n",
    "                \"residual\": test_results[\"targets\"] - test_results[\"predictions\"],\n",
    "                \"abs_error\": np.abs(test_results[\"targets\"] - test_results[\"predictions\"]),\n",
    "                \"abs_percent_error\": (\n",
    "                    np.abs(test_results[\"targets\"] - test_results[\"predictions\"]) / \n",
    "                    np.maximum(np.abs(test_results[\"targets\"]), 1e-8) * 100\n",
    "                )\n",
    "            })\n",
    "            \n",
    "            predictions_path = os.path.join(save_dir, f\"{exp_name}_predictions.csv\")\n",
    "            predictions_df.to_csv(predictions_path, index=False)\n",
    "            \n",
    "            logger.info(f\"ê¸°ë³¸ ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥:\")\n",
    "            logger.info(f\"  - íŒŒì¼ ê²½ë¡œ: {predictions_path}\")\n",
    "            logger.info(f\"  - ì €ì¥ëœ ì˜ˆì¸¡ ê°œìˆ˜: {len(predictions_df):,}ê°œ\")\n",
    "        else:\n",
    "            logger.warning(\"ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ì–´ CSV íŒŒì¼ì„ ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ ìš”ì•½\n",
    "    logger.info(\"=\" * 50)\n",
    "    logger.info(f\"ì‹¤í—˜ ì™„ë£Œ: {exp_name}\")\n",
    "    logger.info(f\"ëª¨ë¸ íƒ€ì…: {config.get('model_type')}\")\n",
    "    logger.info(f\"í…ŒìŠ¤íŠ¸ RMSE: {test_results['metrics']['rmse']:.4f}\")\n",
    "    logger.info(f\"í…ŒìŠ¤íŠ¸ MAE: {test_results['metrics']['mae']:.4f}\")\n",
    "    logger.info(f\"í…ŒìŠ¤íŠ¸ MAPE: {test_results['metrics']['mape']:.2f}%\")\n",
    "\n",
    "    # ëª¨ë¸ ì •ë³´ ì¶œë ¥\n",
    "    model_info = results[\"model_info\"]\n",
    "    if \"total_parameters\" in model_info:\n",
    "        logger.info(f\"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {model_info['total_parameters']:,}\")\n",
    "    if \"trainable_parameters\" in model_info:\n",
    "        logger.info(f\"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {model_info['trainable_parameters']:,}\")\n",
    "\n",
    "    logger.info(f\"ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {model_save_path}\")\n",
    "    logger.info(f\"ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {results_path}\")\n",
    "    logger.info(\"=\" * 50)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
